{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_in_Finance-Deep-Learning-Interaction\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.0 (08.09.2019)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6 and Tensorflow 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Dixon M.F., I. Halperin and P. Bilokon, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as tds\n",
    "from statsmodels.api import add_constant\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this notebook is to illustrate a neural network interpretability method which is compatible with linear regression, including an interaction term. In linear regression, provided the independent variables are scaled, one can view the regression coefficients as a measure of importance of the variables and their interaction effect. Equivalently, the dependent variable can be differentiated w.r.t. the inputs to give the coefficient, with the interaction obtained from the cross-term in the Hessian. Similarly, the derivatives of the network w.r.t. the inputs are a non-linear generalization of interpretability in a linear regression model with interaction effects. Moreover, we should expect the neural network gradients to approximate the regression model coefficients when the data is generated by a linear regression model with interaction terms. Various simple experimental tests, corresponding to Section 4 of Chpt 5, are performed to illustrate the properties of network interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Generation Process (DGP)\n",
    "\n",
    "Generate data from a regression model with an interaction term \n",
    "\n",
    "$Y=X_1+X_2 + X_1X_2+\\epsilon, ~X_1, X_2 \\sim N(0,1,), \\epsilon \\sim N(0,\\sigma_n^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5000\n",
    "np.random.seed(7)\n",
    "X = np.zeros(shape=(M,2))\n",
    "sigma_n = 0.01\n",
    "X[:int(M/2),0]= np.random.randn(int(M/2))\n",
    "X[:int(M/2),1]= np.random.randn(int(M/2))\n",
    "# use antithetic sampling to reduce sample bias in the mean\n",
    "X[int(M/2):,0]= -X[:int(M/2),0]\n",
    "X[int(M/2):,1]= -X[:int(M/2),1]\n",
    "\n",
    "eps= np.random.randn(M)\n",
    "Y= 1.0*X[:,0] + 1.0*X[:,1] + 1.0*X[:,0]*X[:,1] + sigma_n*eps.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use OLS to fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_results = sm.OLS(Y, sm.add_constant(X)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ols=ols_results.predict(sm.add_constant(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.669</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.669</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5052.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 12 May 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:09:20</td>     <th>  Log-Likelihood:    </th> <td> -7103.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5000</td>      <th>  AIC:               </th> <td>1.421e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4997</td>      <th>  BIC:               </th> <td>1.423e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.0243</td> <td>    0.014</td> <td>    1.713</td> <td> 0.087</td> <td>   -0.004</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.9999</td> <td>    0.014</td> <td>   70.236</td> <td> 0.000</td> <td>    0.972</td> <td>    1.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    1.0000</td> <td>    0.014</td> <td>   70.164</td> <td> 0.000</td> <td>    0.972</td> <td>    1.028</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>846.889</td> <th>  Durbin-Watson:     </th> <td>   2.024</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>16614.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.168</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.924</td>  <th>  Cond. No.          </th> <td>    1.02</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.669\n",
       "Model:                            OLS   Adj. R-squared:                  0.669\n",
       "Method:                 Least Squares   F-statistic:                     5052.\n",
       "Date:                Tue, 12 May 2020   Prob (F-statistic):               0.00\n",
       "Time:                        19:09:20   Log-Likelihood:                -7103.5\n",
       "No. Observations:                5000   AIC:                         1.421e+04\n",
       "Df Residuals:                    4997   BIC:                         1.423e+04\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0243      0.014      1.713      0.087      -0.004       0.052\n",
       "x1             0.9999      0.014     70.236      0.000       0.972       1.028\n",
       "x2             1.0000      0.014     70.164      0.000       0.972       1.028\n",
       "==============================================================================\n",
       "Omnibus:                      846.889   Durbin-Watson:                   2.024\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            16614.377\n",
       "Skew:                           0.168   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.924   Cond. No.                         1.02\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with a ffwd neural network with no hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_NN0_model(l1_reg=0.0):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=2, kernel_initializer='normal')) #, activation='None'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KerasRegressor(build_fn=linear_NN0_model, epochs=40, batch_size=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5000/5000 [==============================] - 1s 112us/step - loss: 2.1870 - mean_absolute_error: 1.0362 - mean_squared_error: 2.1870\n",
      "Epoch 2/40\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 1.4521 - mean_absolute_error: 0.7911 - mean_squared_error: 1.4521\n",
      "Epoch 3/40\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 1.1473 - mean_absolute_error: 0.6759 - mean_squared_error: 1.1473\n",
      "Epoch 4/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0432 - mean_absolute_error: 0.6392 - mean_squared_error: 1.0432\n",
      "Epoch 5/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.0133 - mean_absolute_error: 0.6288 - mean_squared_error: 1.0133\n",
      "Epoch 6/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.0068 - mean_absolute_error: 0.6265 - mean_squared_error: 1.0068\n",
      "Epoch 7/40\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 1.0052 - mean_absolute_error: 0.6260 - mean_squared_error: 1.0052\n",
      "Epoch 8/40\n",
      "5000/5000 [==============================] - 0s 90us/step - loss: 1.0048 - mean_absolute_error: 0.6261 - mean_squared_error: 1.0048\n",
      "Epoch 9/40\n",
      "5000/5000 [==============================] - 0s 79us/step - loss: 1.0047 - mean_absolute_error: 0.6259 - mean_squared_error: 1.0047\n",
      "Epoch 10/40\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 1.0050 - mean_absolute_error: 0.6264 - mean_squared_error: 1.0050\n",
      "Epoch 11/40\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 1.0047 - mean_absolute_error: 0.6258 - mean_squared_error: 1.0047\n",
      "Epoch 12/40\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 1.0046 - mean_absolute_error: 0.6258 - mean_squared_error: 1.0046\n",
      "Epoch 13/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0045 - mean_absolute_error: 0.6260 - mean_squared_error: 1.0045\n",
      "Epoch 14/40\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0048 - mean_absolute_error: 0.6259 - mean_squared_error: 1.0048\n",
      "Epoch 15/40\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0048 - mean_absolute_error: 0.6263 - mean_squared_error: 1.0048\n",
      "Epoch 16/40\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0047 - mean_absolute_error: 0.6260 - mean_squared_error: 1.0047\n",
      "Epoch 17/40\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 1.0047 - mean_absolute_error: 0.6260 - mean_squared_error: 1.0047\n",
      "Epoch 18/40\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0046 - mean_absolute_error: 0.6259 - mean_squared_error: 1.0046\n",
      "Epoch 19/40\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 1.0048 - mean_absolute_error: 0.6261 - mean_squared_error: 1.0048\n",
      "Epoch 20/40\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 1.0048 - mean_absolute_error: 0.6261 - mean_squared_error: 1.0048\n",
      "Epoch 21/40\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 1.0048 - mean_absolute_error: 0.6258 - mean_squared_error: 1.0048\n",
      "Epoch 22/40\n",
      "5000/5000 [==============================] - 0s 79us/step - loss: 1.0048 - mean_absolute_error: 0.6262 - mean_squared_error: 1.0048\n",
      "Epoch 23/40\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 1.0049 - mean_absolute_error: 0.6260 - mean_squared_error: 1.0049\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c30b9f9e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the weights are close to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[0.99630827]\n",
      " [0.99237144]]\n",
      "Bias: [0.02239072]\n"
     ]
    }
   ],
   "source": [
    "W=lm.model.layers[0].get_weights()[0]\n",
    "b=lm.model.layers[0].get_weights()[1]\n",
    "print(\"Weights: \"+ str(W))\n",
    "print(\"Bias: \" +str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with a FFW Neural Network with one hidden layer (unactivated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # number of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_NN1_model(l1_reg=0.0):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=2, kernel_initializer='normal')) \n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KerasRegressor(build_fn=linear_NN1_model, epochs=50, batch_size=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 1s 123us/step - loss: 1.6924 - mean_absolute_error: 0.8499 - mean_squared_error: 1.6924\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 0s 89us/step - loss: 1.0064 - mean_absolute_error: 0.6269 - mean_squared_error: 1.0064\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0069 - mean_absolute_error: 0.6264 - mean_squared_error: 1.0069\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.0101 - mean_absolute_error: 0.6284 - mean_squared_error: 1.0101\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.0089 - mean_absolute_error: 0.6268 - mean_squared_error: 1.0089\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 0s 91us/step - loss: 1.0082 - mean_absolute_error: 0.6265 - mean_squared_error: 1.0082\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.0069 - mean_absolute_error: 0.6275 - mean_squared_error: 1.0069\n",
      "Epoch 8/50\n",
      "5000/5000 [==============================] - 0s 89us/step - loss: 1.0089 - mean_absolute_error: 0.6271 - mean_squared_error: 1.0089\n",
      "Epoch 9/50\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0096 - mean_absolute_error: 0.6282 - mean_squared_error: 1.0096\n",
      "Epoch 10/50\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 1.0075 - mean_absolute_error: 0.6270 - mean_squared_error: 1.0075\n",
      "Epoch 11/50\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0097 - mean_absolute_error: 0.6284 - mean_squared_error: 1.0097\n",
      "Epoch 12/50\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 1.0080 - mean_absolute_error: 0.6274 - mean_squared_error: 1.0080\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c32e8ec88>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.37814584  0.30338565 -0.27131096  0.3684254   0.33092746 -0.30008778\n",
      "  -0.2542268  -0.29876462 -0.2947897   0.28479353]\n",
      " [-0.2638578   0.30114004 -0.3520979   0.26289687  0.20439926 -0.35611275\n",
      "  -0.3233128  -0.2558721  -0.3140065   0.320782  ]] [[-0.28841314]\n",
      " [ 0.31222868]\n",
      " [-0.32610312]\n",
      " [ 0.30656126]\n",
      " [ 0.3128089 ]\n",
      " [-0.32401562]\n",
      " [-0.2921686 ]\n",
      " [-0.36572298]\n",
      " [-0.33868715]\n",
      " [ 0.3483182 ]]\n"
     ]
    }
   ],
   "source": [
    "W1=lm.model.get_weights()[0]\n",
    "b1=lm.model.get_weights()[1]\n",
    "W2=lm.model.get_weights()[2]\n",
    "b2=lm.model.get_weights()[3]\n",
    "print(W1, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the coefficients are close to one and the intercept is close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0=np.dot(np.transpose(W2), b1) + b2\n",
    "beta_1=np.dot(np.transpose(W2), W1[0])\n",
    "beta_2=np.dot(np.transpose(W2), W1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03241214] [0.9885408] [0.95098716]\n"
     ]
    }
   ],
   "source": [
    "print(beta_0,beta_1,beta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with a FFW Neural Network with one hidden layer (tanh activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hidden neurons\n",
    "n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with non-linear activation\n",
    "def linear_NN1_model_act(l1_reg=0.0):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=2, kernel_initializer='normal', activation='tanh'))\n",
    "    model.add(Dense(1, kernel_initializer='normal')) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KerasRegressor(build_fn=linear_NN1_model_act, epochs=100, batch_size=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 1s 125us/step - loss: 2.7012 - mean_absolute_error: 1.2210 - mean_squared_error: 2.7012\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 0s 92us/step - loss: 1.9903 - mean_absolute_error: 1.0652 - mean_squared_error: 1.9903\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 0s 93us/step - loss: 1.9608 - mean_absolute_error: 1.0588 - mean_squared_error: 1.9608\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 0s 92us/step - loss: 1.9343 - mean_absolute_error: 1.0521 - mean_squared_error: 1.9343\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 0s 86us/step - loss: 1.9035 - mean_absolute_error: 1.0448 - mean_squared_error: 1.9035\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 0s 89us/step - loss: 1.8692 - mean_absolute_error: 1.0367 - mean_squared_error: 1.8692\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 0s 89us/step - loss: 1.8355 - mean_absolute_error: 1.0270 - mean_squared_error: 1.8355\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.8023 - mean_absolute_error: 1.0189 - mean_squared_error: 1.8023\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 0s 90us/step - loss: 1.7652 - mean_absolute_error: 1.0093 - mean_squared_error: 1.7652\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.7344 - mean_absolute_error: 1.0005 - mean_squared_error: 1.7344\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.7034 - mean_absolute_error: 0.9925 - mean_squared_error: 1.7034\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.6739 - mean_absolute_error: 0.9840 - mean_squared_error: 1.6739\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.6441 - mean_absolute_error: 0.9760 - mean_squared_error: 1.6441\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.6126 - mean_absolute_error: 0.9666 - mean_squared_error: 1.6126\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.5636 - mean_absolute_error: 0.9554 - mean_squared_error: 1.5636\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.4783 - mean_absolute_error: 0.9322 - mean_squared_error: 1.4783\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 0s 99us/step - loss: 1.3792 - mean_absolute_error: 0.9030 - mean_squared_error: 1.3792\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 1s 108us/step - loss: 1.2945 - mean_absolute_error: 0.8768 - mean_squared_error: 1.2945\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.2324 - mean_absolute_error: 0.8592 - mean_squared_error: 1.2324\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.1925 - mean_absolute_error: 0.8468 - mean_squared_error: 1.1925\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.1652 - mean_absolute_error: 0.8393 - mean_squared_error: 1.1652\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.1464 - mean_absolute_error: 0.8338 - mean_squared_error: 1.1464\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.1308 - mean_absolute_error: 0.8298 - mean_squared_error: 1.1308\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 0s 93us/step - loss: 1.1182 - mean_absolute_error: 0.8265 - mean_squared_error: 1.1182\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.1104 - mean_absolute_error: 0.8247 - mean_squared_error: 1.1104\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 0s 86us/step - loss: 1.1006 - mean_absolute_error: 0.8219 - mean_squared_error: 1.1006\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 0s 98us/step - loss: 1.0952 - mean_absolute_error: 0.8207 - mean_squared_error: 1.0952\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 0s 99us/step - loss: 1.0877 - mean_absolute_error: 0.8182 - mean_squared_error: 1.0877\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.0847 - mean_absolute_error: 0.8190 - mean_squared_error: 1.0847\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0786 - mean_absolute_error: 0.8172 - mean_squared_error: 1.0786\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.0730 - mean_absolute_error: 0.8148 - mean_squared_error: 1.0730\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 0s 100us/step - loss: 1.0720 - mean_absolute_error: 0.8154 - mean_squared_error: 1.0720\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 1s 130us/step - loss: 1.0667 - mean_absolute_error: 0.8135 - mean_squared_error: 1.0667\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 0s 97us/step - loss: 1.0608 - mean_absolute_error: 0.8117 - mean_squared_error: 1.0608\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 0s 90us/step - loss: 1.0612 - mean_absolute_error: 0.8122 - mean_squared_error: 1.0612\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 0s 86us/step - loss: 1.0585 - mean_absolute_error: 0.8114 - mean_squared_error: 1.0585 0s - loss: 1.0612 - mean_absolute_error: 0.8172 - mean_squar\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0556 - mean_absolute_error: 0.8107 - mean_squared_error: 1.0556\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 0s 93us/step - loss: 1.0529 - mean_absolute_error: 0.8104 - mean_squared_error: 1.0529\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0514 - mean_absolute_error: 0.8092 - mean_squared_error: 1.0514\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0504 - mean_absolute_error: 0.8092 - mean_squared_error: 1.0504\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 1.0476 - mean_absolute_error: 0.8082 - mean_squared_error: 1.0476\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0477 - mean_absolute_error: 0.8085 - mean_squared_error: 1.0477\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0449 - mean_absolute_error: 0.8075 - mean_squared_error: 1.0449\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0453 - mean_absolute_error: 0.8086 - mean_squared_error: 1.0453\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 0s 79us/step - loss: 1.0429 - mean_absolute_error: 0.8078 - mean_squared_error: 1.0429\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.0396 - mean_absolute_error: 0.8062 - mean_squared_error: 1.0396\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 1.0409 - mean_absolute_error: 0.8071 - mean_squared_error: 1.0409\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 1.0389 - mean_absolute_error: 0.8061 - mean_squared_error: 1.0389\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 1.0381 - mean_absolute_error: 0.8057 - mean_squared_error: 1.0381\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 1.0368 - mean_absolute_error: 0.8054 - mean_squared_error: 1.0368\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 0s 98us/step - loss: 1.0360 - mean_absolute_error: 0.8050 - mean_squared_error: 1.0360\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0349 - mean_absolute_error: 0.8053 - mean_squared_error: 1.0349\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 1.0329 - mean_absolute_error: 0.8043 - mean_squared_error: 1.0329\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 1.0334 - mean_absolute_error: 0.8038 - mean_squared_error: 1.0334\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.0327 - mean_absolute_error: 0.8048 - mean_squared_error: 1.0327\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.0315 - mean_absolute_error: 0.8045 - mean_squared_error: 1.0315\n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0308 - mean_absolute_error: 0.8043 - mean_squared_error: 1.0308\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 0s 97us/step - loss: 1.0287 - mean_absolute_error: 0.8028 - mean_squared_error: 1.0287\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 1.0286 - mean_absolute_error: 0.8034 - mean_squared_error: 1.0286\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.0271 - mean_absolute_error: 0.8029 - mean_squared_error: 1.0271\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0270 - mean_absolute_error: 0.8032 - mean_squared_error: 1.0270\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0272 - mean_absolute_error: 0.8036 - mean_squared_error: 1.0272\n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 0s 94us/step - loss: 1.0245 - mean_absolute_error: 0.8023 - mean_squared_error: 1.0245\n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0247 - mean_absolute_error: 0.8028 - mean_squared_error: 1.0247\n",
      "Epoch 65/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0234 - mean_absolute_error: 0.8024 - mean_squared_error: 1.0234\n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0231 - mean_absolute_error: 0.8015 - mean_squared_error: 1.0231\n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0226 - mean_absolute_error: 0.8020 - mean_squared_error: 1.0226\n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0223 - mean_absolute_error: 0.8016 - mean_squared_error: 1.0223\n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0224 - mean_absolute_error: 0.8034 - mean_squared_error: 1.0224\n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0221 - mean_absolute_error: 0.8021 - mean_squared_error: 1.0221\n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0210 - mean_absolute_error: 0.8021 - mean_squared_error: 1.0210\n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0216 - mean_absolute_error: 0.8015 - mean_squared_error: 1.0216\n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0200 - mean_absolute_error: 0.8016 - mean_squared_error: 1.0200\n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0193 - mean_absolute_error: 0.8004 - mean_squared_error: 1.0193\n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0203 - mean_absolute_error: 0.8020 - mean_squared_error: 1.0203\n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0177 - mean_absolute_error: 0.8004 - mean_squared_error: 1.0177\n",
      "Epoch 77/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0184 - mean_absolute_error: 0.8021 - mean_squared_error: 1.0184\n",
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0174 - mean_absolute_error: 0.8007 - mean_squared_error: 1.0174\n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0163 - mean_absolute_error: 0.8002 - mean_squared_error: 1.0163\n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0182 - mean_absolute_error: 0.8012 - mean_squared_error: 1.0182\n",
      "Epoch 81/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0155 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0155\n",
      "Epoch 82/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0158 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0158\n",
      "Epoch 83/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0142 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0142\n",
      "Epoch 84/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0164 - mean_absolute_error: 0.8015 - mean_squared_error: 1.0164\n",
      "Epoch 85/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0144 - mean_absolute_error: 0.7999 - mean_squared_error: 1.0144\n",
      "Epoch 86/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0147 - mean_absolute_error: 0.7994 - mean_squared_error: 1.0147\n",
      "Epoch 87/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0154 - mean_absolute_error: 0.8011 - mean_squared_error: 1.0154\n",
      "Epoch 88/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0132 - mean_absolute_error: 0.7996 - mean_squared_error: 1.0132\n",
      "Epoch 89/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0129 - mean_absolute_error: 0.7994 - mean_squared_error: 1.0129\n",
      "Epoch 90/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0136 - mean_absolute_error: 0.7995 - mean_squared_error: 1.0136\n",
      "Epoch 91/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0127 - mean_absolute_error: 0.7996 - mean_squared_error: 1.0127\n",
      "Epoch 92/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0127 - mean_absolute_error: 0.7991 - mean_squared_error: 1.0127\n",
      "Epoch 93/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0120 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0120\n",
      "Epoch 94/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0109 - mean_absolute_error: 0.7987 - mean_squared_error: 1.0109\n",
      "Epoch 95/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0136 - mean_absolute_error: 0.7999 - mean_squared_error: 1.0136\n",
      "Epoch 96/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0128 - mean_absolute_error: 0.8003 - mean_squared_error: 1.0128\n",
      "Epoch 97/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0131 - mean_absolute_error: 0.7988 - mean_squared_error: 1.0131\n",
      "Epoch 98/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0097 - mean_absolute_error: 0.7983 - mean_squared_error: 1.0097\n",
      "Epoch 99/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0118 - mean_absolute_error: 0.8004 - mean_squared_error: 1.0118\n",
      "Epoch 100/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0112 - mean_absolute_error: 0.7987 - mean_squared_error: 1.0112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c344d9400>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the activation function is tanh\n",
    "def sensitivities(lm, X):\n",
    "    \n",
    "    W1=lm.model.get_weights()[0]\n",
    "    b1=lm.model.get_weights()[1]\n",
    "    W2=lm.model.get_weights()[2]\n",
    "    b2=lm.model.get_weights()[3]\n",
    "    \n",
    "    \n",
    "    M = np.shape(X)[0]\n",
    "    p = np.shape(X)[1]\n",
    "\n",
    "    beta=np.array([0]*M*(p+1), dtype='float32').reshape(M,p+1)\n",
    "    beta_interact=np.array([0]*M*p*p, dtype='float32').reshape(M,p,p)\n",
    "    \n",
    "    beta[:,0]= (np.dot(np.transpose(W2),np.tanh(b1)) + b2)[0] # intercept \\beta_0= F_{W,b}(0)\n",
    "    for i in range(M):\n",
    " \n",
    "      Z1 = np.tanh(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1)\n",
    "      #Z1 = np.maximum(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1,0) \n",
    "      \n",
    "      D = np.diag(1-Z1**2) \n",
    "      D_prime =np.diag(-2*Z1*(1-Z1**2))   # needed for interaction term     \n",
    "      #D = np.diag(np.sign(Z1))  \n",
    "        \n",
    "      for j in range(p):  \n",
    "          beta[i,j+1]=np.dot(np.transpose(W2),np.dot(D,W1[j]))\n",
    "          #interaction term \n",
    "          for k in range(p):\n",
    "            beta_interact[i,j,k]=np.dot(np.transpose(W2),np.dot(np.diag(W1[j]), np.dot(D_prime,W1[k])))  \n",
    "    \n",
    "            \n",
    "    return(beta, beta_interact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, beta_inter=sensitivities(lm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the intercept is close to one and the coefficients are close to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06321367  1.0291194   0.9991432 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(beta, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1101154e-06 9.6800876e-01 9.7116160e-01]\n"
     ]
    }
   ],
   "source": [
    "print(np.std(beta, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01137162  0.9574556 ]\n",
      " [ 0.9574556  -0.01866435]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(beta_inter, axis=0)) # off-diagonals are interaction terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
