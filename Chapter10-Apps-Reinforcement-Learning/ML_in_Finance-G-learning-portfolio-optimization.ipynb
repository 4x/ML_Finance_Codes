{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_in_Finance-G-learning-portfolio-optimization\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.1 (27.2.2020)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X running Python 3.6.9 with the following packages:\n",
    "# numpy=1.18.1, matplotlib=3.1.3, scipy=1.4.1\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Bilokon P., Dixon M.F. and Halperin I., Machine Learning in Finance: From Theory to Practice, Springer Graduate Textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-learning for portfolio optimization \n",
    "\n",
    "Implementation of semi-analytical G-learning with quadratic rewards for portfolio optimization and wealth management\n",
    "\n",
    "Note: currently only the setting for wealth management is implemented (this corresponds to use_for_WM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import random, linalg\n",
    "\n",
    "# To ignore warnings that are annoying.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_learning_portfolio_opt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_steps,\n",
    "                 lambd, \n",
    "                 eta,\n",
    "                 beta,\n",
    "                 gamma, \n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns\n",
    "                 asset_holdings, # array of shape num_steps x num_stocks\n",
    "                 use_for_WM = True, # use for wealth management tasks\n",
    "                 target_portf=None):\n",
    "        \n",
    "        \n",
    "        self.num_steps = num_steps\n",
    "        self.lambd = lambd\n",
    "        self.eta = eta\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.use_for_WM = use_for_WM\n",
    "        \n",
    "        self.num_assets = exp_returns.shape[1]\n",
    "        \n",
    "        assert exp_returns.shape[0] == self.num_steps\n",
    "        assert Sigma_r.shape[0] == Sigma_r.shape[1]\n",
    "        assert Sigma_r.shape[0] == self.num_assets\n",
    "        \n",
    "        self.exp_returns = exp_returns # array of shape num_steps x num_stocks\n",
    "        self.Sigma_r = Sigma_r # array of shape num_stocks x num_stocks\n",
    "        \n",
    "        self.x_vals = asset_holdings # array of shape num_steps x num_stocks\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "                self.target_portf = target_portf\n",
    "                assert len(self.target_portf) == self.num_steps\n",
    "                \n",
    "        # allocate memory for coefficients of F- and G-functions\n",
    "        self.F_xx = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.F_x = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.F_0 = np.zeros(self.num_steps)\n",
    "        \n",
    "        self.Q_xx = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Q_uu = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Q_ux = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Q_x = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.Q_u = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.Q_0 = np.zeros(self.num_steps)\n",
    "        \n",
    "        # optimal actions for all assets (only used for the final step)\n",
    "        self.opt_action = np.zeros((self.num_steps, self.num_assets))\n",
    "        \n",
    "        # initialize time-dependent parameters of prior policy \n",
    "        self.u_bar_prior = np.zeros((self.num_steps,self.num_assets))\n",
    "        self.v_bar_prior =  np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Sigma_prior =  np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Sigma_prior_inv = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        \n",
    "        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix\n",
    "        for t in range(self.num_steps):\n",
    "            self.v_bar_prior[t,:,:] = 0.1 * np.eye(self.num_assets)\n",
    "            self.Sigma_prior[t,:,:] = 0.1 * np.eye(self.num_assets)\n",
    "            self.Sigma_prior_inv[t,:,:] = np.linalg.inv(self.Sigma_prior[t,:,:])\n",
    "        \n",
    "    def set_terminal_conditions(self):\n",
    "        \n",
    "        # set the terminal condition for the F-function\n",
    "        exp_ret_T = self.exp_returns[-1,:]\n",
    "        exp_ret_T_v = exp_ret_T[:, np.newaxis]\n",
    "        one_plus_exp_ret = np.ones(self.num_assets)[:,np.newaxis] + exp_ret_T_v\n",
    "        if self.use_for_WM:\n",
    "            Sigma_hat = self.Sigma_r + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "            Sigma_tilde = Sigma_hat + (self.eta/self.lambd)*np.eye(self.num_assets)\n",
    "            \n",
    "            # P_tilde is a column vector\n",
    "            self.P_tilde = (self.target_portf[-1]*one_plus_exp_ret - \n",
    "                            (1.0/(2*self.lambd))*np.ones(self.num_assets)[:,np.newaxis]\n",
    "                           )\n",
    "        \n",
    "            Sigma_tilde_inv = np.linalg.pinv(Sigma_tilde)\n",
    "            \n",
    "            # compute P_aux ~= P_tilde - Sigma_hat.dot(x)\n",
    "            P_aux_1 = Sigma_hat.dot(self.x_vals[-1,:])\n",
    "            P_aux = self.P_tilde - P_aux_1[:,np.newaxis]\n",
    "            \n",
    "            # the last action\n",
    "            self.opt_action[-1,:] = Sigma_tilde_inv.dot(P_aux.reshape(-1))\n",
    "            \n",
    "            # F_xx\n",
    "            self.F_xx[-1,:,:] = (- self.lambd*Sigma_hat.dot(np.eye(self.num_assets)\n",
    "                                                           - Sigma_tilde_inv.dot(Sigma_hat)))\n",
    "            # F_x\n",
    "            Sigma_hat_Sigma_til = Sigma_hat.dot(Sigma_tilde_inv)\n",
    "            One_minus_Sigma_hat_til = np.eye(self.num_assets) - Sigma_hat_Sigma_til\n",
    "            \n",
    "            self.F_x[-1,:] =  (Sigma_hat_Sigma_til.dot(np.ones(self.num_assets))+ \n",
    "                               2*self.lambd*self.target_portf[-1]*\n",
    "                               One_minus_Sigma_hat_til.dot(one_plus_exp_ret.reshape(-1)))\n",
    "            \n",
    "            # F_0\n",
    "            P_tilde_Sigma_aux = self.P_tilde.reshape(-1).dot(Sigma_tilde_inv)\n",
    "            Sigma_P_tilde_aux = Sigma_tilde_inv.dot(self.P_tilde.reshape(-1))\n",
    "            P_T = self.target_portf[-1]\n",
    "            \n",
    "            self.F_0[-1] = (P_tilde_Sigma_aux.dot(np.ones(self.num_assets))\n",
    "                            - self.lambd * P_tilde_Sigma_aux.dot(self.P_tilde.reshape(-1))\n",
    "                            + 2*self.lambd * one_plus_exp_ret.reshape(-1).dot(Sigma_P_tilde_aux)\n",
    "                            - self.lambd * P_T**2)\n",
    "            \n",
    "    def G_learning(self, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        find the optimal policy for the time dependent policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # set terminal conditions\n",
    "        self.set_terminal_conditions()\n",
    "        \n",
    "        # allocate iteration numbers for all steps\n",
    "        self.iter_counts = np.zeros(self.num_steps)\n",
    "        \n",
    "        # iterate over time steps backward\n",
    "        for t in range(self.num_steps-2,-1,-1):\n",
    "            self.step_G_learning(t, err_tol, max_iter)\n",
    "            \n",
    "    def step_G_learning(self, t, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        Perform one step of backward iteration for G-learning self-consistent equations\n",
    "        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)\n",
    "        \"\"\"\n",
    "            \n",
    "        # local variables\n",
    "        F_xx_tp1 = self.F_xx[t+1,:,:]\n",
    "        F_x_tp1 = self.F_x[t+1,:]\n",
    "        F_0_tp1 = self.F_0[t+1]\n",
    "            \n",
    "        # make matrix Sigma_hat_t\n",
    "        exp_ret_T = self.exp_returns[t,:]  \n",
    "        exp_ret_T_v = exp_ret_T[:, np.newaxis]\n",
    "        one_plus_exp_ret = np.ones(self.num_assets)[:,np.newaxis] + exp_ret_T_v\n",
    "        Sigma_hat_t = self.Sigma_r + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "            \n",
    "        # matrix A_t = diag(1 + r_bar_t)\n",
    "        A_t = np.diag(np.ones(self.num_assets) + exp_ret_T)\n",
    "                    \n",
    "        # update parameters of Q_function using next-step F-function values\n",
    "        self.update_Q_params(t, A_t,Sigma_hat_t)\n",
    "             \n",
    "        # iterate between policy evaluation and policy improvement  \n",
    "        while self.iter_counts[t] < max_iter:\n",
    "                \n",
    "            curr_u_bar_prior = self.u_bar_prior[t,:].copy() \n",
    "            curr_v_bar_prior = self.v_bar_prior[t,:,:].copy()    \n",
    "                \n",
    "            # compute parameters of F-function for this step from parameters of Q-function\n",
    "            self.update_F_params(t) \n",
    "              \n",
    "            # Policy iteration step: update parameters of the prior policy distribution\n",
    "            # with given Q- and F-function parameters\n",
    "            self.update_policy_params(t)    \n",
    "            \n",
    "            # difference between the current value of u_bar_prior and the previous one\n",
    "            err_u_bar = np.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)\n",
    "            \n",
    "            # divide by num_assets in err_v_bar to get both errors on a comparable scale\n",
    "            err_v_bar = (1/self.num_assets)*np.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)\n",
    "            \n",
    "            # choose the difference from the previous iteration as the maximum of the two errors\n",
    "            tol = 0.5*(err_u_bar + err_v_bar)\n",
    "                        \n",
    "            self.iter_counts[t] += 1\n",
    "            # Repeat the calculation of Q- and F-values\n",
    "            if tol <= err_tol:\n",
    "                break\n",
    "\n",
    "            \n",
    "    def update_Q_params(self,t, A_t,Sigma_hat_t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function\n",
    "        \"\"\" \n",
    "        \n",
    "        self.Q_xx[t,:,:] = ( - self.lambd * Sigma_hat_t \n",
    "                            + self.gamma*( (A_t.dot(self.F_xx[t+1,:,:])).dot(A_t)  \n",
    "                                           + self.Sigma_r * self.F_xx[t+1,:,:] ) )\n",
    "        \n",
    "        self.Q_ux[t,:,:] = 2 * self.Q_xx[t,:,:]\n",
    "        \n",
    "        self.Q_uu[t,:,:] = self.Q_xx[t,:,:] - self.eta * np.eye(self.num_assets)\n",
    "        \n",
    "        self.Q_x[t,:] = (self.lambd * self.target_portf[t] * A_t.dot(np.ones(self.num_assets))\n",
    "                             + self.gamma * A_t.T.dot(self.F_x[t+1,:]) )\n",
    "        \n",
    "        self.Q_u[t,:] = self.Q_x[t,:] - np.ones(self.num_assets)\n",
    "        \n",
    "        self.Q_0[t] = self.gamma * self.F_0[t+1] - self.lambd * self.target_portf[t]**2\n",
    "        \n",
    "        \n",
    "    def update_F_params(self,t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of F-function from t-parameters of G-function\n",
    "        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t\n",
    "        U_t = self.beta * self.Q_ux[t,:,:] + self.Sigma_prior_inv[t,:,:].dot(self.v_bar_prior[t,:,:])\n",
    "        W_t = self.beta * self.Q_u[t,:] +  self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])\n",
    "        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:] - 2 * self.beta * self.Q_uu[t,:,:]\n",
    "        Sigma_p_bar_inv = np.linalg.pinv(Sigma_p_bar)\n",
    "        \n",
    "        # update parameters of F-function\n",
    "        self.F_xx[t,:,:] = self.Q_xx[t,:,:] + (1/(2*self.beta))*(U_t.T.dot(Sigma_p_bar_inv.dot(U_t))\n",
    "                                    - self.v_bar_prior[t,:,:].T.dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].dot(self.v_bar_prior[t,:,:])))\n",
    "        \n",
    "        \n",
    "        self.F_x[t,:] = self.Q_x[t,:] + (1/self.beta)*(U_t.T.dot(Sigma_p_bar_inv.dot(W_t))\n",
    "                                    - self.v_bar_prior[t,:,:].T.dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])))\n",
    "        \n",
    "        \n",
    "        self.F_0[t] = self.Q_0[t] + ( (1/(2*self.beta))*(W_t.dot(Sigma_p_bar_inv.dot(W_t))\n",
    "                                    - self.u_bar_prior[t,:].dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])))\n",
    "                                    - (1/(2*self.beta)) * (np.log(np.linalg.det(self.Sigma_prior[t,:,:]))\n",
    "                                                       + np.log(np.linalg.det(Sigma_p_bar))) )\n",
    "        \n",
    "    def update_policy_params(self,t):\n",
    "        \"\"\"\n",
    "        update parameters of the Gaussian policy using current coefficients of the F- and G-functions\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Sigma_prior_inv[t,:,:] = self.Sigma_prior_inv[t,:,:] - 2 * self.beta * self.Q_uu[t,:,:]\n",
    "        Sigma_prior_new = np.linalg.pinv(self.Sigma_prior_inv[t,:,:])\n",
    "        \n",
    "        self.u_bar_prior[t,:] = Sigma_prior_new.dot(self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])\n",
    "                                              + self.beta * self.Q_u[t,:])\n",
    "        \n",
    "        self.v_bar_prior[t,:,:] = Sigma_prior_new.dot(self.Sigma_prior_inv[t,:,:].dot(self.v_bar_prior[t,:,:])\n",
    "                                              - self.beta * self.Q_ux[t,:,:])\n",
    "        \n",
    "        # assign the new inverse covariance for the prior for the next iteration\n",
    "        self.Sigma_prior[t,:,:] = Sigma_prior_new\n",
    "        \n",
    "        # also assign the same values for the previous time step\n",
    "        if t > 0:\n",
    "            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:]\n",
    "            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:]\n",
    "            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:]\n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using random and meaningless inputs\n",
    "\n",
    "num_steps = 10 \n",
    "num_assets = 100\n",
    "lambd = 1.0\n",
    "eta = 0.5\n",
    "beta = 0.2\n",
    "gamma = 0.95\n",
    "\n",
    "exp_returns = np.random.randn(num_steps,num_assets)\n",
    "Sigma_r = np.cov(exp_returns.T)\n",
    "asset_holdings = np.random.random_integers(low=1, high=10, size=(num_steps,num_assets))\n",
    "target_portf = np.linspace(start=1,stop=2, num = num_steps)\n",
    "\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                 lambd, \n",
    "                 eta,\n",
    "                 beta,\n",
    "                 gamma,                    \n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns\n",
    "                 asset_holdings, # array of shape num_steps x num_stocks\n",
    "                 use_for_WM = True, # use for wealth management tasks\n",
    "                 target_portf=target_portf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the G-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 12.161226 sec\n"
     ]
    }
   ],
   "source": [
    "err_tol=5.0e-5 # 1.0e-4\n",
    "max_iter=300\n",
    "\n",
    "t_0 = time.time()\n",
    "\n",
    "G_learner.G_learning(err_tol=err_tol, max_iter=max_iter)\n",
    "\n",
    "# print('Done in %d iterations'% G_learner.iter_count)\n",
    "print('Done in %f sec'% (time.time() - t_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([300., 300., 115., 106., 135., 146., 174., 146., 300.,   0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_learner.iter_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25966336,  0.49430843, -0.74774777, -0.59642827, -0.42921178,\n",
       "        -0.92598568, -0.36058915,  0.44650461],\n",
       "       [-0.49953031, -0.9611446 , -1.03911031, -0.39389504, -1.44409505,\n",
       "        -0.91085633, -0.10059318, -0.90631267]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_learner.u_bar_prior[6:8,0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute predicted cash installments for all steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1156cfef0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_t = np.zeros(num_steps)\n",
    "for t in range(num_steps):\n",
    "    c_t[t] = np.sum(G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:,:].dot(asset_holdings[t,:]))\n",
    "\n",
    "plt.plot(c_t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = np.array([0,1,2]).reshape((-1,1))\n",
    "\n",
    "ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try to modify the class, the initial copy is saved above\n",
    "\n",
    "\n",
    "class G_learning_portfolio_opt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_steps,\n",
    "                 lambd, \n",
    "                 Omega_mat, # of shape (num_risky_assets + 1)x(num_risky_assets + 1) # eta,\n",
    "                 beta,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky matrix\n",
    "                 asset_holdings, # array of shape num_steps x (num_stocks + 1)\n",
    "                 use_for_WM = True, # use for wealth management tasks\n",
    "                 target_portf=None):\n",
    "        \n",
    "        \n",
    "        self.num_steps = num_steps\n",
    "        self.lambd = lambd\n",
    "        \n",
    "        #self.eta = eta\n",
    "        self.Omega_mat = Omega_mat\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.use_for_WM = use_for_WM\n",
    "        \n",
    "        self.num_risky_assets = num_risky_assets\n",
    "        self.r_f = riskfree_rate\n",
    "        \n",
    "        self.num_assets = num_risky_assets + 1 # exp_returns.shape[1]\n",
    "        \n",
    "        assert exp_returns.shape[0] == self.num_steps\n",
    "        assert Sigma_r.shape[0] == Sigma_r.shape[1]\n",
    "        assert Sigma_r.shape[0] == num_risky_assets # self.num_assets\n",
    "        \n",
    "        self.Sigma_r = Sigma_r # array of shape num_stocks x num_stocks\n",
    "        \n",
    "        # arrays of returns for all assets including the risk-free asset\n",
    "        # array of shape num_steps x (num_stocks + 1) \n",
    "        self.exp_returns = np.hstack((self.r_f * np.ones(self.num_steps).reshape((-1,1)), exp_returns))\n",
    "                                      \n",
    "        # make block-matrix Sigma_r_tilde with Sigma_r_tilde[0,0] = 0, and equity correlation matrix inside\n",
    "        self.Sigma_r_tilde = np.zeros((self.num_assets, self.num_assets))\n",
    "        self.Sigma_r_tilde[1:,1:] = self.Sigma_r\n",
    "        \n",
    "        # asset_holdings is a vector for both the bond and all stocks\n",
    "        self.x_vals = asset_holdings # array of shape num_steps x (num_stocks+1)\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "                self.target_portf = target_portf\n",
    "                assert len(self.target_portf) == self.num_steps\n",
    "#       else:\n",
    "#           raise (RuntimeError, \"A target portfolio profile is required\")\n",
    "                \n",
    "        # allocate memory for coefficients of F- and G-functions\n",
    "        self.F_xx = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.F_x = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.F_0 = np.zeros(self.num_steps)\n",
    "        \n",
    "        self.Q_xx = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Q_uu = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Q_ux = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Q_x = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.Q_u = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.Q_0 = np.zeros(self.num_steps)\n",
    "        \n",
    "        # optimal actions for all assets (only used for the final step)\n",
    "        self.opt_action = np.zeros((self.num_steps,self.num_assets))\n",
    "        \n",
    "        # initialize time-dependent parameters of prior policy \n",
    "        self.u_bar_prior = np.zeros((self.num_steps,self.num_assets))\n",
    "        self.v_bar_prior =  np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Sigma_prior =  np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        self.Sigma_prior_inv = np.zeros((self.num_steps, self.num_assets, self.num_assets))\n",
    "        \n",
    "        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix\n",
    "        for t in range(self.num_steps):\n",
    "            self.v_bar_prior[t,:,:] = 0.1 * np.eye(self.num_assets)\n",
    "            self.Sigma_prior[t,:,:] = 0.1 * np.eye(self.num_assets)\n",
    "            self.Sigma_prior_inv[t,:,:] = np.linalg.inv(self.Sigma_prior[t,:,:])\n",
    "            \n",
    "        # cash installment for all steps\n",
    "        self.c_t = np.zeros(self.num_steps)\n",
    "    \n",
    "    def reward_fun(self, t, x_vals, u_vals, exp_rets, lambd, Sigma_hat):\n",
    "        \"\"\"\n",
    "        The reward function. Currently not used in the code, the reward enters directly in other functions \n",
    "        \"\"\"\n",
    "        x_plus = x_vals + u_vals\n",
    "        aux_1 = - self.lambd * self.target_portf[t]**2\n",
    "        aux_2 = - np.sum(u_vals)\n",
    "        aux_3 = 2*self.lambd * self.target_portf[t] * x_plus.dot(np.ones(num_assets) + exp_rets)\n",
    "        aux_4 = - self.lambd * x_plus.dot(Sigma_hat.dot(x_plus))\n",
    "        aux_5 = - u.dot(self.Omega_mat.dot(u))\n",
    "        \n",
    "        return aux_1 + aux_2 + aux_3 + aux_4 + aux_5  \n",
    "    \n",
    "    def set_terminal_conditions(self):\n",
    "        \"\"\"\n",
    "        set the terminal condition for the F-function\n",
    "        \"\"\"\n",
    "        \n",
    "        # auxiliary quantities to perform matrix calculations\n",
    "        exp_ret_T = self.exp_returns[-1,:]\n",
    "        exp_ret_T_v = exp_ret_T[:, np.newaxis]\n",
    "        one_plus_exp_ret = np.ones(self.num_assets)[:,np.newaxis] + exp_ret_T_v\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "            # Sigma_hat = self.Sigma_r + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "            Sigma_hat = self.Sigma_r_tilde + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "            \n",
    "            Sigma_hat_inv = np.linalg.pinv(Sigma_hat)\n",
    "            \n",
    "            #Sigma_tilde = Sigma_hat + (self.eta/self.lambd)*np.eye(self.num_assets)\n",
    "            Sigma_tilde = Sigma_hat + (1/self.lambd)*self.Omega_mat\n",
    "            \n",
    "            Sigma_tilde_inv = np.linalg.pinv(Sigma_tilde)\n",
    "            \n",
    "            Sigma_hat_sigma_tilde = Sigma_hat.dot(Sigma_tilde)\n",
    "            Sigma_tilde_inv_sig_hat = Sigma_tilde_inv.dot(Sigma_hat)\n",
    "            \n",
    "            Sigma_tilde_sigma_hat = Sigma_tilde.dot(Sigma_hat)\n",
    "            \n",
    "            Theta_m = np.eye(self.num_assets) - Sigma_tilde_inv.dot(Sigma_hat) \n",
    "            \n",
    "            P_T = self.target_portf[-1]\n",
    "    \n",
    "            # P_tilde is a column vector\n",
    "            P_tilde = P_T * one_plus_exp_ret - (1.0/(2*self.lambd))*np.ones(self.num_assets)[:,np.newaxis]\n",
    "                           \n",
    "            # compute P_aux \\equiv P_tilde - Sigma_hat.dot(x)\n",
    "            P_aux_1 = Sigma_hat.dot(self.x_vals[-1,:])\n",
    "            P_aux = P_tilde - P_aux_1[:,np.newaxis]\n",
    "            \n",
    "            # the last action\n",
    "            self.opt_action[-1,:] = Sigma_tilde_inv.dot(P_aux.reshape(-1))\n",
    "            \n",
    "            # though the action at the last step is deterministic, we can feed \n",
    "            # parameters of the prior with these values\n",
    "            self.u_bar_prior[-1,:] = Sigma_tilde_inv.dot(P_tilde.reshape(-1))\n",
    "            self.v_bar_prior[-1,:,:] = - Sigma_tilde_inv.dot(Sigma_hat)\n",
    "            \n",
    "            self.c_t[-1] = np.sum(self.opt_action[-1,:])\n",
    "            \n",
    "            # the coefficients of F-function for the last step\n",
    "            \n",
    "            # F_xx\n",
    "            self.F_xx[-1,:,:] = (- self.lambd * Theta_m.T.dot(Sigma_hat.dot(Theta_m))\n",
    "                                 - Sigma_tilde_inv_sig_hat.T.dot(self.Omega_mat.dot(Sigma_tilde_inv_sig_hat))\n",
    "                                 )\n",
    "                                 \n",
    "                                 \n",
    "            \n",
    "            # F_x\n",
    "            Sigma_hat_Sigma_tilde_inv = Sigma_hat.dot(Sigma_tilde_inv)\n",
    "            \n",
    "            aux_1x = Sigma_hat_Sigma_tilde_inv.dot(np.ones(self.num_assets))\n",
    "            aux_2x = 2*self.lambd * P_T * Theta_m.T.dot(one_plus_exp_ret.reshape(-1))\n",
    "            aux_3x = - 2*self.lambd * (Theta_m.T.dot(Sigma_hat_Sigma_tilde_inv.dot(P_tilde))).reshape(-1)\n",
    "            aux_4x = 2 * (Sigma_hat_Sigma_tilde_inv.dot(self.Omega_mat.dot(Sigma_tilde_inv.dot(P_tilde)))).reshape(-1)\n",
    "    \n",
    "    \n",
    "            self.F_x[-1,:] = aux_1x + aux_2x + aux_3x + aux_4x\n",
    "        \n",
    "            # F_0\n",
    "            P_tilde_Sigma_aux = P_tilde.reshape(-1).dot(Sigma_tilde_inv)\n",
    "            Sigma_P_tilde_aux =  Sigma_tilde_inv.dot(P_tilde.reshape(-1)) # P_tilde_Sigma_aux.T #\n",
    "            \n",
    "            \n",
    "            aux_0 = - self.lambd * P_T**2\n",
    "            aux_1 =  - P_tilde.T.dot(Sigma_tilde_inv.dot(np.ones(self.num_assets)))[0] # this is a matrix of size 1x1        \n",
    "            aux_2 = 2*self.lambd * P_T * one_plus_exp_ret.reshape(-1).dot(Sigma_P_tilde_aux)\n",
    "            aux_3 = - self.lambd * P_tilde_Sigma_aux.dot(Sigma_hat.dot(Sigma_P_tilde_aux))\n",
    "            aux_4 =  -  P_tilde_Sigma_aux.dot(self.Omega_mat.dot(Sigma_P_tilde_aux))\n",
    "    \n",
    "            #print('In set_terminal_conditions: shapes')\n",
    "            #print(aux_1.shape, aux_2.shape, aux_3.shape, aux_4.shape)\n",
    "            #print('aux_1 = ', aux_1)\n",
    "        \n",
    "            self.F_0[-1] =  aux_0 + aux_1 + aux_2 + aux_3 + aux_4\n",
    "            \n",
    "            #print('self.F_0[-1]: done')\n",
    "\n",
    "#             self.F_0[-1] = (- self.lambd * P_T**2 \n",
    "#                             - P_tilde.T.dot(Sigma_tilde_inv.dot(np.ones(self.num_assets)))\n",
    "#                             + 2*self.lambd * P_T * one_plus_exp_ret.reshape(-1).dot(\n",
    "#                                                     Sigma_P_tilde_aux.dot(P_tilde.reshape(-1)))\n",
    "#                             - self.lambd * P_tilde_Sigma_aux.dot(Sigma_hat.dot(Sigma_P_tilde_aux)) \n",
    "#                             -  P_tilde_Sigma_aux.dot(self.Omega_mat.dot(Sigma_P_tilde_aux))\n",
    "                           \n",
    "#                             )\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def G_learning(self, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        find the optimal policy for the time dependent policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print('Doing G-learning, it may take a few seconds...')\n",
    "        \n",
    "        # set terminal conditions\n",
    "        self.set_terminal_conditions()\n",
    "        \n",
    "        # allocate iteration numbers for all steps\n",
    "        self.iter_counts = np.zeros(self.num_steps)\n",
    "        \n",
    "        # iterate over time steps backward\n",
    "        for t in range(self.num_steps-2,-1,-1):\n",
    "            self.step_G_learning(t, err_tol, max_iter)\n",
    "            \n",
    "    def step_G_learning(self, t, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        Perform one step of backward iteration for G-learning self-consistent equations\n",
    "        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)\n",
    "        \"\"\"\n",
    "            \n",
    "        # local variables\n",
    "        F_xx_tp1 = self.F_xx[t+1,:,:]\n",
    "        F_x_tp1 = self.F_x[t+1,:]\n",
    "        F_0_tp1 = self.F_0[t+1]\n",
    "            \n",
    "        # make matrix Sigma_hat_t\n",
    "        exp_ret_T = self.exp_returns[t,:]  \n",
    "        exp_ret_T_v = exp_ret_T[:, np.newaxis]\n",
    "        one_plus_exp_ret = np.ones(self.num_assets)[:,np.newaxis] + exp_ret_T_v\n",
    "        \n",
    "        # Sigma_hat_t = self.Sigma_r + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "        Sigma_hat_t = self.Sigma_r_tilde + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "        \n",
    "        # matrix A_t = diag(1 + r_bar_t)\n",
    "        A_t = np.diag(np.ones(self.num_assets) + exp_ret_T)\n",
    "                    \n",
    "        # update parameters of Q_function using next-step F-function values\n",
    "        self.update_Q_params(t, A_t,Sigma_hat_t)\n",
    "             \n",
    "        # iterate between policy evaluation and policy improvement  \n",
    "        while self.iter_counts[t] < max_iter:\n",
    "                \n",
    "            curr_u_bar_prior = self.u_bar_prior[t,:].copy() \n",
    "            curr_v_bar_prior = self.v_bar_prior[t,:,:].copy()    \n",
    "                \n",
    "            # compute parameters of F-function for this step from parameters of Q-function\n",
    "            self.update_F_params(t) \n",
    "              \n",
    "            # Policy iteration step: update parameters of the prior policy distribution\n",
    "            # with given Q- and F-function parameters\n",
    "            self.update_policy_params(t)    \n",
    "            \n",
    "            # difference between the current value of u_bar_prior and the previous one\n",
    "            err_u_bar = np.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)\n",
    "            \n",
    "            # divide by num_assets in err_v_bar to get both errors on a comparable scale\n",
    "            err_v_bar = (1/self.num_assets)*np.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)\n",
    "            \n",
    "            # choose the difference from the previous iteration as the maximum of the two errors\n",
    "#             tol = np.minimum(err_u_bar, err_v_bar)\n",
    "#             tol = np.maximum(err_u_bar, err_v_bar)\n",
    "            tol = 0.5*(err_u_bar + err_v_bar)\n",
    "            \n",
    "            #print('err_u_bar, err_v_bar, tol', err_u_bar, err_v_bar, tol)\n",
    "            \n",
    "            self.iter_counts[t] += 1\n",
    "            # Repeat the calculation of Q- and F-values\n",
    "            if tol <= err_tol:\n",
    "                break\n",
    "                \n",
    "        self.c_t[t] = np.sum(self.u_bar_prior[t,:] + self.v_bar_prior[t,:,:].dot(self.x_vals[t,:]))\n",
    "\n",
    "            \n",
    "    def update_Q_params(self,t, A_t,Sigma_hat_t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function\n",
    "        \"\"\" \n",
    "        \n",
    "#         self.Q_xx[t,:,:] = ( - self.lambd * Sigma_hat_t \n",
    "#                             + self.gamma*( (A_t.dot(self.F_xx[t+1,:,:])).dot(A_t)  \n",
    "#                                            + self.Sigma_r * self.F_xx[t+1,:,:] ) )\n",
    "        \n",
    "        self.Q_xx[t,:,:] = ( - self.lambd * Sigma_hat_t \n",
    "                            + self.gamma*( (A_t.dot(self.F_xx[t+1,:,:])).dot(A_t)  \n",
    "                                           + self.Sigma_r_tilde * self.F_xx[t+1,:,:] ) )\n",
    "        \n",
    "        \n",
    "        self.Q_ux[t,:,:] = 2 * self.Q_xx[t,:,:]\n",
    "        \n",
    "        #self.Q_uu[t,:,:] = self.Q_xx[t,:,:] - self.eta * np.eye(self.num_assets)\n",
    "        self.Q_uu[t,:,:] = self.Q_xx[t,:,:] - self.Omega_mat \n",
    "        \n",
    "        self.Q_x[t,:] = (self.lambd * self.target_portf[t] * A_t.dot(np.ones(self.num_assets))\n",
    "                             + self.gamma * A_t.T.dot(self.F_x[t+1,:]) )\n",
    "        \n",
    "        self.Q_u[t,:] = self.Q_x[t,:] - np.ones(self.num_assets)\n",
    "        \n",
    "        self.Q_0[t] = self.gamma * self.F_0[t+1] - self.lambd * self.target_portf[t]**2\n",
    "        \n",
    "        \n",
    "    def update_F_params(self,t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of F-function from t-parameters of G-function\n",
    "        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t\n",
    "        U_t = self.beta * self.Q_ux[t,:,:] + self.Sigma_prior_inv[t,:,:].dot(self.v_bar_prior[t,:,:])\n",
    "        W_t = self.beta * self.Q_u[t,:] +  self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])\n",
    "        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:] - 2 * self.beta * self.Q_uu[t,:,:]\n",
    "        Sigma_p_bar_inv = np.linalg.pinv(Sigma_p_bar)\n",
    "        \n",
    "        # update parameters of F-function\n",
    "        self.F_xx[t,:,:] = self.Q_xx[t,:,:] + (1/(2*self.beta))*(U_t.T.dot(Sigma_p_bar_inv.dot(U_t))\n",
    "                                    - self.v_bar_prior[t,:,:].T.dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].dot(self.v_bar_prior[t,:,:])))\n",
    "        \n",
    "        \n",
    "        self.F_x[t,:] = self.Q_x[t,:] + (1/self.beta)*(U_t.T.dot(Sigma_p_bar_inv.dot(W_t))\n",
    "                                    - self.v_bar_prior[t,:,:].T.dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])))\n",
    "        \n",
    "        \n",
    "        self.F_0[t] = self.Q_0[t] + ( (1/(2*self.beta))*(W_t.dot(Sigma_p_bar_inv.dot(W_t))\n",
    "                                    - self.u_bar_prior[t,:].dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])))\n",
    "                                    - (1/(2*self.beta)) * (np.log(np.linalg.det(self.Sigma_prior[t,:,:]))\n",
    "                                                       + np.log(np.linalg.det(Sigma_p_bar))) )\n",
    "        \n",
    "    def update_policy_params(self,t):\n",
    "        \"\"\"\n",
    "        update parameters of the Gaussian policy using current coefficients of the F- and G-functions\n",
    "        \"\"\"\n",
    "        \n",
    "        new_Sigma_prior_inv = self.Sigma_prior_inv[t,:,:] - 2 * self.beta * self.Q_uu[t,:,:]\n",
    "        Sigma_prior_new = np.linalg.pinv(new_Sigma_prior_inv)\n",
    "        \n",
    "        \n",
    "        # update parameters using the previous value of Sigma_prior_inv\n",
    "        self.u_bar_prior[t,:] = Sigma_prior_new.dot(self.Sigma_prior_inv[t,:,:].dot(self.u_bar_prior[t,:])\n",
    "                                              + self.beta * self.Q_u[t,:])\n",
    "        \n",
    "        \n",
    "        self.v_bar_prior[t,:,:] = Sigma_prior_new.dot(self.Sigma_prior_inv[t,:,:].dot(self.v_bar_prior[t,:,:])\n",
    "                                              + self.beta * self.Q_ux[t,:,:])\n",
    "        \n",
    "        # and then assign the new inverse covariance for the prior for the next iteration\n",
    "        self.Sigma_prior[t,:,:] = Sigma_prior_new\n",
    "        self.Sigma_prior_inv[t,:,:] = new_Sigma_prior_inv\n",
    "        \n",
    "        # also assign the same values for the previous time step\n",
    "        if t > 0:\n",
    "            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:]\n",
    "            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:]\n",
    "            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted test\n",
    "\n",
    "# test using random and meaningless inputs\n",
    "\n",
    "num_steps = 5 # 10 # 2 # 5 # \n",
    "\n",
    "num_risky_assets = 99 # num_assets = 100 \n",
    "riskfree_rate = 0.02 # 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "exp_returns = np.random.randn(num_steps,num_risky_assets)\n",
    "Sigma_r = np.cov(exp_returns.T)\n",
    "\n",
    "\n",
    "# current portfolio: random values\n",
    "ref_val = 1\n",
    "x_vals = ref_val * np.ones((num_steps,num_assets))\n",
    "noise_coeff_p = 0.8\n",
    "noise_factors = np.random.uniform(low=1-noise_coeff_p, high=1+noise_coeff_p, size=(num_steps,num_assets))\n",
    "x_vals = noise_factors * x_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108.78850407 101.79869387 106.32500641  97.42720363 101.45957073]\n"
     ]
    }
   ],
   "source": [
    "portf_val = x_vals.sum(axis=1)\n",
    "print(portf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of transaction cost\n",
    "\n",
    "# eta = 0.5\n",
    "\n",
    "fee_bond = 0.05\n",
    "fee_stock = 0.1 # 1.0 # 100 # 1.0 # 0.5 \n",
    "\n",
    "# noticed much worse convergence for fee_stock = 0.2\n",
    "# it converges for fee_stock = 0.5, but it converges faster for fee_stock=1\n",
    "\n",
    "all_fees = np.zeros(num_risky_assets + 1)\n",
    "all_fees[0] = fee_bond\n",
    "all_fees[1:] = fee_stock\n",
    "Omega_mat = np.diag(all_fees)\n",
    "\n",
    "\n",
    "# model parameters\n",
    "lambd = 1.0 # 0.02 # 0.05 # 0.07 # 0.1 # 0.15 # 0.1 # 0.15 # 0.1 # 0.1 # 50 # 10.0\n",
    "beta = 0.5 # 1.0 # 50 # 50.0 # 1.0 # 10 # 10.0 # 0.2\n",
    "gamma = 0.95\n",
    "\n",
    "lambd = 10000 # 2000 # 0.01 # 0.05 # 0.5 # 0.1 # 1.0\n",
    "beta = 10 # 1.0 # 0.2 # 0.1 # 0.2\n",
    "\n",
    "# make target portfolio to be a fixed fraction above the current portfolio value\n",
    "coeff_target = 1.8 # 1.2 # 2.0 # 1.5 # 1.2\n",
    "portf_val = x_vals.sum(axis=1)\n",
    "target_portf = coeff_target * portf_val\n",
    "\n",
    "\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                 lambd, \n",
    "                 Omega_mat, # eta,\n",
    "                 beta,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky matrix                    \n",
    "                 x_vals, # array of shape num_steps x num_stocks\n",
    "                 use_for_WM = True, # use for wealth management tasks\n",
    "                 target_portf=target_portf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "Done in 3.528143 sec\n"
     ]
    }
   ],
   "source": [
    "# Do G-learning\n",
    "\n",
    "err_tol= 1.e-4 # 3.e-4 # 1e-4 # 3.5e-3 # 1.0e-4\n",
    "max_iter=500\n",
    "\n",
    "t_0 = time.time()\n",
    "\n",
    "G_learner.G_learning(err_tol=err_tol, max_iter=max_iter)\n",
    "\n",
    "print('Done in %f sec'% (time.time() - t_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([220., 100.,  70.,  91.,   0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_learner.iter_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute predicted cash installments for all steps\n",
    "\n",
    "c_t = np.zeros(num_steps)\n",
    "for t in range(num_steps):\n",
    "#     c_t[t] = np.sum(G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:,:].dot(x_vals[t,:]))\n",
    "    c_t[t] = G_learner.c_t[t]\n",
    "plt.plot(c_t, label='cash installment')\n",
    "plt.plot(x_vals.sum(axis=1),label='total portfolio value', color ='r')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('optimal cash installment vs total portfolio value')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-111.90728542,  -18.4227222 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare the target portfolio and total holdings\n",
    "\n",
    "total_holdings = x_vals.sum(axis=1)\n",
    "\n",
    "plt.plot(total_holdings,label='total_holdings')\n",
    "plt.plot(target_portf,label='target_portf',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('total_holdings vs target_portf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop intuition for a one-period setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100, 100)\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.5 # semi-annual steps\n",
    "vol_market_ann = 0.2 # annualized vol\n",
    "num_stocks = 99\n",
    "num_risky_assets = num_stocks\n",
    "num_assets = num_stocks + 1\n",
    "\n",
    "vol_market = vol_market_ann * np.sqrt(step_size)\n",
    "\n",
    "# use a diagonal covariance matrix\n",
    "Sigma_r = (vol_market**2) * np.eye(num_stocks)\n",
    "\n",
    "Sigma_r_tilde = np.zeros((num_assets, num_assets))\n",
    "Sigma_r_tilde[1:, 1:] = Sigma_r\n",
    "\n",
    "# risk-free rate\n",
    "r_f = 0.02\n",
    "\n",
    "fee_bond = 0.05 # 1.0 # 0.1\n",
    "fee_stock = 0.1 # 10.0 # 1.0 # 100 # 1.0 # 0.5 \n",
    "\n",
    "# noticed much worse convergence for fee_stock = 0.2\n",
    "# it converges for fee_stock = 0.5, but it converges faster for fee_stock=1\n",
    "\n",
    "all_fees = np.zeros(num_risky_assets + 1)\n",
    "all_fees[0] = fee_bond\n",
    "all_fees[1:] = fee_stock\n",
    "Omega_mat = np.diag(all_fees)\n",
    "\n",
    "# model parameters\n",
    "lambd = 2000 # 500 # 50.0 # 30.0 # 0.05 # 0.5 # 0.1 # 1.0\n",
    "beta = 0.2\n",
    "gamma = 0.95\n",
    "\n",
    "# current portfolio: random values\n",
    "# ref_val = 1\n",
    "# x_vals = ref_val * np.ones(num_assets)\n",
    "# noise_coeff_p = 0.8\n",
    "# noise_factors = np.random.uniform(low=1-noise_coeff_p, high=1+noise_coeff_p, size=num_assets)\n",
    "# x_vals = noise_factors * x_vals\n",
    "\n",
    "# take the values from the previous simulation\n",
    "x_vals_one_step = x_vals[-1,:]\n",
    "\n",
    "# make target portfolio to be a fixed fraction above the current portfolio value\n",
    "\n",
    "coeff_target = 1.2\n",
    "portf_val_one_step = np.sum(x_vals_one_step)\n",
    "target_portf_one_step = coeff_target * portf_val_one_step\n",
    "\n",
    "def reward_fun(x_vals, u, exp_rets, lambd, Sigma_hat, Omega_mat, target_portf):\n",
    "    x_plus = x_vals + u\n",
    "    aux_1 = - lambd * target_portf**2\n",
    "    aux_2 = - np.sum(u)\n",
    "    aux_3 = 2*lambd * target_portf * x_plus.dot(np.ones(num_assets) + exp_rets)\n",
    "    aux_4 = - lambd * x_plus.dot(Sigma_hat.dot(x_plus))\n",
    "    aux_5 = - u.dot(Omega_mat.dot(u))\n",
    "    \n",
    "    print(aux_1,aux_2,aux_3,aux_4,aux_5)\n",
    "    \n",
    "    return aux_1 + aux_2 + aux_3 + aux_4 + aux_5\n",
    "\n",
    "# make random returns\n",
    "\n",
    "# exp_rets = 0.05 * np.ones(num_stocks)\n",
    "# noise_coeff_r = 0.8\n",
    "# noise_factors = np.random.uniform(low=1-noise_coeff_r, high=1+noise_coeff_r, size=num_stocks)\n",
    "# exp_rets = noise_factors * exp_rets\n",
    "\n",
    "# use the values from the previous simulation\n",
    "exp_rets_one_step = exp_returns[-1,:]\n",
    "\n",
    "# add the risk-free rate\n",
    "exp_rets_one_step = np.hstack((np.array([r_f]), exp_rets_one_step))\n",
    "\n",
    "exp_ret_T_one_step = exp_rets_one_step \n",
    "exp_ret_T_v_one_step = exp_ret_T_one_step[:, np.newaxis]\n",
    "one_plus_exp_ret = np.ones(num_assets)[:,np.newaxis] + exp_ret_T_v_one_step\n",
    "\n",
    "# compute the optimal action\n",
    "\n",
    "Sigma_hat = Sigma_r_tilde + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "Sigma_tilde = Sigma_hat + (1/lambd)* Omega_mat\n",
    "            \n",
    "# P_tilde is a column vector\n",
    "P_tilde = target_portf_one_step * one_plus_exp_ret - (1.0/(2*lambd))*np.ones(num_assets)[:,np.newaxis]\n",
    "\n",
    "Sigma_tilde_inv = np.linalg.pinv(Sigma_tilde)\n",
    "            \n",
    "# compute P_aux \\equiv P_tilde - Sigma_hat.dot(x)\n",
    "P_aux_1 = Sigma_hat.dot(x_vals_one_step)\n",
    "P_aux = P_tilde - P_aux_1[:,np.newaxis]\n",
    "            \n",
    "# the optimal action\n",
    "u_opt = Sigma_tilde_inv.dot(P_aux.reshape(-1))\n",
    "\n",
    "x_vals_new = x_vals_one_step + u_opt\n",
    "\n",
    "new_port_val = np.sum(x_vals_new)\n",
    "\n",
    "optimal_cash_inflow = np.sum(u_opt)\n",
    "\n",
    "# calculate the optimal cash inflow in a different way\n",
    "u_bar_prior = Sigma_tilde_inv.dot(P_tilde.reshape(-1))# .reshape(-1)\n",
    "v_bar_prior = - Sigma_tilde_inv.dot(Sigma_hat) # .reshape(-1)\n",
    "\n",
    "print(u_bar_prior.shape, v_bar_prior.shape)\n",
    "\n",
    "optimal_cash_inflow_2 = np.sum(u_bar_prior + v_bar_prior.dot(x_vals_one_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-29646848.139696203 -5.195965498353644 59292465.16724633 -29645739.339028172 -443.69376585160046\n",
      "-29646848.139696203 -0.0 51159226.92660588 -22075185.978339173 -0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exp_rets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1049914dccc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          Omega_mat, target_portf_one_step)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exp_returns[0:5]:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_rets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'init portfolio value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportf_val_one_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_rets' is not defined"
     ]
    }
   ],
   "source": [
    "reward_opt = reward_fun(x_vals_one_step, u_opt, exp_rets_one_step, lambd, Sigma_hat, \n",
    "                        Omega_mat, target_portf_one_step)\n",
    "reward_zero = reward_fun(x_vals_one_step, np.zeros(num_assets), exp_rets_one_step, lambd, Sigma_hat, \n",
    "                         Omega_mat, target_portf_one_step)\n",
    "\n",
    "print('exp_returns[0:5]:', exp_rets[0:5])\n",
    "\n",
    "print('init portfolio value:', portf_val_one_step)\n",
    "print('target portfolio:', target_portf_one_step)\n",
    "print('optimal cash installment:',optimal_cash_inflow)\n",
    "print('optimal cash installment_2:',optimal_cash_inflow_2)\n",
    "print('new portfolio value:', new_port_val)\n",
    "print('optimal reward:', reward_opt)\n",
    "print('suboptimal reward:', reward_zero)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate portfolio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the market factor as a lognormal with a fixed drift and vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_market = 0.05\n",
    "vol_market = 0.25\n",
    "init_market_val = 100.0\n",
    "\n",
    "r_rf = 0.03  # risk-free rate - the first asset will be cash\n",
    "\n",
    "num_steps = 10 \n",
    "dt = 0.25 # quarterly time steps\n",
    "\n",
    "num_assets = 100\n",
    "\n",
    "returns_market = np.zeros(num_steps)\n",
    "market_vals = np.zeros(num_steps)\n",
    "market_vals[0] = 100.0  # initial value\n",
    "\n",
    "\n",
    "        \n",
    "for t in range(1,num_steps):\n",
    "\n",
    "        rand_norm = np.random.randn()\n",
    "        \n",
    "        # use log-returns of market as 'returns_market'\n",
    "        returns_market[t] = mu_market * dt + vol_market * np.sqrt(dt) * rand_norm\n",
    "        \n",
    "        market_vals[t] = market_vals[t-1] * np.exp((mu_market - 0.5*vol_market**2)*dt + \n",
    "                                                         vol_market*np.sqrt(dt)*rand_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(market_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate market betas and idiosyncratic alphas within pre-defined ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_min = 0.05\n",
    "beta_max = 0.85\n",
    "\n",
    "beta_vals = np.random.uniform(low=beta_min, high=beta_max, size=num_assets)\n",
    "\n",
    "alpha_min = - 0.05\n",
    "alpha_max = 0.15\n",
    "\n",
    "alpha_vals = np.random.uniform(low=alpha_min, high=alpha_max, size=num_assets)\n",
    "\n",
    "# Note: values beta_vals[0], alpha_vals[0] will be discarded as the first asset will be cash\n",
    "\n",
    "print(beta_vals[0:10])\n",
    "print(alpha_vals[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate time-dependent expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-independent expected returns would be equal to alpha + beta * expected_market_return \n",
    "# make them time-dependent (and correlated with actual returns) as alpha + beta * oracle_market_returns\n",
    "# oracle expected returns pick into the future! \n",
    "\n",
    "oracle_coeff = 0.2\n",
    "mu_vec = mu_market * np.ones(num_steps)\n",
    "oracle_market_returns = mu_vec * dt + oracle_coeff*(returns_market - mu_vec) \n",
    "\n",
    "expected_returns = np.zeros((num_steps, num_assets))\n",
    "\n",
    "for t in range(num_steps):\n",
    "    expected_returns[t,:] = alpha_vals * dt + beta_vals * oracle_market_returns[t]\n",
    "    # override the first value as the first asset is cash\n",
    "    expected_returns[t,0] = r_rf * dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(expected_returns[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial values of all assets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_min = 20.0\n",
    "val_max = 120.0\n",
    "\n",
    "init_asset_vals = np.random.uniform(low=val_min, high=val_max, size=num_assets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate realized returns and asset prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make realized returns and realized asset values by simulating from a one-factor model \n",
    "# with time-dependent expected returns\n",
    "\n",
    "asset_returns = np.zeros((num_steps, num_assets))\n",
    "asset_vals = np.zeros((num_steps, num_assets))\n",
    "\n",
    "idiosync_vol = 0.02 # vol_market # \n",
    "\n",
    "for t in range(num_steps):\n",
    "    \n",
    "    rand_norm = np.random.randn(num_assets)\n",
    "        \n",
    "    # asset returns are simulated from a one-factor model\n",
    "    asset_returns[t,:] = (expected_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "    # asset values\n",
    "    if t == 0:\n",
    "        asset_vals[t,:] = init_asset_vals\n",
    "    else:\n",
    "        asset_vals[t] = asset_vals[t-1] * (1 + asset_returns[t,:])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "print(expected_returns[t,0:6])\n",
    "print(beta_vals[0:6] * (returns_market[t] - mu_market*dt)) \n",
    "# print(beta_vals[0:6] * (returns_market[t])) \n",
    "print(idiosync_vol * (np.sqrt(1 - beta_vals[0:6]**2) * np.sqrt(dt) * rand_norm[0:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the realized and expected returns\n",
    "# Note that they appear correlated - this is because we constructed them so\n",
    "\n",
    "# pick asset to show\n",
    "asset_idx =  4 # 8 # 6 # 5\n",
    "\n",
    "plt.plot(expected_returns[:,asset_idx],label='expected_return')\n",
    "plt.plot(asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Realized returns vs expected returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the empirical correlation matrix using realized returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat_r = np.cov(asset_returns.T) # + shrink_coeff*np.eye(num_assets)\n",
    "\n",
    "print(cov_mat_r.shape)\n",
    "\n",
    "D,v = np.linalg.eigh(cov_mat_r)\n",
    "# help(np.linalg.eigh)\n",
    "\n",
    "eigenvals = D[::-1]  # put them in a descended order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the largest eigenvalue is the market factor \n",
    "eigenvals[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of eigenvalues\n",
    "\n",
    "n, bins, patches = plt.hist(x=eigenvals, bins=12, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "# n, bins, patches = plt.hist(x=eigenvals, bins='auto', color='#0504aa',\n",
    "#                             alpha=0.7, rwidth=0.85)\n",
    "\n",
    "\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "# x = np.arange(1,len(eigenvals)+1)\n",
    "# y = eigenvals\n",
    "# plt.scatter(x,y)\n",
    "plt.xlabel('Eigenvalues')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Eigenvalues of the synthetic covariance matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the initial portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider here two choices: an equally-weighted and price-weighted portfolio\n",
    "\n",
    "init_port_choice = 'equally_weighted'  # 'equal'\n",
    "\n",
    "init_cash = 1000.0\n",
    "init_total_asset = np.sum(init_asset_vals)\n",
    "\n",
    "# first allocate a 2D array for holdings\n",
    "x_vals = np.zeros((num_steps, num_assets))\n",
    "\n",
    "if init_port_choice == 'equally_weighted':\n",
    "    amount_per_asset = init_cash/init_total_asset\n",
    "    x_vals[0,:] = amount_per_asset * init_asset_vals\n",
    "    \n",
    "elif init_port_choice == 'equal': # 'price_weighted':\n",
    "    # hold equal amounts in each asset\n",
    "    amount_per_asset = init_cash/num_assets\n",
    "    x_vals[0,:] = amount_per_asset * np.ones(num_assets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the target portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a target portfolio term structure by defining it as the initial portfolio growing at some fixed and high rate\n",
    "\n",
    "target_portfolio = [init_cash]\n",
    "\n",
    "target_return = 0.1 # 0.20\n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    target_portfolio.append(target_portfolio[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "target_portfolio = 0.9*np.array(target_portfolio)    \n",
    "print(target_portfolio[0], target_portfolio[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make asset holding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # use model parameters for this simulation\n",
    "lambd = 1.0\n",
    "eta = 10.0 # 2.0 # 0.5\n",
    "beta = 0.2\n",
    "gamma = 0.99 # 0.95\n",
    "\n",
    "Sigma_r = cov_mat_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the analytic expression for LQG (Linear-Quadratic-G-learning) to obtain the holdings\n",
    "\n",
    "optimal_actions = np.zeros((num_steps, num_assets))\n",
    "suboptimal_actions = np.zeros((num_steps, num_assets))\n",
    "\n",
    "# suboptimal actions will be obtained by a uniform randomization of optimal actions\n",
    "noise_coeff_a = 0.15\n",
    "\n",
    "# use the previously created array x_vals for values obtained with optimal actions, \n",
    "# and also create an array for values obtained with sub-optimal actions\n",
    "\n",
    "x_subopt_vals = np.zeros((num_steps, num_assets))\n",
    "\n",
    "# seed with the same initial value\n",
    "x_subopt_vals[0,:] = x_vals[0,:]\n",
    "\n",
    "# prev_x_vals = x_vals[0,:]\n",
    "# prev_x_subopt_vals = x_subopt_vals[0,:]\n",
    "\n",
    "# forward run to create two simulated sets of state variables\n",
    "for t in range(1, num_steps):\n",
    "    exp_ret_t = expected_returns[t,:]\n",
    "    exp_ret_t_v = exp_ret_t[:, np.newaxis]\n",
    "    one_plus_exp_ret = np.ones(num_assets)[:,np.newaxis] + exp_ret_t_v\n",
    "    #if use_for_WM:\n",
    "    Sigma_hat = Sigma_r + one_plus_exp_ret.dot(one_plus_exp_ret.T)\n",
    "    Sigma_tilde = Sigma_hat + (eta/lambd)*np.eye(num_assets)\n",
    "            \n",
    "    # P_tilde is a column vector\n",
    "    P_tilde = (target_portfolio[t]*one_plus_exp_ret - \n",
    "                            (1.0/(2*lambd))*np.ones(num_assets)[:,np.newaxis]\n",
    "                           )\n",
    "        \n",
    "    Sigma_tilde_inv = np.linalg.pinv(Sigma_tilde)\n",
    "            \n",
    "    # compute P_aux \\equiv P_tilde - Sigma_hat.dot(x)\n",
    "    \n",
    "    # here by 'x' we should mean the current time-t value, which is obtained as \n",
    "    # (1+r_t dt) * (x_{t-1} + a_{t-1})\n",
    "    \n",
    "    P_aux_1 = Sigma_hat.dot(x_vals[t-1,:])\n",
    "    # P_aux_1 = Sigma_hat.dot(x_vals[t,:])\n",
    "    \n",
    "    P_aux = P_tilde - P_aux_1[:,np.newaxis]\n",
    "            \n",
    "    # the locally optimal action\n",
    "    optimal_actions[t,:] = Sigma_tilde_inv.dot(P_aux.reshape(-1))\n",
    "    \n",
    "    # randomize the optimal action\n",
    "    noise_factors = np.random.uniform(low=1-noise_coeff_a, high=1+noise_coeff_a, size=num_assets)\n",
    "    suboptimal_actions[t,:] = noise_factors * optimal_actions[t,:]\n",
    "    \n",
    "    # new values at time t+0\n",
    "    x_vals[t,:] = x_vals[t-1,:] + optimal_actions[t,:]\n",
    "    x_subopt_vals[t,:] = x_subopt_vals[t-1,:] + suboptimal_actions[t,:]\n",
    "    \n",
    "    # grow both using realized returns\n",
    "    realized_rets = asset_returns[t,:]\n",
    "    \n",
    "    x_vals[t,:] = (np.ones(num_assets) + dt * realized_rets) * x_vals[t,:]\n",
    "    x_subopt_vals[t,:] = (np.ones(num_assets) + dt * realized_rets)* x_subopt_vals[t,:]\n",
    "    \n",
    "    #x_vals[t,:] = (np.ones(num_assets) +  realized_rets) * x_vals[t,:]\n",
    "    #x_subopt_vals[t,:] = (np.ones(num_assets) + realized_rets)* x_subopt_vals[t,:]\n",
    "    \n",
    "    # that would serve as 'prev_x_vals' for the next step\n",
    "#     prev_x_vals = x_vals[t-1,:] \n",
    "#     prev_x_subopt_vals = x_subopt_vals[t-1,:] + suboptimal_actions[t,:]\n",
    "    \n",
    "    # pass these values as initial values for step t+1\n",
    "    \n",
    "print(x_vals[0,0:5])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the resulting holdings dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two representative holding profiles\n",
    "idx_1 = 3\n",
    "idx_2 = 4\n",
    "\n",
    "plt.plot(x_vals[:,idx_1],label='asset %d' % idx_1)\n",
    "plt.plot(x_vals[:,idx_2],label='asset %d' % idx_2)\n",
    "# plt.plot(asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Position values in different assets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the same position values look like for randomized actions?\n",
    "\n",
    "plt.plot(x_subopt_vals[:,idx_1],label='asset %d' % idx_1)\n",
    "plt.plot(x_subopt_vals[:,idx_2],label='asset %d' % idx_2)\n",
    "# plt.plot(asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Position values with randomized actions')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = 1 #0 #3\n",
    "idx_2 = 7 # 1 #4\n",
    "\n",
    "plt.plot(x_vals[:,idx_1],label='asset %d' % idx_1)\n",
    "plt.plot(x_vals[:,idx_2],label='asset %d' % idx_2)\n",
    "# plt.plot(asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Position values in different assets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train G-learning on simulated portfolio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# exp_returns = np.random.randn(num_steps,num_assets)\n",
    "# Sigma_r = np.cov(exp_returns.T)\n",
    "\n",
    "# asset_holdings = np.random.random_integers(low=1, high=10, size=(num_steps,num_assets))\n",
    "# target_portf = np.linspace(start=1,stop=2, num = num_steps)\n",
    "\n",
    "# create a G-learner\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                 lambd, \n",
    "                 eta,\n",
    "                 beta,\n",
    "                 gamma,                    \n",
    "                 expected_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns\n",
    "                 x_vals, # array of shape num_steps x num_stocks\n",
    "                 use_for_WM = True, # use for wealth management tasks\n",
    "                 target_portf=target_portfolio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do G-learning\n",
    "\n",
    "err_tol=1.e-4 # 5.0e-5 # 1.0e-4\n",
    "max_iter=500\n",
    "\n",
    "t_0 = time.time()\n",
    "\n",
    "# G_learner.step(t=num_steps-2,err_tol=1.0e-4, max_iter=50)\n",
    "G_learner.G_learning(err_tol=err_tol, max_iter=max_iter)\n",
    "\n",
    "# print('Done in %d iterations'% G_learner.iter_count)\n",
    "print('Done in %f sec'% (time.time() - t_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_learner.iter_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predicted cash installments for all steps\n",
    "\n",
    "c_t = np.zeros(num_steps)\n",
    "for t in range(num_steps):\n",
    "    c_t[t] = np.sum(G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:,:].dot(x_vals[t,:]))\n",
    "\n",
    "plt.plot(c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make array of expected returns of shape num_steps x num_assets\n",
    "\n",
    "num_steps = 10 \n",
    "num_assets = 12\n",
    "\n",
    "# np.random.seed(42)\n",
    "mu_last = 0.1 *np.random.rand(num_assets)\n",
    "\n",
    "# trim the data to avoid possibe outliers\n",
    "mu_min = - 0.20\n",
    "mu_max = 0.20\n",
    "\n",
    "idx_min = np.where(mu_last < mu_min)\n",
    "idx_max = np.where(mu_last > mu_max)\n",
    "\n",
    "if len(idx_min) > 0:\n",
    "    mu_last[idx_min] = mu_min\n",
    "if len(idx_max) > 0:\n",
    "    mu_last[idx_max] = mu_max\n",
    "    \n",
    "# now make a time series of expected returns by randomizing around these values across times\n",
    "\n",
    "mu_vals = np.zeros((num_steps, num_assets))\n",
    "for t in range(num_steps):\n",
    "    mu_vals[t,:] = mu_last*(np.ones(num_assets) + np.random.rand(num_assets))\n",
    "    \n",
    "# similarly trim the resulting values to trim possible outliers\n",
    "idx_min = np.where(mu_vals < mu_min)\n",
    "idx_max = np.where(mu_vals > mu_max)\n",
    "\n",
    "if len(idx_min) > 0:\n",
    "    mu_vals[idx_min] = mu_min\n",
    "if len(idx_max) > 0:\n",
    "    mu_vals[idx_max] = mu_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_vals[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make a covariance matrix of asset returns\n",
    "# make it by taking cov of realized values of mu\n",
    "\n",
    "# cov_mat = np.cov((0.1* np.ones((num_steps, num_assets)) + mu_vals).T)\n",
    "\n",
    "# shrink the covariance matrix of mu to a unit matrix to make it better conditioned\n",
    "shrink_coeff = 0.1\n",
    "\n",
    "cov_mat = (1 - shrink_coeff) * np.cov(mu_vals.T) + shrink_coeff*np.eye(num_assets)\n",
    "cov_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D,v = np.linalg.eigh(cov_mat)\n",
    "# help(np.linalg.eigh)\n",
    "\n",
    "eigenvals = D[::-1]  \n",
    "eigenvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D,v = np.linalg.eigh(cov_mat)\n",
    "# help(np.linalg.eigh)\n",
    "\n",
    "eigenvals = D[::-1]  # put them in a descended order\n",
    "\n",
    "# plot\n",
    "n, bins, patches = plt.hist(x=eigenvals, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "# x = np.arange(1,len(eigenvals)+1)\n",
    "# y = eigenvals\n",
    "# plt.scatter(x,y)\n",
    "plt.xlabel('Eigenvalues')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Eigenvalues of the synthetic covariance matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my old code:\n",
    "def GenerateNdLognormal(timeLine,\n",
    "                        numPaths,\n",
    "                        numFactors,\n",
    "                        initValues,\n",
    "                        vols,\n",
    "                        meanVals,\n",
    "                        corrMat):\n",
    "    \"\"\"\n",
    "    Simulate multivariate lognormal process\n",
    "    \"\"\"\n",
    "    \n",
    "    # First generate independent normal variates\n",
    "    # then mix them using Choleski decomposition\n",
    "    # then use these values to construct increments of individual components\n",
    "\n",
    "    numSteps = len(timeLine)\n",
    "    \n",
    "    aMat = sp.linalg.cholesky(corrMat)\n",
    "\n",
    "    sVals = sp.zeros((numSteps+1,numPaths,numFactors),'float')  # matrix of stock values\n",
    "    sVals[0,:,:] = tile(initValues.reshape(-1,1),(numPaths,1))        \n",
    "\n",
    "    #seed(500000)\n",
    "    randNorm = np.random.randn((numSteps,numPaths,numFactors)) # random numbers\n",
    "        \n",
    "    for i in range(1,numSteps):\n",
    "        thisRandNorm = np.dot(aMat,randNorm[i,:,:])\n",
    "        sVals[i,:,:] = sVals[i-1,:,:] * np.exp((self.mu - 0.5*self.vol**2)*self.dt + \n",
    "                                                         self.vol*np.sqrt(self.dt)*thisRandNorm[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # Number of scenarios\n",
    "vol = 0.2\n",
    "\n",
    "# use the covariance matrix we constructed to simulate correlated returns\n",
    "L = np.linalg.cholesky(cov_mat)\n",
    "\n",
    "# A = np.eye(num_assets) + 0.3*random.rand(num_assets ,num_assets)\n",
    "# B = np.dot(A,A.transpose())\n",
    "# L = np.linalg.cholesky(B)\n",
    "\n",
    "# use random initial values of assets\n",
    "S0 = 50*np.ones(num_assets) + 100.0 * np.random.rand(num_assets)\n",
    "\n",
    "# override values that are too small or negative\n",
    "min_S0 = 5\n",
    "bad_idx = np.where(S0 < min_S0)[0]\n",
    "if len(bad_idx) > 0:\n",
    "    S0[bad_idx] = S0\n",
    "\n",
    "# allocate matrix of realized returns    \n",
    "rets = np.array([0]*num_assets * N * num_steps, dtype='float64').reshape(num_steps, num_assets, N)\n",
    "\n",
    "for t in range(num_steps-1):\n",
    "    Z = np.random.randn(num_assets,N)\n",
    "    rets[t,:,:] = 0.1*(np.kron(mu_vals[t,:],np.ones(N)).reshape(num_assets, N) + vol * np.inner(L,Z.T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rets[:,0:10,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
