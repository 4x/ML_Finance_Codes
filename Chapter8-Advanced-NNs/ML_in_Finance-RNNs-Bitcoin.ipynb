{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKVCOxZOqjuP"
   },
   "outputs": [],
   "source": [
    "# Alpha_RNNs-Bitcoin\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.1 (27.2.2020)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6 and Tensorflow 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# M.F Dixon, Industrial Forecasting with Exponentially Smoothed Recurrent Neural Networks, preprint, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTkRUh4Nqjuk"
   },
   "source": [
    "## An Introduction to Prediction with RNNs\n",
    "\n",
    "### Overview\n",
    "- This notebook provides an example of how Keras can be used to train and test TensorFlow RNNs for time series prediction. The example dataset is for predicting from noisy, non-stationary data.\n",
    "- Statistical methods used for autoregressive models shall be used to identify the sequence length needed in the RNN and to diagnose the model error.\n",
    "- Plain RNNs are not suited to non-stationary time series modeling. We can use a GRU or LSTM to model non-stationary data, since these models exhibit dynamic auto-correlation structure.\n",
    "- Unlike classical time series methods, e.g. ARIMA, there are no parametric assumptions on the distribution of the errors, and non-linear relationships between response and predictors can be captured. \n",
    "- The data is snapshots of the USD value of Coinbase every minute over 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLVqutsCqjut"
   },
   "source": [
    "#### Statistician's note\n",
    "- We choose to build a model which provides strong predictive power, at the expense of reduced explanatory power. \n",
    "- Our choice to use a recurrent neural network is predicated on each observation in the time series being dependent on previous observations. The ordering of the observations therefore matters and $X$ is not iid.\n",
    "- Once the input data is appropriately scaled, model building starts with 'feature selection' - identifying the relevant features to include in the model. \n",
    "\n",
    "- In this notebook, we assume that we've already identifed the relevant set of features (i.e. there is only one time series provided).\n",
    "- Our primary concern is assessing the extent to which the model is over-fitting, by comparing the in- and out-of-sample MSEs.\n",
    "\n",
    "#### Implementation notes\n",
    "- It is important to ensure that `shuffle=False` in the fit function, otherwise the ordering of sequences is not preserved. This is especially important for methods which have memory beyond the current sequence (i.e. all methods except RNNs).\n",
    "- Time series cross-validation must be used for hyper-parameter tuning because the ordering of the data matters. In particular, the model must never use training data more recent than the forecasting observation date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "ZZ3LB7e6rwA4",
    "outputId": "ff696c1b-e20f-495b-9ccb-bece8d9b37e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## Only needed if the notebook is being run on Google Colab\\n\\n# mount the Google Drive\\nfrom google.colab import drive\\nmountpoint = '/content/drive'\\ndrive.mount(mountpoint, force_remount=True)\\n\\n# add the directory containing coinbase.csv, alphaRNN.py, \\n# & alphatRNN.py to the environment's PATH variable:\\nimport sys\\npath = mountpoint + '/My Drive/alpha-RNN' \\nsys.path.append(path)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Only needed if the notebook is being run on Google Colab\n",
    "\n",
    "# mount the Google Drive\n",
    "from google.colab import drive\n",
    "mountpoint = '/content/drive'\n",
    "drive.mount(mountpoint, force_remount=True)\n",
    "\n",
    "# add the directory containing coinbase.csv, alphaRNN.py, \n",
    "# & alphatRNN.py to the environment's PATH variable:\n",
    "import sys\n",
    "path = mountpoint + '/My Drive/alpha-RNN' \n",
    "sys.path.append(path)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JhA4PNWxqjuw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dez/anaconda3/envs/malfoy/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "import keras.initializers\n",
    "from keras.layers import Dense, Layer, LSTM, GRU, SimpleRNN, RNN\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# note that the directory containing these two .py's must be in the path variable:\n",
    "from alphaRNN import AlphaRNN\n",
    "from alphatRNN import AlphatRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exs-mpLQqju7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5QbSSsmqjvF"
   },
   "source": [
    "### Example Data\n",
    "- The example dataset $X$ is a chronologically ordered time series. The ordering of the observations matters and each observation is not assumed to be independent (as with cross-sectional classification data). \n",
    "\n",
    "- Each observation in $X$ has one variable (a.k.a. univariate time series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojxc_gL-qjvJ"
   },
   "source": [
    "Loading the data into a Pandas Dataframe, then viewing the first ten observations and the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1OUq236qjvN"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + '/coinbase.csv', index_col=[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "EIE0n8_OqjvX",
    "outputId": "f35903df-4e5b-4686-b2a5-3768118fdb32",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>13598.814422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:01:00</th>\n",
       "      <td>13596.912824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:02:00</th>\n",
       "      <td>13568.356998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:03:00</th>\n",
       "      <td>13560.425182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:04:00</th>\n",
       "      <td>13552.270931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:05:00</th>\n",
       "      <td>13552.279636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:06:00</th>\n",
       "      <td>13552.362423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:07:00</th>\n",
       "      <td>13570.273982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:08:00</th>\n",
       "      <td>13593.489523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:09:00</th>\n",
       "      <td>13614.862762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              USD\n",
       " timestamp                       \n",
       "2018-01-01 00:00:00  13598.814422\n",
       "2018-01-01 00:01:00  13596.912824\n",
       "2018-01-01 00:02:00  13568.356998\n",
       "2018-01-01 00:03:00  13560.425182\n",
       "2018-01-01 00:04:00  13552.270931\n",
       "2018-01-01 00:05:00  13552.279636\n",
       "2018-01-01 00:06:00  13552.362423\n",
       "2018-01-01 00:07:00  13570.273982\n",
       "2018-01-01 00:08:00  13593.489523\n",
       "2018-01-01 00:09:00  13614.862762"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains missing values; in order to prevent this causing errors, we replace these with adjacent values from the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662 observations are missing.\n",
      "This is 0.368% of the total.\n"
     ]
    }
   ],
   "source": [
    "nof_missing_values = sum(np.isnan(df['USD']))\n",
    "\n",
    "print(nof_missing_values, 'observations are missing.')\n",
    "print('This is {:.3f}% of the total.'.format(nof_missing_values*100/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gUXzLXEoqjv5",
    "outputId": "9b519613-55f8-44e7-d189-df14e481643a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 observations are missing.\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(method=\"backfill\")\n",
    "\n",
    "nof_missing_values = sum(np.isnan(df['USD']))\n",
    "\n",
    "print('Now', nof_missing_values, 'observations are missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xp8DNh3Lqjvj"
   },
   "source": [
    "# RNN Regression\n",
    "We consider a univariate prediction problem where the time series is given by 'USD' in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AruP1dVpqjvm"
   },
   "outputs": [],
   "source": [
    "use_features = ['USD'] # continuous input\n",
    "target = 'USD' # continuous output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgFn0_Zhqjvy"
   },
   "source": [
    "### Stationarity\n",
    "It is essential to determine whether the time series is \"stationary\". Informally, stationarity is when the auto-covariance is independent of time. Failure to establish stationarity will almost certainly lead to misinterpretation of model identification and diagnostic tests. Moreover, stationarity is decisive in characterizing the prediction problem and whether to use a more advanced architecture. In particular, we can expect a plain RNN to perform poorly if the data is non-stationary as the RNN exhibits fixed auto-covariance. \n",
    "\n",
    "We perform an Augmented Dickey-Fuller test to establish stationarity under the assumption that the time series has a constant bias but does not exhibit a time trend. In other words, we assume that the time series is already de-trended. \n",
    "\n",
    "If the stationarity test fails, even after first de-trending the time series, then one potential recourse is to simply take differences of time series and predict $\\Delta y_t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azMbPKuXqjv1"
   },
   "source": [
    "The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. If the p-value is above $(1-\\alpha)$, then we cannot reject that there is a unit root. Note that a subset of the time series is used to reduce the memory requirements of the test. We use the first 200000 samples to test for stationarity. While the test statistic is sensitive to the data size, the ADF test is always accepted at the 99\\% level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_DF3IABqjwA"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6beb8d220eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musedlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madfuller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muse_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/malfoy/lib/python3.6/site-packages/statsmodels/tsa/stattools.py\u001b[0m in \u001b[0;36madfuller\u001b[0;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mregresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             icbest, bestlag = _autolag(OLS, xdshort, fullRHS, startlag,\n\u001b[0;32m--> 257\u001b[0;31m                                        maxlag, autolag)\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             icbest, bestlag, alres = _autolag(OLS, xdshort, fullRHS, startlag,\n",
      "\u001b[0;32m~/anaconda3/envs/malfoy/lib/python3.6/site-packages/statsmodels/tsa/stattools.py\u001b[0m in \u001b[0;36m_autolag\u001b[0;34m(mod, endog, exog, startlag, maxlag, method, modargs, fitargs, regresults)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartlag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartlag\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmaxlag\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mmod_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"aic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/malfoy/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m                     hasattr(self, 'rank')):\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv_wexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingular_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpinv_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 self.normalized_cov_params = np.dot(\n\u001b[1;32m    292\u001b[0m                     self.pinv_wexog, np.transpose(self.pinv_wexog))\n",
      "\u001b[0;32m~/anaconda3/envs/malfoy/lib/python3.6/site-packages/statsmodels/tools/tools.py\u001b[0m in \u001b[0;36mpinv_extended\u001b[0;34m(X, rcond)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0ms_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/malfoy/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(df[use_features[0]][:2000].values) #¢# 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLjxzajyqjwJ"
   },
   "outputs": [],
   "source": [
    "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'.format(adf, p, nobs, cvs)\n",
    "print(adf_results_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxx3NIqxqjwT"
   },
   "source": [
    "Here we accept the null as the p-value is larger than 0.01, thus we can not reject the test at the 99% confidence level. This suggests that the time series is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Vo_X2UDqjwV"
   },
   "source": [
    "#### Autoregressive Model Identification: The partial auto-correlation\n",
    "It is important to determine the number of lags, the sequence length, required in the RNN by statistical analysis. A brute-force approach will in general be too time consuming.\n",
    "\n",
    "A partial auto-correlation at lag $h\\geq 2$ is a conditional auto-correlation between a variable, $X_t$, and its $h^{th}$ lag, $X_{t-h}$ under the assumption that we control for the values of the intermediate lags, $X_{t-1},\\dots, X_{t-h+1}$:\n",
    "\n",
    "$$\\tau_h:=\\tau(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1}):=\\frac{\\gamma(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1})}{\\sqrt{\\gamma(X_t |X_{t-1},\\dots, X_{t-h+1})\\gamma(X_{t-h} |X_{t-1},\\dots, X_{t-h+1}))}},\n",
    "$$ \n",
    "where $\\gamma_h:=\\gamma(X_tX_{t-h})$ is the lag-$h$ autocovariance. The partial autocorrelation function $\\tau_h:\\mathbb{N} \\rightarrow [-1,1]$ is a map $h:\\mapsto \\tau_h$.\n",
    "\n",
    "The estimated partial auto-correlation function (PACF) can be used to identify the order of an autoregressive time series model. Values of $|\\tau_h|$ greater or equal to $\\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{T}}$, where T is the number of observations and $\\Phi(z)$ is the standard normal CDF, are significant lag $h$ partial autocorelations at the $\\alpha$ confidence level.\n",
    "\n",
    "We use the stattools package to estimate the PACF. The 'nlags' parameter is the maximum number of lags used for PACF estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKLHw7tqqjwY"
   },
   "outputs": [],
   "source": [
    "pacf = sm.tsa.stattools.pacf(df[use_features], nlags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Phi^{-1}(0.99) \\simeq 2.58$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzsy_bjyqjwh"
   },
   "outputs": [],
   "source": [
    "T = len(df[use_features])\n",
    "\n",
    "sig_test = lambda tau_h: np.abs(tau_h) > 2.58/np.sqrt(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsQTxSq5qjwf"
   },
   "source": [
    "We find the first lag which isn't significant at the 99% level and automatically determine the number of lags needed in our autoregressive model as one below this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzsy_bjyqjwh"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pacf)):\n",
    "    if sig_test(pacf[i]) == False:\n",
    "        n_steps = i - 1\n",
    "        print('n_steps set to', n_steps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fnv19w5tqjwy"
   },
   "source": [
    "This may lead to a high order model, with more lags than strictly necessary. We could view this value, informally, as an upper bound on the number of lags needed. We can also simply identify the order of the model based on the plot of the PACF. In this case, a minimum of 4 lags appears satisfactory, although more may be needed. Unlike autoregressive models, the advantage of using fewer parameters is purely computational as adding more lags does not increase the number of parameters, only the size of the tensorial representation of the sequence data in TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "2YHK-wIIqjw1",
    "outputId": "f8c02cc7-2422-4a9e-9c80-1a641347fc54"
   },
   "outputs": [],
   "source": [
    "plt.plot(pacf, label='pacf')\n",
    "plt.plot([2.58/np.sqrt(T)]*30, label='99% confidence interval (upper)')\n",
    "plt.plot([-2.58/np.sqrt(T)]*30, label='99% confidence interval (lower)')\n",
    "plt.xlabel('number of lags')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHSnkymxqjw7"
   },
   "source": [
    "### Splitting the time series into training and testing sets\n",
    "Split the training and test set by using the first 80% of the time series and the remaining 20% for the test set. Note that the test set must be in the future of the training set to avoid look-ahead bias. Also, random sampling of the data can not be used as this would eliminate the auto-correlation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7D1y9USqjw9"
   },
   "outputs": [],
   "source": [
    "train_weight = 0.8\n",
    "split = int(len(df)*train_weight)\n",
    "\n",
    "df_train = df[use_features].iloc[:split]\n",
    "df_test = df[use_features].iloc[split:]\n",
    "print(len(df_test)/len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "JB3yo5YkqjxE"
   },
   "source": [
    "\n",
    "### Scaling\n",
    "Standardization of the data is important to avoid potential scaling difficulties in the fitting of the model. When there is more than one feature (covariate), scaling avoids one feature dominating over another due to disparate scales.\n",
    "\n",
    "To avoid introducing a look-ahead bias into the prediction, we must re-scale the training data without knowledge of the test set. Hence, we will simply standardize the training set using the mean and standard deviation of the training set and not the whole time series. Additionally, to avoid introducing a systematic bias into test set, we use the identical normalization for the test set - the mean and standard deviation of the training set are used to normalize the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2GbNeZ4qjxG"
   },
   "outputs": [],
   "source": [
    "# note that for a multivariate time series, you would need to scale \n",
    "# each variable by its own mean and standard deviation in the training set\n",
    "mu = np.float(df_train.mean())\n",
    "sigma = np.float(df_train.std())\n",
    "\n",
    "stdize_input = lambda x: (x - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxQ09iVxqjxL"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.apply(stdize_input)\n",
    "\n",
    "df_test = df_test.apply(stdize_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ooDbr1Giqjxa"
   },
   "source": [
    "### Data formatting for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80KLxffJqjxd"
   },
   "source": [
    "TensorFlow uses tensors to represent data. To perform sequence learning, the time series of variables must be transformed to a series of over-lapping sub-sequences. \n",
    "\n",
    "For example, consider a univariate time series of increasing integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk_D3SrKqjxe"
   },
   "source": [
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avKQL4MWqjxg"
   },
   "source": [
    "Setting the sequence length to 10, for example, we move the window forward by one observation at a time and construct new sequences:\n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10\n",
    "\n",
    "2 3 4 5 6 7 8 9 10 11\n",
    "\n",
    "3 4 5 6 7 8 9 10 11 12\n",
    "\n",
    "4 5 6 7 8 9 10 11 12 13\n",
    "\n",
    "5 6 7 8 9 10 11 12 13 14\n",
    "\n",
    "6 7 8 9 10 11 12 13 14 15\n",
    "\n",
    "\n",
    "Let's define the following function for reshaping the data into one-step ahead times series prediction format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPCndJ8sqjxj"
   },
   "outputs": [],
   "source": [
    "def get_lagged_features(value, n_steps, n_steps_ahead):\n",
    "    \"\"\"\n",
    "    value: feature value to be lagged\n",
    "    n_steps: number of lags, i.e. sequence length\n",
    "    n_steps_ahead: forecasting horizon\n",
    "    \"\"\"\n",
    "    lag_list = []\n",
    "    for lag in range(n_steps+n_steps_ahead-1, n_steps_ahead-1, -1):\n",
    "        lag_list.append(value.shift(lag))\n",
    "    return pd.concat(lag_list, axis=1)\n",
    "\n",
    "get_lagged_features(pd.DataFrame([1,3,5,7,2,4,6,8]), 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOk1scB1qjxn"
   },
   "source": [
    "We shall first transform the training input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMP_lraNqjxq"
   },
   "outputs": [],
   "source": [
    "n_steps_ahead=4\n",
    "\n",
    "x_train_list = []\n",
    "for use_feature in use_features:\n",
    "    x_train_reg = get_lagged_features(df_train, n_steps, n_steps_ahead).dropna()\n",
    "    x_train_list.append(x_train_reg)\n",
    "x_train_reg = pd.concat(x_train_list, axis=1)\n",
    "\n",
    "col_ords = []\n",
    "for i in range(n_steps):\n",
    "    for j in range(len(use_features)):\n",
    "        print(i + j * n_steps)\n",
    "        col_ords.append(i + j * n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMP_lraNqjxq"
   },
   "outputs": [],
   "source": [
    "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
    "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
    "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
    "y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMP_lraNqjxq"
   },
   "outputs": [],
   "source": [
    "x_train_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMP_lraNqjxq"
   },
   "outputs": [],
   "source": [
    "x_test_list = []\n",
    "for use_feature in use_features:\n",
    "    x_test_reg = get_lagged_features(df_test, n_steps,n_steps_ahead).dropna()\n",
    "    x_test_list.append(x_test_reg)\n",
    "x_test_reg = pd.concat(x_test_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMP_lraNqjxq"
   },
   "outputs": [],
   "source": [
    "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
    "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
    "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
    "\n",
    "y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "av-Dyaweqjxw"
   },
   "source": [
    "Print the shapes of each tensor. The first digit is the number of observations. For feature arrays, the second digit is the sequence length (i.e. the number of lags in the model) and the final digit is the dimension of each element in the sequence or output vector respectively. In this case, the example performs univariate time series analysis and so the dimension of the input and output is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VbIMhSYYqjxx",
    "outputId": "01222a29-7331-4520-f2c8-9d3f3f141a51"
   },
   "outputs": [],
   "source": [
    "print(x_train_reg.shape,y_train_reg.shape,x_test_reg.shape,y_test_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T870gAP2qjx3"
   },
   "outputs": [],
   "source": [
    "train_batch_size = y_train_reg.shape[0]\n",
    "test_batch_size = y_test_reg.shape[0]\n",
    "time_size = y_train_reg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgJidF_6qjx9"
   },
   "outputs": [],
   "source": [
    "x_train_reg = pd.concat(x_train_list, axis=1)\n",
    "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
    "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
    "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AsUwPwYhqjyF"
   },
   "outputs": [],
   "source": [
    "x_test_reg = pd.concat(x_test_list, axis=1)\n",
    "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
    "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
    "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aieEXctCqjyK"
   },
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBFnFLZZqjyM"
   },
   "outputs": [],
   "source": [
    "def AlphatRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(AlphatRNN(n_units, activation='tanh', recurrent_activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def AlphaRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(AlphaRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def SimpleRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(SimpleRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def GRU_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(GRU(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def LSTM_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True)) \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Bmr9nNVqjyR"
   },
   "outputs": [],
   "source": [
    "max_epoches=2000\n",
    "batch_size=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGCWFF1-qjyW"
   },
   "source": [
    "Use a batch size of 1000 as the dataset is reasonably large and the training time would be too long otherwise. 20 epoches have been used here, but a better approach would be to use a stopping criteria through a call back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrDjZpVdqjyZ"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=100, min_delta=1e-7, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jr_42Ab_qjye"
   },
   "outputs": [],
   "source": [
    "params = {'rnn': {'model':'', 'function':SimpleRNN_, 'l1_reg':0.0, 'H':20, 'color': 'blue', 'label':'RNN'}, \n",
    "          'alpharnn': {'model':'', 'function':AlphaRNN_,'l1_reg':0.0, 'H':10,'color': 'green', 'label': '$\\\\alpha$-RNN' }, \n",
    "          'alphatrnn': {'model':'', 'function':AlphatRNN_,'l1_reg':0.0, 'H':5, 'color': 'cyan', 'label': '$\\\\alpha_t$-RNN'},\n",
    "          'gru': {'model':'', 'function':GRU_,'l1_reg':0.0, 'H':10, 'color': 'orange', 'label': 'GRU'},\n",
    "          'lstm': {'model':'', 'function':LSTM_,'l1_reg':0.0, 'H':10, 'color':'red', 'label': 'LSTM'}\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKcoWfmHqjyi"
   },
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dM8rOT-rqjyj"
   },
   "outputs": [],
   "source": [
    "cross_val=False # Warning: Changing this to True will take several hours to run\n",
    "if cross_val:\n",
    "    n_units = [5,10,20]\n",
    "    l1_reg = [0, 0.001, 0.01, 0.1]\n",
    "    tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    param_grid = dict(n_units=n_units,l1_reg=l1_reg)\n",
    "\n",
    "    for key in params.keys(): # params[key]['function']\n",
    "        model = KerasRegressor(build_fn=params[key]['function'], epochs=max_epoches, batch_size=batch_size, verbose=2)\n",
    "        grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=1, verbose=2)\n",
    "        grid_result = grid.fit(x_train_reg[:30000],y_train_reg[:30000],callbacks=[es])\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        params_ = grid_result.cv_results_['params']\n",
    "        for mean, stdev, param_ in zip(means, stds, params_):\n",
    "            print(\"%f (%f) with %r\" % (mean, stdev, param_))\n",
    "\n",
    "        params[key]['H'] = grid_result.best_params_['n_units']\n",
    "        params[key]['l1_reg']= grid_result.best_params_['l1_reg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BqEXCGyoqjyo"
   },
   "source": [
    "# Train cross-validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0FpTqNWqjyq"
   },
   "outputs": [],
   "source": [
    "for key in params.keys():\n",
    "    tf.set_random_seed(0)\n",
    "    model=params[key]['function'](params[key]['H'],params[key]['l1_reg'])\n",
    "    model.fit(x_train_reg,y_train_reg,epochs=max_epoches, batch_size=batch_size,callbacks=[es],shuffle=False)\n",
    "    params[key]['model']=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQmA1vK1mbCM"
   },
   "outputs": [],
   "source": [
    "# optionally save the fitted model\n",
    "for key in params.keys():\n",
    "  params[key]['model'].save(key + '.hdf5', overwrite=True)  # creates a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyLz5ik1ANCv"
   },
   "outputs": [],
   "source": [
    "# optionally load the fitted model\n",
    "for key in params.keys():\n",
    "  params[key]['model']=load_model(key + '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9OE7I07dEhV"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fC3xzTJ4cHG4",
    "outputId": "1e84ce92-289c-4e07-fe25-c3f5b4a01926"
   },
   "outputs": [],
   "source": [
    "model = params['alpharnn']['model']\n",
    "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
    "weights = model.get_weights()\n",
    "\n",
    "for name, weight in zip(names, weights):\n",
    "    if name =='alpha_rnn_1/alpha:0':\n",
    "      print(\"alpha= \" + str(sigmoid(weight)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1gZjjPgqjy3"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEhlyKjSqjy4"
   },
   "source": [
    "We shall now apply the fitted RNN model to the training set and the test set, separately. We can then informally assess the extent\n",
    "of over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "t3a1F-Y8qjy5",
    "outputId": "c183b29d-0f1b-48b0-859d-27e73ac8f919"
   },
   "outputs": [],
   "source": [
    "for key in params.keys():\n",
    "  model=params[key]['model']\n",
    "  model.summary()\n",
    "    \n",
    "  params[key]['MSE_train']= mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:],model.predict(x_train_reg, verbose=1))\n",
    "  params[key]['predict'] = model.predict(x_test_reg, verbose=1) \n",
    "  params[key]['MSE_test']= mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:],params[key]['predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I99U7i80qjy-"
   },
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OjslcI9Aqjy_"
   },
   "source": [
    "We observe the in-sample prediction is reliable (upper plot). Most of the out-of-sample prediction (lower plot) is also reliable although the end of the predicted sequence appears to degrade. Such a degradation indicates the prediction horizon might be too long and the model should be retrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_ADoAkeqjzB"
   },
   "source": [
    "Finally assess the performance of the model in and out-of-sample using the MSE. We expect the mean error to be larger on the test set than on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "colab_type": "code",
    "id": "f67u-snqqjzD",
    "outputId": "ca4f8a64-3780-4cc1-a3f5-b738606f5c77"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "upper = 2000\n",
    "plt.plot(df_test.index[n_steps+n_steps_ahead-1:upper], df_test[use_feature][n_steps+n_steps_ahead-1:upper], color=\"black\", label=\"Observed\")\n",
    "\n",
    "for key in params.keys():\n",
    "  print(key)\n",
    "  plt.plot(df_test.index[n_steps+n_steps_ahead-1:upper], params[key]['predict'][:upper-(n_steps+n_steps_ahead-1), 0], color=params[key]['color'], label=params[key]['label'])\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.ylabel('Y', fontsize=20)\n",
    "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "XSX5YYRCqjzH",
    "outputId": "99f4a429-9643-4e42-ea49-8efa6bf190df"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "\n",
    "for key in params.keys():\n",
    "   plt.plot(df_test.index[n_steps+n_steps_ahead-1:upper], df_test[use_feature][n_steps+n_steps_ahead-1:upper]-params[key]['predict'][:upper-(n_steps+n_steps_ahead-1), 0], color=params[key]['color'], label=params[key]['label'] + \" (\" +  str(round(params[key]['MSE_test'],7)) +\")\")\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Training)', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.ylabel('Y-$\\hat{Y}$', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "6c_adfrzqjzM"
   },
   "source": [
    "### Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGHvu0xVqjzO"
   },
   "source": [
    "A fitted time series model must be examined for underfitting with a white noise test. We analyze the model residuals (i.e. the error $u_t$) to determine whether it is white noise or whether it is auto-correlated. The latter case provides statistical evidence that more lags are needed in the RNN. Box and Pierce propose the Portmanteau statistic:\n",
    "\n",
    "$$Q^*(m)=T\\sum_{l=1}^m\\hat{\\tau}_l^2,$$\n",
    "\n",
    "as a test statistic for the null hypothesis $$H_0:\\tau_1=\\dots=\\tau_m=0$$ against the alternative hypothesis $$H_a:\\tau_i\\neq 0$$ for some $$i\\in\\{1,\\dots,m\\}$$, where $T$ is the number of observations, $\\hat{\\tau}_i$ are the sample autocorrelations of the residual, and $m$ is the maximum lag used in the test. There are several heuristics in the statistics literature to determine the maximum lag such as the Schwert statistic. \n",
    "\n",
    "The Box-Pierce statistic follows an asymptotically chi-squared distribution with $m$ degrees of freedom.\n",
    "\n",
    "The Ljung-Box test statistic increases the power of the test in finite samples:\n",
    "$$Q(m)=T(T+2)\\sum_{l=1}^m\\frac{\\hat{\\tau}_l^2}{T-l}$$.\n",
    "\n",
    "This statistic also follows an asymptotically chi-squared distribution with $m$ degrees of freedom. The decision rule is to reject $H_0$ if $Q(m)>\\chi_{\\alpha}^2$ where $\\chi_{\\alpha}^2$ denotes the $100(1-\\alpha)^{th}$ percentile of a chi-squared distribution with m degrees of freedom and is the significance level for rejecting $H_0$.\n",
    "\n",
    "The test can be time consuming and we select a subset of the residuals. Here we simply set the maximum lag to 20. In the results below, we find that the p-values are all smaller than 0.01, indicating that we can reject the null at the 99% confidence level for any lag. This is strong evidence that the model is under-fitting and more lags are needed in our model. Unlike an auto-regressive model, increasing the number of lags in the RNN does not increase the number of weights. Thus there is no danger of over-fitting by increasing the lag, although there will be an increase in the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bpznKjiqjzQ"
   },
   "outputs": [],
   "source": [
    "T=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIPFOuKAqjzV"
   },
   "outputs": [],
   "source": [
    "residual=df_train[use_feature][n_steps:(n_steps+T)].values-(sigma*pred_train[:T, 0]+mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-radzroqjzb"
   },
   "outputs": [],
   "source": [
    "lb,p=sm.stats.diagnostic.acorr_ljungbox(residual, lags=20, boxpierce=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWvgE9ouqjzf"
   },
   "source": [
    "The Box-Ljung test statistics are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rn0gmXkDqjzg",
    "outputId": "517efe22-705e-420e-f8ed-e3d4f0180835"
   },
   "outputs": [],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0MmBUaDqjzk"
   },
   "source": [
    "The p-values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTEIIp-Sqjzn",
    "outputId": "15031561-6612-43b6-99c0-dafa4a5c034e"
   },
   "outputs": [],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_in_Finance-RNNs-Bitcoin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
