{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_in_Finance-RNNs-Coinbase\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.1 (27.2.2020)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6 and Tensorflow 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Dixon M.F, Halperin, I. and P. Bilokon, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Prediction with RNNs\n",
    "\n",
    "### Overview\n",
    "- This notebook provides an example of how TensorFlow can be used to train and test a RNNs for time series prediction. The example dataset is for predicting from noisy non-stationarity data.\n",
    "- Statistical methods used for autoregressive models shall be used to identify the sequence length needed in the RNN and to diagnose the model error.\n",
    "- Plain RNNs are not suited to non-stationary time series modeling. We can use a GRU or LSTM to model non-stationarity data, since these models exhibit dynamic auto-correlation structure.\n",
    "- Unlike classical time series methods, e.g. ARIMA, there are no parameteric assumptions on the distribution of the errors, and non-linear relationships between response and predictors can be captured without over-fitting. \n",
    "- The data is one minute snapshots of the USD value of coinbase over 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistician's note\n",
    "- We choose to build a model which provides strong predictive power, in favor of explantory power. \n",
    "- Our choice to use a recurrent neural network is predicated on each observation in the time series being dependent on previous observations. The ordering of the observations therefore matters and $X$ is not iid.\n",
    "- Once the input data is appropriately scaled, model building starts with 'feature selection' -identifying the relevant features to include in the model. \n",
    "- In this tutorial, we assume that we've already identifed the relevant set of features (i.e. there is only one time series provided).\n",
    "- Our primary concern in assessing the extent to which the model is over-fitting by comparing the in and out-of-sample MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Layer, LSTM, GRU, SimpleRNN\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l1,l2\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.layers import Layer,RNN\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "import keras.layers\n",
    "from keras.legacy import interfaces\n",
    "from keras import *\n",
    "from alphaRNN import *\n",
    "from alphatRNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Data\n",
    "- The example dataset is a chronologically ordered time series. The ordering of the observations matters and each observation is not assumed to be independent (as with cross-sectional classification data). \n",
    "\n",
    "- Each observation $X$ has one variable (a.k.a. univariate time series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pandas Dataframe, viewing the first ten rows and the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/coinbase.csv', index_col=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>13598.814422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:01:00</th>\n",
       "      <td>13596.912824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:02:00</th>\n",
       "      <td>13568.356998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:03:00</th>\n",
       "      <td>13560.425182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:04:00</th>\n",
       "      <td>13552.270931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              USD\n",
       " timestamp                       \n",
       "2018-01-01 00:00:00  13598.814422\n",
       "2018-01-01 00:01:00  13596.912824\n",
       "2018-01-01 00:02:00  13568.356998\n",
       "2018-01-01 00:03:00  13560.425182\n",
       "2018-01-01 00:04:00  13552.270931"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Regression\n",
    "We consider a univariate prediction problem where the time series is given by 'USD' in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = ['USD'] # continuous input\n",
    "target = 'USD' # continuous output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity\n",
    "It is essential to determine whether the time series is \"stationary\". Informally, stationarity is when the auto-covariance is independent of time. Failure to establish stationarity will almost certainly lead to misinterpretation of model identification and diagnostics tests. Moreover, stationarity is decisive in characterizing the prediction problem and whether to use a more advanced architecture. In particular, we can expect a plain RNN to perform poorly if the data is non-stationary as the RNN exhibits fixed auto-covariance. \n",
    "\n",
    "We perform an Augmented Dickey-Fuller test to establish stationarity under the assumption that the time series has a constant bias but does not exhibit a time trend. In other words, we assume that the time series is already de-trended. \n",
    "\n",
    "If the stationarity test fails, even after first de-trending the time series, then one potential recourse is to simply take differences of time series and predict $\\Delta y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. If the p-value is above $(1-\\alpha)$, then we cannot reject that there is a unit root. Note that a subset of the time series is used to reduce the computation time of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(method=\"backfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs,aic=sm.tsa.stattools.adfuller(df[use_features[0]][:200000].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf,p, nobs, cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we accept the null as the p-value is larger than 0.01, thus we can not reject the test at the 99% confidence level. This suggests that the time series is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive Model Identification: The partial auto-correlation\n",
    "It is important to determine the number of lags, the sequence length, required in the RNN by statistical analysis. A brute-force approach will in general be too time consuming.\n",
    "\n",
    "A partial auto-correlation at lag $h\\geq 2$ is a conditional auto-correlation between a variable, $X_t$, and its $h^{th}$ lag, $X_{t-h}$ under the assumption that we control for the values of the intermediate lags, $X_{t-1},\\dots, X_{t-h+1}$:\n",
    "\n",
    "$$\\tau_h:=\\tau(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1}):=\\frac{\\gamma(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1})}{\\sqrt{\\gamma(X_t |X_{t-1},\\dots, X_{t-h+1})\\gamma(X_{t-h} |X_{t-1},\\dots, X_{t-h+1}))}},\n",
    "$$ \n",
    "where $\\gamma_h:=\\gamma(X_tX_{t-h})$ is the lag-$h$ autocovariance. The partial autocorrelation function $\\tau_h:\\mathbb{N} \\rightarrow [-1,1]$ is a map $h:\\mapsto \\tau_h$.\n",
    "\n",
    "The estimated partial auto-correlation function (PACF) can be used to identify the order of an autoregressive time series model. Values of $|\\tau_h|$ greater or equal to $\\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{T}}$, where T is the number of observations and $\\Phi(z)$ is the standard normal CDF, are significant lag $h$ partial autocorelations at the $\\alpha$ confidence level.\n",
    "\n",
    "We use the stattools package to estimat the PACF. The 'nlags' parameter is the maximum number of lags used for PACF estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf=sm.tsa.stattools.pacf(df[use_features], nlags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the first lag which isn't significant at the 99% level and automatically determine the number of lags needed in our autoregressive model as one below this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=np.where(np.array(np.abs(pacf)>2.58/np.sqrt(len(df[use_features])))==False)[0][0] -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may lead to a high order model, with more lags than strictly necessary. We could view this value, informally, as an upper bound on the number of lags needed. We can also simply identify the order of the model based on the plot of the PACF. In this case, a minimum of 4 lags appears satisfactory, although more may be needed. Unlike autoregressive models, the advantage of using fewer parameters is purely computational as adding more lags does not increase the number of parameters, only the size of the tensorial representation of the sequence data in TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c314de320>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pacf, label='pacf')\n",
    "plt.plot([2.58/np.sqrt(len(df[use_features]))]*30, label='99% confidence interval (upper)')\n",
    "plt.plot([-2.58/np.sqrt(len(df[use_features]))]*30, label='99% confidence interval (lower)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training and test set by using the first 80% of the time series and the remaining 20% for the test set. Note that the test set must be in the future of the training set to avoid look-ahead bias. Also, random sampling of the data can not be used as this would eliminate the auto-correlation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weight = 0.8\n",
    "split = int(len(df)*train_weight)\n",
    "df_train = df.iloc[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scaling\n",
    "Standardization of the data is important to avoid potential scaling difficulties in the fitting of the model. When there is more than one feature (covariate), scaling avoids one feature dominating over another due to disparate scales.\n",
    "\n",
    "To avoid introducing a look-ahead bias into the prediction, we must re-scale the training data without knowledge of the test set. Hence, we will simply standardize the training set using the mean and standard deviation of the training set and not the whole time series. Additionally, to avoid introducing a systematic bias into test set, we use the identical normalization for the test set - the mean and standard deviation of the training set are used to normalize the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.float(df_train[use_features].mean())\n",
    "sigma = np.float(df_train[use_features].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df_train = df_train[use_features].apply(lambda x: (x - mu) / sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[split:]\n",
    "std_df_test = df[use_features].apply(lambda x: (x - mu) / sigma).iloc[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow uses tensors to represent data. To perform sequence learning, the time series of variables must be transformed to a series of over-lapping sub-sequences. \n",
    "\n",
    "For example, consider a univariate time series of increasing integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the sequence length to 10, for example, we move the window forward by one observation at a time and construct new sequences:\n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10\n",
    "\n",
    "2 3 4 5 6 7 8 9 10 11\n",
    "\n",
    "3 4 5 6 7 8 9 10 11 12\n",
    "\n",
    "4 5 6 7 8 9 10 11 12 13\n",
    "\n",
    "5 6 7 8 9 10 11 12 13 14\n",
    "\n",
    "6 7 8 9 10 11 12 13 14 15\n",
    "\n",
    "\n",
    "Let's define the following function for reshaping the data into one-step ahead times series prediction format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lagged_features(value, n_steps, n_steps_ahead):\n",
    "    \"\"\"\n",
    "    value: feature value to be lagged\n",
    "    n_steps: number of lags, i.e. sequence length\n",
    "    n_steps_ahead: forecasting horizon\n",
    "    \"\"\"\n",
    "    lag_list = []\n",
    "    for lag in range(n_steps+n_steps_ahead-1, n_steps_ahead-1, -1):\n",
    "        lag_list.append(value.shift(lag))\n",
    "    return pd.concat(lag_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first transform the training input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_ahead=10\n",
    "\n",
    "x_train_list = []\n",
    "for use_feature in use_features:\n",
    "    x_train_reg = get_lagged_features(df_train, n_steps, n_steps_ahead).dropna()\n",
    "    x_train_list.append(x_train_reg)\n",
    "x_train_reg = pd.concat(x_train_list, axis=1)\n",
    "\n",
    "col_ords = []\n",
    "for i in range(n_steps):\n",
    "    for j in range(len(use_features)):\n",
    "        col_ords.append(i + j * n_steps)\n",
    "\n",
    "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
    "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
    "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))\n",
    "y_train_reg = np.reshape(y_train_reg, (y_train_reg.shape[0], 1, 1))\n",
    "\n",
    "x_test_list = []\n",
    "for use_feature in use_features:\n",
    "    x_test_reg = get_lagged_features(df_test, n_steps,n_steps_ahead).dropna()\n",
    "    x_test_list.append(x_test_reg)\n",
    "x_test_reg = pd.concat(x_test_list, axis=1)\n",
    "\n",
    "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
    "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
    "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))\n",
    "\n",
    "y_test_reg = np.reshape(y_test_reg, (y_test_reg.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of each tensor. The first digit is the number of observations. For feature arrays, the second digit is the sequence length (i.e. the number of lags in the model) and the final digit is the dimension of each element in the sequence or output vector respectively. In this case, the example performs univariate time series analysis and so the dimension of the input and output is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(361340, 4, 1) (361340, 1) (90333, 4, 1) (90453, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_reg.shape,y_train_reg.shape,x_test_reg.shape,y_test_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = y_train_reg.shape[0]\n",
    "test_batch_size = y_test_reg.shape[0]\n",
    "time_size = y_train_reg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reg = pd.concat(x_train_list, axis=1)\n",
    "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
    "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
    "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], np.int(x_train_reg.shape[1] / len(use_features)), len(use_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_reg = pd.concat(x_test_list, axis=1)\n",
    "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
    "y_test_reg = df_test.loc[x_test_reg.index, [target]].values\n",
    "x_test_reg = np.reshape(x_test_reg.values, (x_test_reg.shape[0], np.int(x_test_reg.shape[1]/len(use_features)), len(use_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlphatRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(AlphatRNN(n_units, activation='tanh', recurrent_activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def AlphaRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(AlphaRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))\n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def SimpleRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(SimpleRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True, stateful=False))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def GRU_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(GRU(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def LSTM_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1]), unroll=True)) \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoches=2000\n",
    "batch_size=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a batch size of 1000 as the dataset is reasonably large and the training time would be too long otherwise. 20 epoches have been used here, but a better approach would be to use a stopping criteria through a call back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50, min_delta=1e-7, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'rnn': {'model':'', 'function':SimpleRNN_, 'l1_reg':0.0, 'H':10, 'color': 'blue', 'label':'RNN'}, \n",
    "          'alpharnn': {'model':'', 'function':AlphaRNN_,'l1_reg':0.001, 'H':5,'color': 'green', 'label': '$\\\\alpha$-RNN' }, \n",
    "          'alphatrnn': {'model':'', 'function':AlphatRNN_,'l1_reg':0.0, 'H':5, 'color': 'cyan', 'label': '$\\\\alpha_t$-RNN'},\n",
    "          'gru': {'model':'', 'function':GRU_,'l1_reg':0.001, 'H':10, 'color': 'orange', 'label': 'GRU'},\n",
    "          'lstm': {'model':'', 'function':LSTM_,'l1_reg':0.0, 'H':5, 'color':'red', 'label': 'LSTM'}\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val=True # Warning: Changing this to True will take several hours to run\n",
    "if cross_val:\n",
    "    n_units = [5,10,20]\n",
    "    l1_reg = [0, 0.001, 0.01] #, 0.1]\n",
    "    tscv = TimeSeriesSplit(n_splits = 3)\n",
    "    param_grid = dict(n_units=n_units,l1_reg=l1_reg)\n",
    "\n",
    "    for key in params.keys(): # params[key]['function']\n",
    "        model = KerasRegressor(build_fn=params[key]['function'], epochs=max_epoches, batch_size=batch_size, verbose=2)\n",
    "        grid = GridSearchCV(estimator=model,param_grid=param_grid, cv=tscv, n_jobs=1, verbose=2)\n",
    "        grid_result = grid.fit(x_train_reg,y_train_reg,callbacks=[es])\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        params_ = grid_result.cv_results_['params']\n",
    "        for mean, stdev, param_ in zip(means, stds, params_):\n",
    "            print(\"%f (%f) with %r\" % (mean, stdev, param_))\n",
    "\n",
    "        params[key]['H'] = grid_result.best_params_['n_units']\n",
    "        params[key]['l1_reg']= grid_result.best_params_['l1_reg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train cross-validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in params.keys():\n",
    "    set_random_seed(0)\n",
    "    model=params[key]['function'](params[key]['H'],params[key]['l1_reg'])\n",
    "    model.fit(x_train_reg,y_train_reg,epochs=max_epoches, batch_size=batch_size,callbacks=[es],shuffle=False)\n",
    "    params[key]['model']=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally save the fitted model\n",
    "#reg_model.save('model/gru_regression_coinbase.hdf5', overwrite=True)  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally load a fitted model if this file exists.\n",
    "#reg_model = load_model('model/gru_regression_coinbase.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now apply the fitted RNN model to the training set and the test set, separately. We can then informally assess the extent\n",
    "of over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in params.keys():\n",
    "  model=params[key]['model']\n",
    "  model.summary()\n",
    "    \n",
    "  params[key]['MSE_train']= mean_squared_error(df_train[use_feature][n_steps+n_steps_ahead-1:],model.predict(x_train_reg, verbose=1))\n",
    "  params[key]['predict'] = model.predict(x_test_reg, verbose=1) \n",
    "  params[key]['MSE_test']= mean_squared_error(df_test[use_feature][n_steps+n_steps_ahead-1:],params[key]['predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the in-sample prediction is reliable (upper plot). Most of the out-of-sample prediction (lower plot) is also reliable although the end of the predicted sequence appears to degrade. Such a degradation indicates the prediction horizon might be too long and the model should be retrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally assess the performance of the model in and out-of-sample using the MSE. We expect the mean error to be larger on the test set than on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "upper = 2000\n",
    "plt.plot(df_test.index[n_steps+n_steps_ahead-1:upper], df_test[use_feature][n_steps+n_steps_ahead-1:upper], color=\"black\", label=\"Observed\")\n",
    "\n",
    "for key in params.keys():\n",
    "  plt.plot(df_test.index[n_steps+n_steps_ahead-1:upper], params[key]['predict'][:upper-(n_steps+n_steps_ahead-1), 0], color=params[key]['color'], label=params[key]['label'])\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.ylabel('Y', fontsize=20)\n",
    "plt.title('Observed vs Model (Testing)', fontsize=16)\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "\n",
    "for key in params.keys():\n",
    "  plt.plot(df_test.index[n_steps+n_steps_ahead-1:upper], df_test[use_feature][n_steps+n_steps_ahead-1:upper]-params[key]['predict'][:upper-(n_steps+n_steps_ahead-1), 0], color=params[key]['color'], label=params[key]['label'] + \" (\" +  str(round(params[key]['MSE_test'],3)) +\")\")\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Training)', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.ylabel('Y-$\\hat{Y}$', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fitted time series model must be examined for underfitting with a white noise test. We analyze the model residuals (i.e. the error $u_t$) to determine whether it is white noise or whether it is auto-correlated. The latter case provides statistical evidence that more lags are needed in the RNN. Box and Pierce propose the Portmanteau statistic:\n",
    "\n",
    "$$Q^*(m)=T\\sum_{l=1}^m\\hat{\\tau}_l^2,$$\n",
    "\n",
    "as a test statistic for the null hypothesis $$H_0:\\tau_1=\\dots=\\tau_m=0$$ against the alternative hypothesis $$H_a:\\tau_i\\neq 0$$ for some $$i\\in\\{1,\\dots,m\\}$$, where $T$ is the number of observations, $\\hat{\\tau}_i$ are the sample autocorrelations of the residual, and $m$ is the maximum lag used in the test. There are several heuristics in the statistics literature to determine the maximum lag such as the Schwert statistic. \n",
    "\n",
    "The Box-Pierce statistic follows an asymptotically chi-squared distribution with $m$ degrees of freedom.\n",
    "\n",
    "The Ljung-Box test statistic increases the power of the test in finite samples:\n",
    "$$Q(m)=T(T+2)\\sum_{l=1}^m\\frac{\\hat{\\tau}_l^2}{T-l}$$.\n",
    "\n",
    "This statistic also follows an asymptotically chi-squared distribution with $m$ degrees of freedom. The decision rule is to reject $H_0$ if $Q(m)>\\chi_{\\alpha}^2$ where $\\chi_{\\alpha}^2$ denotes the $100(1-\\alpha)^{th}$ percentile of a chi-squared distribution with m degrees of freedom and is the significance level for rejecting $H_0$.\n",
    "\n",
    "The test can be time consuming and we select a subset of the residuals. Here we simply set the maximum lag to 20. In the results below, we find that the p-values are all smaller than 0.01, indicating that we can reject the null at the 99% confidence level for any lag. This is strong evidence that the model is under-fitting and more lags are needed in our model. Unlike an auto-regressive model, increasing the number of lags in the RNN does not increase the number of weights. Thus there is no danger of over-fitting by increasing the lag, although there will be an increase in the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual=df_train[use_feature][n_steps:(n_steps+T)].values-(sigma*pred_train[:T, 0]+mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb,p=sm.stats.diagnostic.acorr_ljungbox(residual, lags=20, boxpierce=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Box-Ljung test statistics are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17145.79470896, 17175.19795798, 17253.24855096, 17314.11952185,\n",
       "       17398.53486084, 17453.94200507, 17458.58977022, 17458.86007284,\n",
       "       17459.12506354, 17487.48212127, 17611.68797122, 17635.65544221,\n",
       "       17637.7658394 , 17637.88988689, 17640.91371593, 17668.61017841,\n",
       "       17707.4052102 , 17728.21403799, 17741.87293566, 17765.9719557 ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
