{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha_RNNs-HFT\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.1 (27.2.2020)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X running Python 3.6.9 with the following packages:\n",
    "# tensorflow=2.0.0, keras=2.3.1, scikit-learn=0.22.1, numpy=1.18.1, matplotlib=3.1.3, pandas=1.0.3, statsmodels=0.10.1\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Bilokon P., Dixon M.F. and Halperin I., Machine Learning in Finance: From Theory to Practice, Springer Graduate Textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Introduction to Prediction with RNNs\n",
    "\n",
    "### Overview\n",
    "- This notebook provides an example of how Keras can be used to train and test TensorFlow RNNs for time series prediction. The example dataset is for predicting from noisy, non-stationary data.\n",
    "- Statistical methods used for autoregressive models shall be used to identify the sequence length needed in the RNN and to diagnose the model error.\n",
    "- Plain RNNs are not suited to non-stationary time series modeling. We can use a GRU or LSTM to model non-stationary data, since these models exhibit dynamic auto-correlation structure.\n",
    "- Unlike classical time series methods, e.g. ARIMA, there are no parametric assumptions on the distribution of the errors, and non-linear relationships between response and predictors can be captured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistician's note\n",
    "- We choose to build a model which provides strong predictive power, at the expense of reduced explanatory power. \n",
    "- Our choice to use a recurrent neural network is predicated on each observation in the time series being dependent on previous observations. The ordering of the observations therefore matters and $X$ is not iid.\n",
    "- Once the input data is appropriately scaled, model building starts with 'feature selection' - identifying the relevant features to include in the model. \n",
    "\n",
    "- In this notebook, we assume that we've already identifed the relevant set of features (i.e. there is only one time series provided).\n",
    "- Our primary concern is assessing the extent to which the model is over-fitting, by comparing the in- and out-of-sample MSEs.\n",
    "\n",
    "#### Implementation notes\n",
    "- It is important to ensure that `shuffle=False` in the fit function, otherwise the ordering of sequences is not preserved. This is especially important for methods which have memory beyond the current sequence (i.e. all methods except RNNs).\n",
    "- Time series cross-validation must be used for hyper-parameter tuning because the ordering of the data matters. In particular, the model must never use training data more recent than the forecasting observation date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dez/anaconda3/envs/malfoy/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "import keras.initializers\n",
    "from keras.layers import Dense, Layer, LSTM, GRU, SimpleRNN, RNN\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# note that the directory containing these two .py's must be in the path variable:\n",
    "from alphaRNN import AlphaRNN\n",
    "from alphatRNN import AlphatRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Data\n",
    "- The example dataset is a chronologically ordered time series. The ordering of the observations matters and each observation is not assumed to be independent (as with cross-sectional classification data). \n",
    "\n",
    "- For simplicity, since the timestamps are not important for this tutorial, an integer index has been used for the Pandas Dataframe. A real dataset would typically be indexed by a unique timestamp. The observations are not uniform and in general are less than a micro-second apart.\n",
    "\n",
    "- Each observation $X$ has three variables (a.k.a. features). Feature 3 is the smart price (VWAP) and the label indicates whether the book will up-tick, stay flat or down-tick in the next time interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pandas Dataframe, viewing the first ten rows and the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/HFT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  label\n",
       "0   0.515301       0.72   0.710953      0\n",
       "1   0.515301       0.72   0.710953      0\n",
       "2   0.515301       0.72   0.710953      0\n",
       "3   0.515301       0.72   0.710953      0\n",
       "4   0.515301       0.72   0.710953      0\n",
       "5   0.515301       0.72   0.710953      0\n",
       "6   0.515301       0.72   0.710953      0\n",
       "7   0.515301       0.72   0.710953      0\n",
       "8   0.515301       0.72   0.710953      0\n",
       "9   0.515301       0.72   0.710953      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Regression\n",
    "We consider a univariate prediction problem where the time series is given by 'feature_3' in the data frame, and for each input sequence we predict the value 10 time-steps into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = ['feature_3'] # continuous input\n",
    "target = ['feature_3'] # continuous output\n",
    "n_steps_ahead = 10 # forecasting horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity\n",
    "It is essential to determine whether the time series is \"stationary\". Informally, stationarity is when the auto-covariance is independent of time. Failure to establish stationarity will almost certainly lead to misinterpretation of model identification and diagnostics tests. Moreover, stationarity is decisive in characterizing the prediction problem and whether to use a more advanced architecture. In particular, we can expect a plain RNN to perform poorly if the data is non-stationary as the RNN exhibits fixed auto-covariance. \n",
    "\n",
    "We perform an Augmented Dickey-Fuller test to establish stationarity under the assumption that the time series has a constant bias but does not exhibit a time trend. In other words, we assume that the time series is already de-trended. \n",
    "\n",
    "If the stationarity test fails, even after first de-trending the time series, then one potential recourse is to simply take differences of time series and predict $\\Delta y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. If the p-value is above $(1-\\alpha)$, then we cannot reject that there is a unit root. Note that a subset of the time series is used to reduce the computation time of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df['feature_3'][:200000]\n",
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF: -5.314335374054756\n",
      "p-value: 5.11341915615086e-06,\n",
      "N: 19964, \n",
      "critical values: {'1%': -3.4306775967247427, '5%': -2.8616847862243144, '10%': -2.5668470657535196}\n"
     ]
    }
   ],
   "source": [
    "adf_results_string = 'ADF: {}\\np-value: {},\\nN: {}, \\ncritical values: {}'\n",
    "print(adf_results_string.format(adf, p, nobs, cvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive Model Identification: The partial auto-correlation\n",
    "It is important to determine the number of lags, the sequence length, required in the RNN by statistical analysis. A brute-force approach will in general be too time-consuming.\n",
    "\n",
    "A partial auto-correlation at lag $h\\geq 2$ is a conditional auto-correlation between a variable, $X_t$, and its $h^{th}$ lag, $X_{t-h}$ under the assumption that we control for the values of the intermediate lags, $X_{t-1},\\dots, X_{t-h+1}$:\n",
    "\n",
    "$$\\begin{align}\\tau_h&:=\\tau(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1})\\\\\n",
    "&:=\\frac{\\gamma(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1})}{\\sqrt{\\gamma(X_t |X_{t-1},\\dots, X_{t-h+1})\\gamma(X_{t-h} |X_{t-1},\\dots, X_{t-h+1}))}}\n",
    ",\\end{align}$$\n",
    "where $\\gamma_h:=\\gamma(X_tX_{t-h})$ is the lag-$h$ autocovariance. The partial autocorrelation function $\\tau_h:\\mathbb{N} \\rightarrow [-1,1]$ is a map $h:\\mapsto \\tau_h$.\n",
    "\n",
    "The estimated partial auto-correlation function (PACF) can be used to identify the order of an autoregressive time series model. Values of $|\\tau_h|$ greater or equal to $\\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{T}}$, where $T$ is the number of observations and $\\Phi(z)$ is the standard normal CDF, are significant lag $h$ partial autocorelations at the $\\alpha$ confidence level.\n",
    "\n",
    "We use the stattools package to estimat the PACF. The `nlags` parameter is the maximum number of lags used for PACF estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf = sm.tsa.stattools.pacf(df[use_features], nlags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Phi^{-1}(0.99) \\simeq 2.58$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzsy_bjyqjwh"
   },
   "outputs": [],
   "source": [
    "T = len(df[use_features])\n",
    "\n",
    "sig_test = lambda tau_h: np.abs(tau_h) > 2.58/np.sqrt(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the first lag which isn't significant at the 99% level and automatically determine the number of lags needed in our autoregressive model as one below this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps set to 23\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pacf)):\n",
    "    if sig_test(pacf[i]) == False:\n",
    "        n_steps = i - 1\n",
    "        print('n_steps set to', n_steps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may lead to a high order model, with more lags than strictly necessary. We could view this value, informally, as an upper bound on the number of lags needed. We can also simply identify the order of the model based on the plot of the PACF. In this case, a minimum of 2 lags appears satisfactory, although more may be needed. Unlike autoregressive models, the advantage of using fewer parameters is purely computational as adding more lags does not increase the number of parameters, only the size of the tensorial representation of the sequence data in TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pacf, label='pacf')\n",
    "plt.plot([2.58/np.sqrt(T)]*30, label='99% confidence interval (upper)')\n",
    "plt.plot([-2.58/np.sqrt(T)]*30, label='99% confidence interval (lower)')\n",
    "plt.xlabel('number of lags')\n",
    "plt.xticks(np.arange(0,30,2))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the time series into training and testing sets\n",
    "Split the training and test set by using the first 80% of the time series and the remaining 20% for the test set. Note that the test set must be in the future of the training set to avoid look-ahead bias. Also, random sampling of the data can not be used as this would eliminate the auto-correlation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weight = 0.8\n",
    "split = int(len(df)*train_weight)\n",
    "\n",
    "df_train = df[use_features].iloc[:split]\n",
    "df_test = df[use_features].iloc[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scaling\n",
    "Standardization of the data is important to avoid potential scaling difficulties in the fitting of the model. When there is more than one feature (covariate), scaling avoids one feature dominating over another due to disparate scales.\n",
    "\n",
    "To avoid introducing a look-ahead bias into the prediction, we must re-scale the training data without knowledge of the test set. Hence, we will simply standardize the training set using the mean and standard deviation of the training set and not the whole time series. Additionally, to avoid introducing a systematic bias into test set, we use the identical normalization for the test set - the mean and standard deviation of the training set are used to normalize the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that for a multivariate time series, you would need to scale \n",
    "# each variable by its own mean and standard deviation in the training set\n",
    "mu = np.float(df_train.mean())\n",
    "sigma = np.float(df_train.std())\n",
    "\n",
    "stdize_input = lambda x: (x - mu) / sigma\n",
    "\n",
    "df_train = df_train.apply(stdize_input)\n",
    "df_test = df_test.apply(stdize_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow uses tensors to represent data. To perform sequence learning, the time series of variables must be transformed to a series of over-lapping sub-sequences. \n",
    "\n",
    "For example, consider a univariate time series of increasing integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the sequence length to 10, for example, we move the window forward by one observation at a time and construct new sequences:\n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10\n",
    "\n",
    "2 3 4 5 6 7 8 9 10 11\n",
    "\n",
    "3 4 5 6 7 8 9 10 11 12\n",
    "\n",
    "4 5 6 7 8 9 10 11 12 13\n",
    "\n",
    "5 6 7 8 9 10 11 12 13 14\n",
    "\n",
    "6 7 8 9 10 11 12 13 14 15\n",
    "\n",
    "\n",
    "Let's define the following function for reshaping the data into one-step ahead times series prediction format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lagged_features(df, n_steps, n_steps_ahead):\n",
    "    \"\"\"\n",
    "    df: pandas DataFrame of time series to be lagged\n",
    "    n_steps: number of lags, i.e. sequence length\n",
    "    n_steps_ahead: forecasting horizon\n",
    "    \"\"\"\n",
    "    lag_list = []\n",
    "    for lag in range(n_steps + n_steps_ahead - 1, n_steps_ahead - 1, -1):\n",
    "        lag_list.append(df.shift(lag))\n",
    "    lag_array = np.dstack([i[n_steps+n_steps_ahead-1:] for i in lag_list])\n",
    "    # We swap the last two dimensions so each slice along the first dimension\n",
    "    # is the same shape as the corresponding segment of the input time series \n",
    "    lag_array = np.swapaxes(lag_array, 1, -1)\n",
    "    return lag_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first transform the training input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_lagged_features(df_train[use_features], n_steps, n_steps_ahead)\n",
    "y_train =  df_train[target].values[n_steps + n_steps_ahead - 1:]\n",
    "\n",
    "x_test = get_lagged_features(df_test[use_features], n_steps, n_steps_ahead)\n",
    "y_test =  df_test[target].values[n_steps + n_steps_ahead - 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of each tensor. The first digit is the number of observations. For feature arrays, the second digit is the sequence length (i.e. the number of lags in the model) and the final digit is the dimension of each element in the sequence or output vector respectively. In this case, the example performs univariate time series analysis and so the dimension of the input and output is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(826793, 23, 1), (826793, 1), (206675, 23, 1), (206675, 1)]\n"
     ]
    }
   ],
   "source": [
    "print([tensor.shape for tensor in (x_train, y_train, x_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlphatRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(AlphatRNN(n_units, activation='tanh', recurrent_activation='sigmoid', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def AlphaRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(AlphaRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True))\n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def SimpleRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(SimpleRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True, stateful=False))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def GRU_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(GRU(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def LSTM_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True)) \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=100, min_delta=1e-7, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'rnn': {\n",
    "        'model': None, 'function': SimpleRNN_, 'l1_reg': 0.0, 'H': 20, \n",
    "        'color': 'blue', 'label':'RNN'}, \n",
    "    'alpharnn': {\n",
    "        'model': None, 'function': AlphaRNN_, 'l1_reg': 0.0, 'H': 10, \n",
    "        'color': 'green', 'label': '$\\\\alpha$-RNN' }, \n",
    "    'alphatrnn': {\n",
    "        'model': None, 'function':AlphatRNN_, 'l1_reg': 0.0, 'H': 5, \n",
    "        'color': 'cyan', 'label': '$\\\\alpha_t$-RNN'},\n",
    "    'gru': {\n",
    "        'model': None, 'function':GRU_,'l1_reg': 0.0, 'H': 10, \n",
    "        'color': 'orange', 'label': 'GRU'},\n",
    "    'lstm': {\n",
    "        'model': None, 'function': LSTM_,'l1_reg': 0.0, 'H': 10, \n",
    "        'color':'red', 'label': 'LSTM'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below performs a grid search to optimise the `n_units` and `l1_reg` for each of the models. \n",
    "\n",
    "The results are cross-validated to avoid over-fitting. Scikit-Learn's `TimeSeriesSplit` function is used to partition the data into 5 pairs of training and testing sets, where the test data is always ahead of the training data in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val = False # Warning: Changing this to True will take several hours to run\n",
    "\n",
    "if cross_val:\n",
    "    n_units = [5, 10, 20]\n",
    "    l1_reg = [0, 0.001, 0.01, 0.1]\n",
    "    \n",
    "    # A dictionary containing a list of values to be iterated through\n",
    "    # for each parameter of the model included in the search\n",
    "    param_grid = {'n_units': n_units, 'l1_reg': l1_reg}\n",
    "    \n",
    "    # In the kth split, TimeSeriesSplit returns first \n",
    "    # k folds as train set and the (k+1)th fold as test set.\n",
    "    tscv = TimeSeriesSplit(n_splits = 3)\n",
    "    \n",
    "    # A grid search is performed for each of the models, and the parameter set which\n",
    "    # performs best over all the cross-validation splits is saved in the `params` dictionary\n",
    "    for key in params.keys():\n",
    "        print('*********', key)\n",
    "        model = KerasRegressor(build_fn=params[key]['function'], epochs=3, \n",
    "                               batch_size=batch_size, verbose=2)\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                            cv=tscv, n_jobs=1, verbose=2)\n",
    "        grid_result = grid.fit(x_train, y_train, callbacks=[es])\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        \n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        params_ = grid_result.cv_results_['params']\n",
    "        for mean, stdev, param_ in zip(means, stds, params_):\n",
    "            print(\"%f (%f) with %r\" % (mean, stdev, param_))\n",
    "            \n",
    "        params[key]['H'] = grid_result.best_params_['n_units']\n",
    "        params[key]['l1_reg']= grid_result.best_params_['l1_reg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models with selected parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the grid search was performed, the parameters `n_units` and `l1_reg` with the best cross-validated results on the training set are used. If not, the values from the initialisation of `params` above are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in params.keys():\n",
    "    tf.random.set_seed(0)\n",
    "    print('Training', key, 'model')\n",
    "    model = params[key]['function'](params[key]['H'], params[key]['l1_reg'])\n",
    "    model.fit(x_train, y_train, epochs=max_epochs, \n",
    "              batch_size=batch_size, callbacks=[es], shuffle=False)\n",
    "    params[key]['model'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally save the fitted models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally save the fitted model\n",
    "for key in params.keys():\n",
    "    params[key]['model'].save(key + '-RNNs-HFT.hdf5', overwrite=True)  # creates a HDF5 fil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally load the fitted models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally load the fitted model\n",
    "custom_objects = {'AlphaRNN': AlphaRNN, 'AlphatRNN': AlphatRNN}\n",
    "\n",
    "for key in params.keys():\n",
    "    params[key]['model'] = load_model(key + '-RNNs-HFT.hdf5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the value of $\\alpha \\in [0,1]$ for the alpha-RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = params['alpharnn']['model']\n",
    "\n",
    "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "for name, weight in zip(names, weights):\n",
    "    if 'alpha:0' in name:\n",
    "      print(\"alpha= \" + str(sigmoid(*weight)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in params.keys():\n",
    "    model = params[key]['model']\n",
    "    model.summary()\n",
    "    \n",
    "    params[key]['pred_train'] = model.predict(x_train, verbose=1)\n",
    "    params[key]['MSE_train'] = mean_squared_error(y_train, params[key]['pred_train'])\n",
    "    \n",
    "    params[key]['pred_test'] = model.predict(x_test, verbose=1) \n",
    "    params[key]['MSE_test'] = mean_squared_error(y_test, params[key]['pred_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now apply the fitted RNN model to the training set and the test set, separately. We can then informally assess the extent\n",
    "of over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare = params.keys() # e.g. ['rnn', 'alpharnn'] or ['lstm']\n",
    "l, u = (None, None) # lower and upper indices of range to plot \n",
    "ds = max(1, len(y_train[l:u])//10**4) # set `None` to disable downsampling\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = np.arange(len(y_train))[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_train'][l:u:ds]\n",
    "    label = params[key]['label'] + ' (train MSE: %.2e)' % params[key]['MSE_train']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.plot(x_vals, y_train[l:u:ds], c=\"black\", label=\"Observed\", lw=1)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.xlabel('Time (ticks)', fontsize=14)\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Training)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys() # e.g. ['rnn', 'alpharnn'] or ['lstm']\n",
    "l, u = (None, None) # lower and upper indices of range to plot \n",
    "ds = max(1, len(y_test[l:u])//10**4) # set `None` to disable downsampling\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = len(y_train) + np.arange(len(y_test))[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_test'][l:u:ds]\n",
    "    label = params[key]['label'] + ' (test MSE: %.2e)' % params[key]['MSE_test']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.plot(x_vals, y_test[l:u:ds], c=\"black\", label=\"Observed\", lw=1)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Testing)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys() # e.g. ['rnn', 'alpharnn'] or ['lstm']\n",
    "l, u = (None, None) # e.g. (None, 100000) lower and upper indices of range to plot \n",
    "ds = max(1, len(y_test[l:u])//10**4) # set `None` to disable downsampling\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = len(y_train) + np.arange(len(y_test))[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_test'][l:u:ds] - y_test[l:u:ds]\n",
    "    label = params[key]['label'] + ' (test MSE: %.2e)' % params[key]['MSE_test']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.xlabel('Time (ticks)', fontsize=14)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Testing)', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys() # e.g. ['rnn', 'alpharnn'] or ['lstm']\n",
    "l, u = (None, None) # lower and upper indices of range to plot - e.g. (None, 10000)\n",
    "ds = max(1, len(y_train[l:u])//10**4) # set `None` to disable downsampling\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = np.arange(len(y_train))[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_train'][l:u:ds] - y_train[l:u:ds]\n",
    "    label = params[key]['label'] + ' (train MSE: %.2e)' % params[key]['MSE_train']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.xlabel('Time (ticks)', fontsize=14)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Training)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fitted time series model must be examined for underfitting with a white noise test. We analyze the model residuals (i.e. the error $u_t$) to determine whether it is white noise or whether it is auto-correlated. The latter case provides statistical evidence that more lags are needed in the RNN. Box and Pierce propose the Portmanteau statistic:\n",
    "\n",
    "$$Q^*(m)=T\\sum_{l=1}^m\\hat{\\tau}_l^2,$$  \n",
    "as a test statistic for the null hypothesis $H_0:\\tau_1=\\dots=\\tau_m=0$ against the alternative hypothesis $H_a:\\tau_i\\neq 0$ for some $i\\in\\{1,\\dots,m\\}$, where $T$ is the number of observations, $\\hat{\\tau}_i$ are the sample autocorrelations of the residual, and $m$ is the maximum lag used in the test. There are several heuristics in the statistics literature to determine the maximum lag such as the Schwert statistic. \n",
    "\n",
    "The Box-Pierce statistic follows an asymptotically chi-squared distribution with $m$ degrees of freedom.\n",
    "\n",
    "The Ljung-Box test statistic increases the power of the test in finite samples:\n",
    "$$Q(m)=T(T+2)\\sum_{l=1}^m\\frac{\\hat{\\tau}_l^2}{T-l}$$\n",
    "This statistic also follows an asymptotically chi-squared distribution with $m$ degrees of freedom. The decision rule is to reject $H_0$ if $Q(m)>\\chi_{\\alpha}^2$ where $\\chi_{\\alpha}^2$ denotes the $100(1-\\alpha)^{th}$ percentile of a chi-squared distribution with m degrees of freedom and is the significance level for rejecting $H_0$.\n",
    "\n",
    "The test can be time consuming and we select a subset of the residuals. Here we simply set the maximum lag to 20. In the results below, we find that the p-values are all smaller than 0.01, indicating that we can reject the null at the 99% confidence level for any lag. This is strong evidence that the model is under-fitting and more lags are needed in our model. Unlike an auto-regressive model, increasing the number of lags in the RNN does not increase the number of weights. Thus there is no danger of over-fitting by increasing the lag, although there will be an increase in the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bpznKjiqjzQ"
   },
   "outputs": [],
   "source": [
    "# number of samples to use for computing test statistic\n",
    "n = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIPFOuKAqjzV"
   },
   "outputs": [],
   "source": [
    "key = 'alpharnn'\n",
    "predicted = params[key]['pred_test']\n",
    "residual = df_test[-n:] - predicted[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-radzroqjzb"
   },
   "outputs": [],
   "source": [
    "lb, p = sm.stats.diagnostic.acorr_ljungbox(residual, lags=20, boxpierce=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWvgE9ouqjzf"
   },
   "source": [
    "The Box-Ljung test statistics are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rn0gmXkDqjzg",
    "outputId": "517efe22-705e-420e-f8ed-e3d4f0180835"
   },
   "outputs": [],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0MmBUaDqjzk"
   },
   "source": [
    "The p-values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTEIIp-Sqjzn",
    "outputId": "15031561-6612-43b6-99c0-dafa4a5c034e"
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
