{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trading'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e773d0945281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpreadTrading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/ml/tutorials/dq-MM/tgym/envs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpreadTrading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trading'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this example we demonstrate how to implement a DQN agent and\n",
    "train it to trade optimally on an order book price signal.\n",
    "Training time is short and results are unstable.\n",
    "Do not hesitate to run several times and/or tweak parameters to get better results.\n",
    "Inspired from https://github.com/keon/deep-q-learning\n",
    "\"\"\"\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tgym.envs import SpreadTrading\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 episodes,\n",
    "                 episode_length,\n",
    "                 memory_size=2000,\n",
    "                 train_interval=100,\n",
    "                 gamma=0.95,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=64,\n",
    "                 epsilon_min=0.01\n",
    "                 ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = [None] * memory_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decrement = (self.epsilon - epsilon_min)\\\n",
    "            * train_interval / (episodes * episode_length)  # linear decrease rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_interval = train_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.brain = self._build_brain()\n",
    "        self.i = 0\n",
    "\n",
    "    def _build_brain(self):\n",
    "        \"\"\"Build the agent's brain\n",
    "        \"\"\"\n",
    "        brain = Sequential()\n",
    "        neurons_per_layer = 24\n",
    "        activation = \"relu\"\n",
    "        brain.add(Dense(neurons_per_layer,\n",
    "                        input_dim=self.state_size,\n",
    "                        activation=activation))\n",
    "        brain.add(Dense(neurons_per_layer, activation=activation))\n",
    "        brain.add(Dense(self.action_size, activation='linear'))\n",
    "        brain.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return brain\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Acting Policy of the DQNAgent\n",
    "        \"\"\"\n",
    "        action = np.zeros(self.action_size)\n",
    "        valid_actions = []\n",
    "        position = state[-3:]\n",
    "        \n",
    "        if all(position == [1,0,0]): # flat\n",
    "            valid_actions = [0,1,2]\n",
    "        elif all(position == [0,1,0]):  # long\n",
    "            valid_actions = [0,2]  # hold or sell\n",
    "        else: # short\n",
    "            valid_actions = [0,1]  # hold or buy\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:    \n",
    "            action[valid_actions[random.randrange(len(valid_actions))]] = 1   \n",
    "        else:\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            act_values = self.brain.predict(state)\n",
    "            #print act_values[0]\n",
    "            action[valid_actions[np.argmax(act_values[0][valid_actions])]] = 1\n",
    "        return action\n",
    "\n",
    "    def observe(self, state, action, reward, next_state, done, warming_up=False):\n",
    "        \"\"\"Memory Management and training of the agent\n",
    "        \"\"\"\n",
    "        self.i = (self.i + 1) % self.memory_size\n",
    "        self.memory[self.i] = (state, action, reward, next_state, done)\n",
    "        if (not warming_up) and (self.i % self.train_interval) == 0:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decrement\n",
    "            state, action, reward, next_state, done = self._get_batches()\n",
    "            reward += (self.gamma\n",
    "                       * np.logical_not(done)\n",
    "                       * np.amax(self.brain.predict(next_state),\n",
    "                                 axis=1))\n",
    "            q_target = self.brain.predict(state)\n",
    "            \n",
    "            \n",
    "            q_target[action[0], action[1]] = reward\n",
    "            return self.brain.fit(state, q_target,\n",
    "                                  batch_size=self.batch_size,\n",
    "                                  epochs=1,\n",
    "                                  verbose=False)\n",
    "\n",
    "    def _get_batches(self):\n",
    "        \"\"\"Selecting a batch of memory\n",
    "           Split it into categorical subbatches\n",
    "           Process action_batch into a position vector\n",
    "        \"\"\"\n",
    "        batch = np.array(random.sample(self.memory, self.batch_size))\n",
    "        state_batch = np.concatenate(batch[:, 0])\\\n",
    "            .reshape(self.batch_size, self.state_size)\n",
    "        action_batch = np.concatenate(batch[:, 1])\\\n",
    "            .reshape(self.batch_size, self.action_size)\n",
    "        reward_batch = batch[:, 2]\n",
    "        next_state_batch = np.concatenate(batch[:, 3])\\\n",
    "            .reshape(self.batch_size, self.state_size)\n",
    "        done_batch = batch[:, 4]\n",
    "        # action processing\n",
    "        action_batch = np.where(action_batch == 1)\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tgym.gens.csvstream import CSVStreamer\n",
    "# Instantiating the environmnent\n",
    "generator = CSVStreamer(filename='../data/AMZN-L1.csv')\n",
    "\n",
    "episodes = 200\n",
    "episode_length = 400\n",
    "trading_fee = .0\n",
    "time_fee = 0\n",
    "history_length = 2\n",
    "environment = SpreadTrading(spread_coefficients=[1],\n",
    "                            data_generator=generator,\n",
    "                                trading_fee=trading_fee,\n",
    "                                time_fee=time_fee,\n",
    "                                history_length=history_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = environment.reset()\n",
    "# Instantiating the agent\n",
    "memory_size = 3000\n",
    "state_size = len(state)\n",
    "gamma = 0.96\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "action_size = len(SpreadTrading._actions)\n",
    "train_interval = 10\n",
    "learning_rate = 0.001\n",
    "agent = DQNAgent(state_size=state_size,\n",
    "                     action_size=action_size,\n",
    "                     memory_size=memory_size,\n",
    "                     episodes=episodes,\n",
    "                     episode_length=episode_length,\n",
    "                     train_interval=train_interval,\n",
    "                     gamma=gamma,\n",
    "                     learning_rate=learning_rate,\n",
    "                     batch_size=batch_size,\n",
    "                     epsilon_min=epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:0| rew:190.7| eps:1.0| loss:1320.7024\n",
      "Ep:1| rew:205.4| eps:0.99| loss:322.8394\n",
      "Ep:2| rew:213.2| eps:0.99| loss:164.0004\n",
      "Ep:3| rew:206.1| eps:0.98| loss:228.3607\n",
      "Ep:4| rew:193.2| eps:0.98| loss:235.4003\n",
      "Ep:5| rew:217.1| eps:0.97| loss:207.3416\n",
      "Ep:6| rew:234.5| eps:0.97| loss:131.1903\n",
      "Ep:7| rew:183.1| eps:0.96| loss:70.4631\n",
      "Ep:8| rew:167.2| eps:0.96| loss:231.1891\n",
      "Ep:9| rew:164.6| eps:0.95| loss:31.6883\n",
      "Ep:10| rew:216.5| eps:0.95| loss:128.4852\n",
      "Ep:11| rew:228.6| eps:0.94| loss:37.8735\n",
      "Ep:12| rew:153.1| eps:0.94| loss:47.451\n",
      "Ep:13| rew:145.8| eps:0.93| loss:35.3577\n",
      "Ep:14| rew:166.0| eps:0.93| loss:45.2217\n",
      "Ep:15| rew:188.6| eps:0.92| loss:20.8173\n",
      "Ep:16| rew:99.4| eps:0.92| loss:38.2144\n",
      "Ep:17| rew:139.8| eps:0.91| loss:55.0336\n",
      "Ep:18| rew:145.2| eps:0.91| loss:15.7032\n",
      "Ep:19| rew:90.7| eps:0.9| loss:17.2164\n",
      "Ep:20| rew:133.2| eps:0.9| loss:13.0918\n",
      "Ep:21| rew:157.4| eps:0.89| loss:24.3297\n",
      "Ep:22| rew:162.7| eps:0.89| loss:31.9908\n",
      "Ep:23| rew:136.0| eps:0.88| loss:33.9085\n",
      "Ep:24| rew:124.3| eps:0.88| loss:20.988\n",
      "Ep:25| rew:121.8| eps:0.87| loss:21.2155\n",
      "Ep:26| rew:131.2| eps:0.87| loss:7.3684\n",
      "Ep:27| rew:107.3| eps:0.86| loss:226.5197\n",
      "Ep:28| rew:100.3| eps:0.86| loss:2051.8882\n",
      "Ep:29| rew:153.8| eps:0.85| loss:2237.5264\n",
      "Ep:30| rew:119.6| eps:0.85| loss:634.8828\n",
      "Ep:31| rew:126.5| eps:0.84| loss:141.6414\n",
      "Ep:32| rew:154.2| eps:0.84| loss:151.9571\n",
      "Ep:33| rew:159.0| eps:0.83| loss:207.3652\n",
      "Ep:34| rew:127.8| eps:0.83| loss:497.2773\n",
      "Ep:35| rew:133.2| eps:0.82| loss:44.5124\n",
      "Ep:36| rew:177.0| eps:0.82| loss:93.3626\n",
      "Ep:37| rew:128.1| eps:0.81| loss:127.9203\n",
      "Ep:38| rew:173.2| eps:0.81| loss:49.6293\n",
      "Ep:39| rew:173.7| eps:0.8| loss:25.889\n",
      "Ep:40| rew:163.4| eps:0.8| loss:617.5057\n",
      "Ep:41| rew:176.7| eps:0.79| loss:340.7276\n",
      "Ep:42| rew:136.3| eps:0.79| loss:339.2628\n",
      "Ep:43| rew:193.2| eps:0.78| loss:110.2288\n",
      "Ep:44| rew:212.8| eps:0.78| loss:466.5658\n",
      "Ep:45| rew:187.1| eps:0.77| loss:191.7846\n",
      "Ep:46| rew:164.2| eps:0.77| loss:223.3781\n",
      "Ep:47| rew:155.1| eps:0.76| loss:259.3936\n",
      "Ep:48| rew:156.5| eps:0.76| loss:57.3521\n",
      "Ep:49| rew:165.2| eps:0.75| loss:18.1503\n",
      "Ep:50| rew:196.3| eps:0.75| loss:21.0247\n",
      "Ep:51| rew:190.2| eps:0.74| loss:12.4639\n",
      "Ep:52| rew:160.3| eps:0.74| loss:3.9543\n",
      "Ep:53| rew:187.2| eps:0.73| loss:10.1311\n",
      "Ep:54| rew:150.0| eps:0.73| loss:22.7736\n",
      "Ep:55| rew:158.8| eps:0.72| loss:10.7933\n",
      "Ep:56| rew:151.8| eps:0.72| loss:18.6703\n",
      "Ep:57| rew:184.7| eps:0.71| loss:64.0978\n",
      "Ep:58| rew:174.9| eps:0.71| loss:15.6896\n",
      "Ep:59| rew:204.2| eps:0.7| loss:37.5546\n",
      "Ep:60| rew:179.0| eps:0.7| loss:68.9188\n",
      "Ep:61| rew:151.2| eps:0.69| loss:51.1175\n",
      "Ep:62| rew:137.7| eps:0.69| loss:391.8592\n",
      "Ep:63| rew:187.5| eps:0.68| loss:8.9852\n",
      "Ep:64| rew:139.5| eps:0.68| loss:50.7608\n",
      "Ep:65| rew:141.2| eps:0.67| loss:120.7284\n",
      "Ep:66| rew:180.9| eps:0.67| loss:10.2314\n",
      "Ep:67| rew:188.6| eps:0.66| loss:15.2982\n",
      "Ep:68| rew:160.3| eps:0.66| loss:15.4464\n",
      "Ep:69| rew:103.0| eps:0.65| loss:10.7495\n",
      "Ep:70| rew:138.1| eps:0.65| loss:31.1471\n",
      "Ep:71| rew:141.3| eps:0.64| loss:63.4959\n",
      "Ep:72| rew:107.2| eps:0.64| loss:36.6088\n",
      "Ep:73| rew:113.2| eps:0.63| loss:25.6335\n",
      "Ep:74| rew:138.7| eps:0.63| loss:101.7907\n",
      "Ep:75| rew:141.4| eps:0.62| loss:48.5061\n",
      "Ep:76| rew:120.2| eps:0.62| loss:83.537\n",
      "Ep:77| rew:136.1| eps:0.61| loss:106.0313\n",
      "Ep:78| rew:137.6| eps:0.61| loss:5.7443\n",
      "Ep:79| rew:135.0| eps:0.6| loss:42.9996\n",
      "Ep:80| rew:172.2| eps:0.6| loss:65.9362\n",
      "Ep:81| rew:144.1| eps:0.59| loss:40.1166\n",
      "Ep:82| rew:163.2| eps:0.59| loss:36.1934\n",
      "Ep:83| rew:150.0| eps:0.58| loss:17.0102\n",
      "Ep:84| rew:182.2| eps:0.58| loss:7.9186\n",
      "Ep:85| rew:204.3| eps:0.57| loss:4.0924\n",
      "Ep:86| rew:154.0| eps:0.57| loss:5.5424\n",
      "Ep:87| rew:157.4| eps:0.56| loss:2.4048\n",
      "Ep:88| rew:158.6| eps:0.56| loss:1.3282\n",
      "Ep:89| rew:176.9| eps:0.55| loss:1.2242\n",
      "Ep:90| rew:139.1| eps:0.55| loss:1.795\n",
      "Ep:91| rew:128.1| eps:0.54| loss:1.8999\n",
      "Ep:92| rew:150.4| eps:0.54| loss:2.672\n",
      "Ep:93| rew:112.4| eps:0.53| loss:95.0417\n",
      "Ep:94| rew:152.7| eps:0.53| loss:60.7633\n",
      "Ep:95| rew:110.7| eps:0.52| loss:89.9263\n",
      "Ep:96| rew:153.9| eps:0.52| loss:6.0889\n",
      "Ep:97| rew:116.6| eps:0.51| loss:4.6168\n",
      "Ep:98| rew:141.0| eps:0.51| loss:42.2743\n",
      "Ep:99| rew:177.8| eps:0.51| loss:17.1664\n",
      "Ep:100| rew:178.7| eps:0.5| loss:2.6204\n",
      "Ep:101| rew:190.6| eps:0.5| loss:2.5996\n",
      "Ep:102| rew:172.0| eps:0.49| loss:3.8234\n",
      "Ep:103| rew:146.4| eps:0.49| loss:1.3226\n",
      "Ep:104| rew:138.2| eps:0.48| loss:2.8171\n",
      "Ep:105| rew:129.0| eps:0.48| loss:1.7732\n",
      "Ep:106| rew:120.9| eps:0.47| loss:1.0355\n",
      "Ep:107| rew:133.2| eps:0.47| loss:1.1799\n",
      "Ep:108| rew:123.2| eps:0.46| loss:1.4602\n",
      "Ep:109| rew:144.9| eps:0.46| loss:0.9317\n",
      "Ep:110| rew:116.0| eps:0.45| loss:0.8178\n",
      "Ep:111| rew:207.3| eps:0.45| loss:6.4128\n",
      "Ep:112| rew:196.1| eps:0.44| loss:719.8365\n",
      "Ep:113| rew:134.0| eps:0.44| loss:8719.1738\n",
      "Ep:114| rew:100.7| eps:0.43| loss:367.7935\n",
      "Ep:115| rew:132.0| eps:0.43| loss:236.2919\n",
      "Ep:116| rew:118.9| eps:0.42| loss:230.7863\n",
      "Ep:117| rew:90.9| eps:0.42| loss:4883.0664\n",
      "Ep:118| rew:86.7| eps:0.41| loss:39.5117\n",
      "Ep:119| rew:144.9| eps:0.41| loss:31.4336\n",
      "Ep:120| rew:113.9| eps:0.4| loss:11.6107\n",
      "Ep:121| rew:138.9| eps:0.4| loss:15.1587\n",
      "Ep:122| rew:140.5| eps:0.39| loss:15.3879\n",
      "Ep:123| rew:92.8| eps:0.39| loss:15.6233\n",
      "Ep:124| rew:88.7| eps:0.38| loss:9.0707\n",
      "Ep:125| rew:106.7| eps:0.38| loss:6.0199\n",
      "Ep:126| rew:120.9| eps:0.37| loss:8.6225\n",
      "Ep:127| rew:73.5| eps:0.37| loss:12.6221\n",
      "Ep:128| rew:93.2| eps:0.36| loss:53.8208\n",
      "Ep:129| rew:82.9| eps:0.36| loss:28.622\n",
      "Ep:130| rew:72.5| eps:0.35| loss:16.4558\n",
      "Ep:131| rew:54.1| eps:0.35| loss:51.4382\n",
      "Ep:132| rew:41.3| eps:0.34| loss:120.7255\n",
      "Ep:133| rew:45.2| eps:0.34| loss:122.2546\n",
      "Ep:134| rew:56.9| eps:0.33| loss:203.3176\n",
      "End of data reached, rewinding.\n",
      "Ep:135| rew:84.4| eps:0.33| loss:64.2199\n",
      "Ep:136| rew:215.2| eps:0.32| loss:247.2944\n",
      "Ep:137| rew:202.0| eps:0.32| loss:19.4316\n",
      "Ep:138| rew:263.6| eps:0.31| loss:52.1212\n",
      "Ep:139| rew:261.5| eps:0.31| loss:46.0739\n",
      "Ep:140| rew:239.5| eps:0.3| loss:19.0214\n",
      "Ep:141| rew:471.1| eps:0.3| loss:17.7226\n",
      "Ep:142| rew:174.3| eps:0.29| loss:5.2987\n",
      "Ep:143| rew:222.3| eps:0.29| loss:13.8839\n",
      "Ep:144| rew:109.5| eps:0.28| loss:55.5041\n",
      "Ep:145| rew:209.8| eps:0.28| loss:29.0609\n",
      "Ep:146| rew:179.9| eps:0.27| loss:5.5425\n",
      "Ep:147| rew:139.8| eps:0.27| loss:5.3141\n",
      "Ep:148| rew:156.3| eps:0.26| loss:2.6395\n",
      "Ep:149| rew:157.9| eps:0.26| loss:6.6449\n",
      "Ep:150| rew:176.8| eps:0.25| loss:83.42\n",
      "Ep:151| rew:113.3| eps:0.25| loss:17.8569\n",
      "Ep:152| rew:168.1| eps:0.24| loss:4.7346\n",
      "Ep:153| rew:200.4| eps:0.24| loss:2.2799\n",
      "Ep:154| rew:253.1| eps:0.23| loss:5.3914\n",
      "Ep:155| rew:135.1| eps:0.23| loss:42.8607\n",
      "Ep:156| rew:137.1| eps:0.22| loss:2.7615\n",
      "Ep:157| rew:165.0| eps:0.22| loss:5.3021\n",
      "Ep:158| rew:177.4| eps:0.21| loss:2.5928\n",
      "Ep:159| rew:126.6| eps:0.21| loss:1.5518\n",
      "Ep:160| rew:169.5| eps:0.2| loss:2.4601\n",
      "Ep:161| rew:171.1| eps:0.2| loss:4.0583\n",
      "Ep:162| rew:112.2| eps:0.19| loss:2.4135\n",
      "Ep:163| rew:138.9| eps:0.19| loss:1.9947\n",
      "Ep:164| rew:164.5| eps:0.18| loss:1.5546\n",
      "Ep:165| rew:177.3| eps:0.18| loss:1.3405\n",
      "Ep:166| rew:168.8| eps:0.17| loss:2.1226\n",
      "Ep:167| rew:135.9| eps:0.17| loss:1.1879\n",
      "Ep:168| rew:182.6| eps:0.16| loss:1.4836\n",
      "Ep:169| rew:135.4| eps:0.16| loss:0.9468\n",
      "Ep:170| rew:111.4| eps:0.15| loss:11.3621\n",
      "Ep:171| rew:66.4| eps:0.15| loss:46.6386\n",
      "Ep:172| rew:128.6| eps:0.14| loss:34.0484\n"
     ]
    }
   ],
   "source": [
    "# Warming up the agent\n",
    "for _ in range(memory_size):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = environment.step(action)\n",
    "        agent.observe(state, action, reward, next_state, done, warming_up=True)\n",
    "        \n",
    "rews = []\n",
    "losses = []\n",
    "epsilons = []\n",
    "# Training the agent\n",
    "for ep in range(episodes):\n",
    "    state = environment.reset()\n",
    "    rew = 0\n",
    "    for _ in range(episode_length):\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        for position in environment._positions:\n",
    "          if all(environment._position==environment._positions[position]):\n",
    "            position_name = position\n",
    "        \n",
    "    \n",
    "        for _action in environment._actions:\n",
    "          if all(action==environment._actions[_action]):\n",
    "            action_name = _action\n",
    "        \n",
    "        next_state, reward, done, _ = environment.step(action)\n",
    "        \n",
    "        for position in environment._positions:\n",
    "          if all(environment._position==environment._positions[position]):\n",
    "            next_position_name = position\n",
    "        \n",
    "        \n",
    "           \n",
    "        #print position_name, action_name, next_position_name\n",
    "        loss = agent.observe(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        rew += reward\n",
    "    print(\"Ep:\" + str(ep)\n",
    "           + \"| rew:\" + str(round(rew, 2))\n",
    "           + \"| eps:\" + str(round(agent.epsilon, 2))\n",
    "           + \"| loss:\" + str(round(loss.history[\"loss\"][0], 4)))\n",
    "    rews.append(rew)\n",
    "    epsilons.append(agent.epsilon)\n",
    "    losses.append(loss.history[\"loss\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(epsilons)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('eps')\n",
    "plt.savefig('epsilons.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rews)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.savefig('rewards.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('losses')\n",
    "plt.ylabel('losses')\n",
    "plt.savefig('losses.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the agent\n",
    "done = False\n",
    "state = environment.reset()\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    \n",
    "    for position in environment._positions:\n",
    "          if all(environment._position==environment._positions[position]):\n",
    "            position_name = position\n",
    "        \n",
    "    for _action in environment._actions:\n",
    "          if all(action==environment._actions[_action]):\n",
    "            action_name = _action\n",
    "    \n",
    "    state, _, done, info = environment.step(action)\n",
    "    \n",
    "    for position in environment._positions:\n",
    "          if all(environment._position==environment._positions[position]):\n",
    "            next_position_name = position\n",
    "               \n",
    "    #print position_name, action_name, next_position_name\n",
    "    \n",
    "    if 'status' in info and info['status'] == 'Closed plot':\n",
    "        done = True\n",
    "    else:\n",
    "        environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
