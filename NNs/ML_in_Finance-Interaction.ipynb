{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_in_Finance-Deep-Learning-Interpretability\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.0 (08.09.2019)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6 and Tensorflow 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Bilokon P., Dixon M.F. and I. Halperin, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as tds\n",
    "from statsmodels.api import add_constant\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Generation Process (DGP)\n",
    "\n",
    "$Y=X_1+X_2 + X_1X_2+\\epsilon, ~X_1, X_2 \\sim N(0,1,), \\epsilon \\sim N(0,\\sigma_n^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5000\n",
    "np.random.seed(7)\n",
    "X = np.zeros(shape=(M,2))\n",
    "sigma_n = 0.01\n",
    "X[:int(M/2),0]= np.random.randn(int(M/2))\n",
    "X[:int(M/2),1]= np.random.randn(int(M/2))\n",
    "\n",
    "X[int(M/2):,0]= -X[:int(M/2),0]\n",
    "X[int(M/2):,1]= -X[:int(M/2),1]\n",
    "\n",
    "eps= np.random.randn(M)\n",
    "Y= 1.0*X[:,0] + 1.0*X[:,1] + 1.0*X[:,0]*X[:,1] + sigma_n*eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use OLS to fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_results = sm.OLS(Y, sm.add_constant(X)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ols=ols_results.predict(sm.add_constant(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.503</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.503</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2526.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Sep 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:29:19</td>     <th>  Log-Likelihood:    </th> <td> -8821.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5000</td>      <th>  AIC:               </th> <td>1.765e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4997</td>      <th>  BIC:               </th> <td>1.767e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.0283</td> <td>    0.020</td> <td>    1.415</td> <td> 0.157</td> <td>   -0.011</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.9905</td> <td>    0.020</td> <td>   49.342</td> <td> 0.000</td> <td>    0.951</td> <td>    1.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    1.0038</td> <td>    0.020</td> <td>   49.944</td> <td> 0.000</td> <td>    0.964</td> <td>    1.043</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>312.522</td> <th>  Durbin-Watson:     </th> <td>   1.986</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1343.060</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.116</td>  <th>  Prob(JB):          </th> <td>2.28e-292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.528</td>  <th>  Cond. No.          </th> <td>    1.02</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.503\n",
       "Model:                            OLS   Adj. R-squared:                  0.503\n",
       "Method:                 Least Squares   F-statistic:                     2526.\n",
       "Date:                Wed, 11 Sep 2019   Prob (F-statistic):               0.00\n",
       "Time:                        11:29:19   Log-Likelihood:                -8821.7\n",
       "No. Observations:                5000   AIC:                         1.765e+04\n",
       "Df Residuals:                    4997   BIC:                         1.767e+04\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0283      0.020      1.415      0.157      -0.011       0.067\n",
       "x1             0.9905      0.020     49.342      0.000       0.951       1.030\n",
       "x2             1.0038      0.020     49.944      0.000       0.964       1.043\n",
       "==============================================================================\n",
       "Omnibus:                      312.522   Durbin-Watson:                   1.986\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1343.060\n",
       "Skew:                           0.116   Prob(JB):                    2.28e-292\n",
       "Kurtosis:                       5.528   Cond. No.                         1.02\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with a ffwd neural network with no hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_NN0_model(l1_reg=0.0):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=2, kernel_initializer='normal')) #, activation='None'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KerasRegressor(build_fn=linear_NN0_model, epochs=40, batch_size=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5000/5000 [==============================] - 0s 98us/step - loss: 3.1989 - mean_absolute_error: 1.3359 - mean_squared_error: 3.1989\n",
      "Epoch 2/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 2.4760 - mean_absolute_error: 1.1770 - mean_squared_error: 2.4760\n",
      "Epoch 3/40\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 2.1612 - mean_absolute_error: 1.1040 - mean_squared_error: 2.1612\n",
      "Epoch 4/40\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 2.0463 - mean_absolute_error: 1.0780 - mean_squared_error: 2.0463\n",
      "Epoch 5/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 2.0098 - mean_absolute_error: 1.0700 - mean_squared_error: 2.0098\n",
      "Epoch 6/40\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 2.0003 - mean_absolute_error: 1.0685 - mean_squared_error: 2.0003\n",
      "Epoch 7/40\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.9975 - mean_absolute_error: 1.0680 - mean_squared_error: 1.9975\n",
      "Epoch 8/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.9968 - mean_absolute_error: 1.0684 - mean_squared_error: 1.9968\n",
      "Epoch 9/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9969 - mean_absolute_error: 1.0685 - mean_squared_error: 1.9969\n",
      "Epoch 10/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9971 - mean_absolute_error: 1.0687 - mean_squared_error: 1.9971\n",
      "Epoch 11/40\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.9968 - mean_absolute_error: 1.0685 - mean_squared_error: 1.9968\n",
      "Epoch 12/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.9968 - mean_absolute_error: 1.0684 - mean_squared_error: 1.9968\n",
      "Epoch 13/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9966 - mean_absolute_error: 1.0685 - mean_squared_error: 1.9966\n",
      "Epoch 14/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9971 - mean_absolute_error: 1.0685 - mean_squared_error: 1.9971\n",
      "Epoch 15/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9968 - mean_absolute_error: 1.0684 - mean_squared_error: 1.9968\n",
      "Epoch 16/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9969 - mean_absolute_error: 1.0687 - mean_squared_error: 1.9969\n",
      "Epoch 17/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9968 - mean_absolute_error: 1.0684 - mean_squared_error: 1.9968\n",
      "Epoch 18/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.9968 - mean_absolute_error: 1.0685 - mean_squared_error: 1.9968\n",
      "Epoch 19/40\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 1.9968 - mean_absolute_error: 1.0685 - mean_squared_error: 1.9968\n",
      "Epoch 20/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9969 - mean_absolute_error: 1.0686 - mean_squared_error: 1.9969\n",
      "Epoch 21/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.9972 - mean_absolute_error: 1.0684 - mean_squared_error: 1.9972\n",
      "Epoch 22/40\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.9971 - mean_absolute_error: 1.0686 - mean_squared_error: 1.9971\n",
      "Epoch 23/40\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.9968 - mean_absolute_error: 1.0684 - mean_squared_error: 1.9968\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c33f1e940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the weights are close to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.9865925],\n",
       "        [0.9967309]], dtype=float32), array([0.02955675], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with a FFW Neural Network with one hidden layer (unactivated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # number of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_NN1_model(l1_reg=0.0):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=2, kernel_initializer='normal')) \n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KerasRegressor(build_fn=linear_NN1_model, epochs=50, batch_size=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 1s 114us/step - loss: 2.7009 - mean_absolute_error: 1.2199 - mean_squared_error: 2.7009\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 1.9986 - mean_absolute_error: 1.0678 - mean_squared_error: 1.9986\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 1.9991 - mean_absolute_error: 1.0695 - mean_squared_error: 1.9991\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 2.0035 - mean_absolute_error: 1.0701 - mean_squared_error: 2.0035\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 2.0022 - mean_absolute_error: 1.0698 - mean_squared_error: 2.0022\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 0s 86us/step - loss: 1.9999 - mean_absolute_error: 1.0687 - mean_squared_error: 1.9999\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.9998 - mean_absolute_error: 1.0690 - mean_squared_error: 1.9998\n",
      "Epoch 8/50\n",
      "5000/5000 [==============================] - 0s 99us/step - loss: 2.0014 - mean_absolute_error: 1.0684 - mean_squared_error: 2.0014\n",
      "Epoch 9/50\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 2.0028 - mean_absolute_error: 1.0696 - mean_squared_error: 2.0028\n",
      "Epoch 10/50\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 2.0011 - mean_absolute_error: 1.0700 - mean_squared_error: 2.0011\n",
      "Epoch 11/50\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 2.0010 - mean_absolute_error: 1.0691 - mean_squared_error: 2.0010\n",
      "Epoch 12/50\n",
      "5000/5000 [==============================] - 1s 109us/step - loss: 2.0027 - mean_absolute_error: 1.0702 - mean_squared_error: 2.0027\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c341fe710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.37814584  0.30338565 -0.27131096  0.3684254   0.33092746 -0.30008778\n",
      "  -0.2542268  -0.29876462 -0.2947897   0.28479353]\n",
      " [-0.2638578   0.30114004 -0.3520979   0.26289687  0.20439926 -0.35611275\n",
      "  -0.3233128  -0.2558721  -0.3140065   0.320782  ]] [[-0.28841314]\n",
      " [ 0.31222868]\n",
      " [-0.32610312]\n",
      " [ 0.30656126]\n",
      " [ 0.3128089 ]\n",
      " [-0.32401562]\n",
      " [-0.2921686 ]\n",
      " [-0.36572298]\n",
      " [-0.33868715]\n",
      " [ 0.3483182 ]]\n"
     ]
    }
   ],
   "source": [
    "W1=lm.model.get_weights()[0]\n",
    "b1=lm.model.get_weights()[1]\n",
    "W2=lm.model.get_weights()[2]\n",
    "b2=lm.model.get_weights()[3]\n",
    "print(W1, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the coefficients are close to one and the intercept is close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0=np.dot(np.transpose(W2), b1) + b2\n",
    "beta_1=np.dot(np.transpose(W2), W1[0])\n",
    "beta_2=np.dot(np.transpose(W2), W1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03241214] [0.9885408] [0.95098716]\n"
     ]
    }
   ],
   "source": [
    "print(beta_0,beta_1,beta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with a FFW Neural Network with one hidden layer (tanh activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hidden neurons\n",
    "n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with non-linear activation\n",
    "def linear_NN1_model_act(l1_reg=0.0):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=2, kernel_initializer='normal', activation='tanh'))\n",
    "    model.add(Dense(1, kernel_initializer='normal')) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KerasRegressor(build_fn=linear_NN1_model_act, epochs=100, batch_size=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 1s 125us/step - loss: 2.7012 - mean_absolute_error: 1.2210 - mean_squared_error: 2.7012\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 0s 92us/step - loss: 1.9903 - mean_absolute_error: 1.0652 - mean_squared_error: 1.9903\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 0s 93us/step - loss: 1.9608 - mean_absolute_error: 1.0588 - mean_squared_error: 1.9608\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 0s 92us/step - loss: 1.9343 - mean_absolute_error: 1.0521 - mean_squared_error: 1.9343\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 0s 86us/step - loss: 1.9035 - mean_absolute_error: 1.0448 - mean_squared_error: 1.9035\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 0s 89us/step - loss: 1.8692 - mean_absolute_error: 1.0367 - mean_squared_error: 1.8692\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 0s 89us/step - loss: 1.8355 - mean_absolute_error: 1.0270 - mean_squared_error: 1.8355\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.8023 - mean_absolute_error: 1.0189 - mean_squared_error: 1.8023\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 0s 90us/step - loss: 1.7652 - mean_absolute_error: 1.0093 - mean_squared_error: 1.7652\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.7344 - mean_absolute_error: 1.0005 - mean_squared_error: 1.7344\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.7034 - mean_absolute_error: 0.9925 - mean_squared_error: 1.7034\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.6739 - mean_absolute_error: 0.9840 - mean_squared_error: 1.6739\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.6441 - mean_absolute_error: 0.9760 - mean_squared_error: 1.6441\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.6126 - mean_absolute_error: 0.9666 - mean_squared_error: 1.6126\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.5636 - mean_absolute_error: 0.9554 - mean_squared_error: 1.5636\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.4783 - mean_absolute_error: 0.9322 - mean_squared_error: 1.4783\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 0s 99us/step - loss: 1.3792 - mean_absolute_error: 0.9030 - mean_squared_error: 1.3792\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 1s 108us/step - loss: 1.2945 - mean_absolute_error: 0.8768 - mean_squared_error: 1.2945\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.2324 - mean_absolute_error: 0.8592 - mean_squared_error: 1.2324\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.1925 - mean_absolute_error: 0.8468 - mean_squared_error: 1.1925\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.1652 - mean_absolute_error: 0.8393 - mean_squared_error: 1.1652\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.1464 - mean_absolute_error: 0.8338 - mean_squared_error: 1.1464\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.1308 - mean_absolute_error: 0.8298 - mean_squared_error: 1.1308\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 0s 93us/step - loss: 1.1182 - mean_absolute_error: 0.8265 - mean_squared_error: 1.1182\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 0s 87us/step - loss: 1.1104 - mean_absolute_error: 0.8247 - mean_squared_error: 1.1104\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 0s 86us/step - loss: 1.1006 - mean_absolute_error: 0.8219 - mean_squared_error: 1.1006\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 0s 98us/step - loss: 1.0952 - mean_absolute_error: 0.8207 - mean_squared_error: 1.0952\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 0s 99us/step - loss: 1.0877 - mean_absolute_error: 0.8182 - mean_squared_error: 1.0877\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.0847 - mean_absolute_error: 0.8190 - mean_squared_error: 1.0847\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0786 - mean_absolute_error: 0.8172 - mean_squared_error: 1.0786\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.0730 - mean_absolute_error: 0.8148 - mean_squared_error: 1.0730\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 0s 100us/step - loss: 1.0720 - mean_absolute_error: 0.8154 - mean_squared_error: 1.0720\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 1s 130us/step - loss: 1.0667 - mean_absolute_error: 0.8135 - mean_squared_error: 1.0667\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 0s 97us/step - loss: 1.0608 - mean_absolute_error: 0.8117 - mean_squared_error: 1.0608\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 0s 90us/step - loss: 1.0612 - mean_absolute_error: 0.8122 - mean_squared_error: 1.0612\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 0s 86us/step - loss: 1.0585 - mean_absolute_error: 0.8114 - mean_squared_error: 1.0585 0s - loss: 1.0612 - mean_absolute_error: 0.8172 - mean_squar\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0556 - mean_absolute_error: 0.8107 - mean_squared_error: 1.0556\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 0s 93us/step - loss: 1.0529 - mean_absolute_error: 0.8104 - mean_squared_error: 1.0529\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0514 - mean_absolute_error: 0.8092 - mean_squared_error: 1.0514\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0504 - mean_absolute_error: 0.8092 - mean_squared_error: 1.0504\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 1.0476 - mean_absolute_error: 0.8082 - mean_squared_error: 1.0476\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0477 - mean_absolute_error: 0.8085 - mean_squared_error: 1.0477\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0449 - mean_absolute_error: 0.8075 - mean_squared_error: 1.0449\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0453 - mean_absolute_error: 0.8086 - mean_squared_error: 1.0453\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 0s 79us/step - loss: 1.0429 - mean_absolute_error: 0.8078 - mean_squared_error: 1.0429\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.0396 - mean_absolute_error: 0.8062 - mean_squared_error: 1.0396\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 0s 78us/step - loss: 1.0409 - mean_absolute_error: 0.8071 - mean_squared_error: 1.0409\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 1.0389 - mean_absolute_error: 0.8061 - mean_squared_error: 1.0389\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 0s 76us/step - loss: 1.0381 - mean_absolute_error: 0.8057 - mean_squared_error: 1.0381\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 0s 80us/step - loss: 1.0368 - mean_absolute_error: 0.8054 - mean_squared_error: 1.0368\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 0s 98us/step - loss: 1.0360 - mean_absolute_error: 0.8050 - mean_squared_error: 1.0360\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 0s 84us/step - loss: 1.0349 - mean_absolute_error: 0.8053 - mean_squared_error: 1.0349\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 0s 77us/step - loss: 1.0329 - mean_absolute_error: 0.8043 - mean_squared_error: 1.0329\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 0s 81us/step - loss: 1.0334 - mean_absolute_error: 0.8038 - mean_squared_error: 1.0334\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 0s 88us/step - loss: 1.0327 - mean_absolute_error: 0.8048 - mean_squared_error: 1.0327\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 0s 75us/step - loss: 1.0315 - mean_absolute_error: 0.8045 - mean_squared_error: 1.0315\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0308 - mean_absolute_error: 0.8043 - mean_squared_error: 1.0308\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 0s 97us/step - loss: 1.0287 - mean_absolute_error: 0.8028 - mean_squared_error: 1.0287\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 0s 82us/step - loss: 1.0286 - mean_absolute_error: 0.8034 - mean_squared_error: 1.0286\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 0s 85us/step - loss: 1.0271 - mean_absolute_error: 0.8029 - mean_squared_error: 1.0271\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0270 - mean_absolute_error: 0.8032 - mean_squared_error: 1.0270\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0272 - mean_absolute_error: 0.8036 - mean_squared_error: 1.0272\n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 0s 94us/step - loss: 1.0245 - mean_absolute_error: 0.8023 - mean_squared_error: 1.0245\n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 0s 74us/step - loss: 1.0247 - mean_absolute_error: 0.8028 - mean_squared_error: 1.0247\n",
      "Epoch 65/100\n",
      "5000/5000 [==============================] - 0s 83us/step - loss: 1.0234 - mean_absolute_error: 0.8024 - mean_squared_error: 1.0234\n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0231 - mean_absolute_error: 0.8015 - mean_squared_error: 1.0231\n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0226 - mean_absolute_error: 0.8020 - mean_squared_error: 1.0226\n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0223 - mean_absolute_error: 0.8016 - mean_squared_error: 1.0223\n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0224 - mean_absolute_error: 0.8034 - mean_squared_error: 1.0224\n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0221 - mean_absolute_error: 0.8021 - mean_squared_error: 1.0221\n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0210 - mean_absolute_error: 0.8021 - mean_squared_error: 1.0210\n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0216 - mean_absolute_error: 0.8015 - mean_squared_error: 1.0216\n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0200 - mean_absolute_error: 0.8016 - mean_squared_error: 1.0200\n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0193 - mean_absolute_error: 0.8004 - mean_squared_error: 1.0193\n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0203 - mean_absolute_error: 0.8020 - mean_squared_error: 1.0203\n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0177 - mean_absolute_error: 0.8004 - mean_squared_error: 1.0177\n",
      "Epoch 77/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0184 - mean_absolute_error: 0.8021 - mean_squared_error: 1.0184\n",
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0174 - mean_absolute_error: 0.8007 - mean_squared_error: 1.0174\n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0163 - mean_absolute_error: 0.8002 - mean_squared_error: 1.0163\n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0182 - mean_absolute_error: 0.8012 - mean_squared_error: 1.0182\n",
      "Epoch 81/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0155 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0155\n",
      "Epoch 82/100\n",
      "5000/5000 [==============================] - 0s 73us/step - loss: 1.0158 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0158\n",
      "Epoch 83/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0142 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0142\n",
      "Epoch 84/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0164 - mean_absolute_error: 0.8015 - mean_squared_error: 1.0164\n",
      "Epoch 85/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0144 - mean_absolute_error: 0.7999 - mean_squared_error: 1.0144\n",
      "Epoch 86/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0147 - mean_absolute_error: 0.7994 - mean_squared_error: 1.0147\n",
      "Epoch 87/100\n",
      "5000/5000 [==============================] - 0s 71us/step - loss: 1.0154 - mean_absolute_error: 0.8011 - mean_squared_error: 1.0154\n",
      "Epoch 88/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0132 - mean_absolute_error: 0.7996 - mean_squared_error: 1.0132\n",
      "Epoch 89/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0129 - mean_absolute_error: 0.7994 - mean_squared_error: 1.0129\n",
      "Epoch 90/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0136 - mean_absolute_error: 0.7995 - mean_squared_error: 1.0136\n",
      "Epoch 91/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0127 - mean_absolute_error: 0.7996 - mean_squared_error: 1.0127\n",
      "Epoch 92/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0127 - mean_absolute_error: 0.7991 - mean_squared_error: 1.0127\n",
      "Epoch 93/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0120 - mean_absolute_error: 0.8001 - mean_squared_error: 1.0120\n",
      "Epoch 94/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0109 - mean_absolute_error: 0.7987 - mean_squared_error: 1.0109\n",
      "Epoch 95/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0136 - mean_absolute_error: 0.7999 - mean_squared_error: 1.0136\n",
      "Epoch 96/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0128 - mean_absolute_error: 0.8003 - mean_squared_error: 1.0128\n",
      "Epoch 97/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0131 - mean_absolute_error: 0.7988 - mean_squared_error: 1.0131\n",
      "Epoch 98/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0097 - mean_absolute_error: 0.7983 - mean_squared_error: 1.0097\n",
      "Epoch 99/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0118 - mean_absolute_error: 0.8004 - mean_squared_error: 1.0118\n",
      "Epoch 100/100\n",
      "5000/5000 [==============================] - 0s 72us/step - loss: 1.0112 - mean_absolute_error: 0.7987 - mean_squared_error: 1.0112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c344d9400>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the activation function is tanh\n",
    "def sensitivities(lm, X):\n",
    "    \n",
    "    W1=lm.model.get_weights()[0]\n",
    "    b1=lm.model.get_weights()[1]\n",
    "    W2=lm.model.get_weights()[2]\n",
    "    b2=lm.model.get_weights()[3]\n",
    "    \n",
    "    \n",
    "    M = np.shape(X)[0]\n",
    "    p = np.shape(X)[1]\n",
    "\n",
    "    beta=np.array([0]*M*(p+1), dtype='float32').reshape(M,p+1)\n",
    "    beta_interact=np.array([0]*M*p*p, dtype='float32').reshape(M,p,p)\n",
    "    \n",
    "    beta[:,0]= (np.dot(np.transpose(W2),np.tanh(b1)) + b2)[0] # intercept \\beta_0= F_{W,b}(0)\n",
    "    for i in range(M):\n",
    " \n",
    "      Z1 = np.tanh(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1)\n",
    "      #Z1 = np.maximum(np.dot(np.transpose(W1),np.transpose(X[i,])) + b1,0) \n",
    "      \n",
    "      D = np.diag(1-Z1**2) \n",
    "      D_prime =np.diag(-2*Z1*(1-Z1**2))   # needed for interaction term     \n",
    "      #D = np.diag(np.sign(Z1))  \n",
    "        \n",
    "      for j in range(p):  \n",
    "          beta[i,j+1]=np.dot(np.transpose(W2),np.dot(D,W1[j]))\n",
    "          #interaction term \n",
    "          for k in range(p):\n",
    "            beta_interact[i,j,k]=np.dot(np.transpose(W2),np.dot(np.diag(W1[j]), np.dot(D_prime,W1[k])))  \n",
    "    \n",
    "            \n",
    "    return(beta, beta_interact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, beta_inter=sensitivities(lm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the intercept is close to one and the coefficients are close to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06321367  1.0291194   0.9991432 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(beta, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1101154e-06 9.6800876e-01 9.7116160e-01]\n"
     ]
    }
   ],
   "source": [
    "print(np.std(beta, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01137162  0.9574556 ]\n",
      " [ 0.9574556  -0.01866435]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(beta_inter, axis=0)) # off-diagonals are interaction terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
