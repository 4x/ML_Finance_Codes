{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_in_Finance-Autoencoders\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.0 (24.7.2019)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6 and Tensorflow 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Bilokon P., Dixon M.F. and I. Halperin, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of this notebook is to first review PCA for yield curve dimension reduction. Linear autoencoders are then compared with PCA. Finally we evaluate deep autoencoders for yield curve compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "Principal component analysis requires finding the weights which maximize the maximum weighted variance of the data $Y$: \n",
    "$$\\max_{w:||w||=1}Var(w^TY) = \\max_{w:||w||=1}w^TVar(Y)w$$\n",
    "\n",
    "Setting $w=v$, with $v$ the eigenvector corresponding to the largest eigenvalue $\\lambda$ of $C:=Var(Y)$ will yield an orthogonal projection which is an optimal solution. From the eigenvalue problem, we can obtain the eigenvalues of $V$:\n",
    "\n",
    "$$C v=\\lambda v$$\n",
    "$$(C-\\lambda I)v=0$$\n",
    "$$ |C-\\lambda I|=0$$\n",
    "\n",
    "Plugging the eigenvalues into second equation above gives the eigenvectors. The result can be written as:\n",
    "\n",
    "$$\\Lambda=V^TCV,$$\n",
    "\n",
    "where $\\Lambda$ is the diagonal matrix of descending eigenvalues and $P$ is the corresponding orthornormal matrix of eigenvectors. Rearanging gives the spectral decomposition of the covariance matrix.\n",
    "\n",
    "$$C=P\\Lambda P^T$$.\n",
    "\n",
    "The transformation of Y onto the orthonormal basis spanned by the columns of $P$ is:\n",
    "$$X=P^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs & Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Autoencoder\n",
    "def LinearAE(y, dimension, learning_rate = 1e-4, regularization = 5e-4, epochs=10):\n",
    "    input = Input(shape=(y.shape[1],))\n",
    "    encoded = Dense(dimension, activation='linear',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(input)\n",
    "    decoded = Dense(y.shape[1], activation='linear',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(encoded)\n",
    "    autoencoder = models.Model(input, decoded)\n",
    "    autoencoder.compile(optimizer=optimizers.adam(lr=learning_rate), loss='mean_squared_error')\n",
    "    autoencoder.fit(y, y, epochs=epochs, batch_size=4, shuffle=True)\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('yield_curve.csv', sep=',')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'],infer_datetime_format=True)\n",
    "\n",
    "df.set_index('Date', drop=True, inplace=True)\n",
    "\n",
    "df.index.names = [None]\n",
    "\n",
    "df.drop('Index', axis=1, inplace=True)\n",
    "\n",
    "dt = df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uncomment to perform PCA on the daily yield changes $\\Delta Y_0$.\n",
    "#delta=df.diff(1)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.plot(df.index,df)\n",
    "plt.xlim(df.index.min(), df.index.max())\n",
    "# plt.ylim(0, 0.1)\n",
    "plt.axhline(y=0,c=\"grey\",linewidth=0.5,zorder=0)\n",
    "for i in range(df.index.min().year, df.index.max().year+1):\n",
    "    plt.axvline(x=df.index[df.index.searchsorted(pd.datetime(i,1,1))-1],\n",
    "                c=\"grey\", linewidth=0.5, zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = 6\n",
    "rows = 3\n",
    "num_years = df.index.max().year-df.index.min().year\n",
    "rows = math.ceil(num_years/cols)\n",
    "\n",
    "plt.figure(figsize=(24,(24/cols)*rows))\n",
    "\n",
    "plt.subplot2grid((rows,cols), (0,0), colspan=cols, rowspan=rows)\n",
    "\n",
    "\n",
    "colnum = 0\n",
    "rownum = 0\n",
    "for year in range(df.index.min().year,df.index.max().year+1):\n",
    "    year_start = df.index[df.index.searchsorted(pd.datetime(year,1,1))]\n",
    "    year_end = df.index[df.index.searchsorted(pd.datetime(year,12,31))]\n",
    "    \n",
    "    plt.subplot2grid((rows,cols), (rownum,colnum), colspan=1, rowspan=1)\n",
    "    plt.title('{0}'.format(year))\n",
    "    plt.xlim(0, len(dt.index)-1)\n",
    "    plt.ylim(np.min(dt.values), np.max(dt.values))\n",
    "    plt.xticks(range(len(dt.index)), dt.index, size='small')\n",
    "    \n",
    "    plt.plot(dt.ix[:,year_start:year_end].values)\n",
    "    \n",
    "    if colnum != cols-1:\n",
    "        colnum += 1\n",
    "    else:\n",
    "        colnum = 0\n",
    "        rownum += 1\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection onto Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the PCA (Eigenvectors & Eigenvalues of the covariance matrix)\n",
    "pcaA = PCA(n_components=3, copy=True, whiten=False)\n",
    "\n",
    "# transform the dataset onto the first two eigenvectors\n",
    "pcaA.fit(df)\n",
    "dpca = pd.DataFrame(pcaA.transform(df))\n",
    "dpca.index = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and plot the reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.matmul(df-np.mean(df, axis=0),pcaA.components_.T)\n",
    "Y_recon_pca=np.matmul(x, pcaA.components_) + np.array(np.mean(df, axis=0)).reshape(1,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.plot(df.index, pd.DataFrame(Y_recon_pca-df))\n",
    "plt.xlim(df.index.min(), df.index.max())\n",
    "# plt.ylim(0, 0.1)\n",
    "plt.axhline(y=0,c=\"grey\",linewidth=0.5,zorder=0)\n",
    "for i in range(df.index.min().year, df.index.max().year+1):\n",
    "    plt.axvline(x=df.index[df.index.searchsorted(pd.datetime(i,1,1))-1],\n",
    "                c=\"grey\", linewidth=0.5, zorder=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the amount of variance explained by each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,pc in enumerate(pcaA.explained_variance_ratio_):\n",
    "    print('{0}.\\t{1:2.2f}%'.format(i+1,pc*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loading vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10))\n",
    "plt.title('First {0} PCA components'.format(np.shape(np.transpose(pcaA.components_))[-1]))\n",
    "\n",
    "plt.plot(np.transpose(pcaA.components_[0]), label='1. PC')\n",
    "plt.xticks(range(len(dt.index)), dt.index, size='small')\n",
    "plt.plot(np.transpose(pcaA.components_[1]), label='2. PC')\n",
    "plt.plot(np.transpose(pcaA.components_[2]), label='3. PC')\n",
    "\n",
    "plt.legend() #'upper right')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the linear auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae = LinearAE(df, 3, regularization=0, epochs=300)\n",
    "(w1,b1,w2,b2)=ae.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the first singular vectors of the decoder weight matrix\n",
    "The left singular vectors approximate the PCA loading vectors (up to a sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_linear_ae, _, _) = np.linalg.svd(w2.T, full_matrices=False)# PCA by applying SVD to linear autoencoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(df, axis=0)\n",
    "x_hat=np.matmul(df-mu,p_linear_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae = pd.DataFrame(x_hat)\n",
    "dae.index = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonalize the sample covariance matrix with the m-loading vectors:\n",
    "$$\\text{P}_m^T \\text{Y}_0 \\text{Y}_0^T \\text{P}_m$$\n",
    "\n",
    "The sample covariance matrix is given by C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.matmul((df-mu).T,df-mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the transformed sample covariances\n",
    "Begin with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda=np.matmul(pcaA.components_,np.matmul(C, pcaA.components_.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(Lambda, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the explained variances by each component (check with above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*np.diag(Lambda)/np.sum(np.diag(Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the transformation of the covariance matrix using the decoder weights. Note that the matrix $\\Lambda$ is no longer diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda=np.matmul(w2,np.matmul(C, w2.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(Lambda, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, perform the transformation of the covariance matrix using the left singular vectors of the decoder weights. Note that the matrix $\\Lambda$ is diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda=np.matmul(p_linear_ae.T,np.matmul(C, p_linear_ae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(Lambda, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the explained variances by each diagonal component and compare with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*np.diag(Lambda)/np.sum(np.diag(Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data projected onto the principal components\n",
    "First show the data on the principal components obtained by PCA. Then show the data using the left singular vectors from the decoder weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "merged_years = 1\n",
    "pc1 = 0\n",
    "pc2 = 1\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "plt.title('Projection on {0}. and {1}. PC'.format(pc1+1,pc2+1))\n",
    "plt.axhline(y=0,c=\"grey\",linewidth=1.0,zorder=0)\n",
    "plt.axvline(x=0,c=\"grey\",linewidth=1.0,zorder=0)\n",
    "    \n",
    "sc = plt.scatter(dpca.loc[:,pc1],dpca.loc[:,pc2], c=[d.year for d in dpca.index], cmap='rainbow')\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_ticks(ticks=np.unique([d.year for d in dpca.index])[::1])\n",
    "cb.set_ticklabels(np.unique([d.year for d in dpca.index])[::1])\n",
    "\n",
    "for year in range(dpca.index.min().year,dpca.index.max().year+1,merged_years):\n",
    "    year_start = dpca.index[dpca.index.searchsorted(pd.datetime(year,1,1))]\n",
    "    year_end = dpca.index[dpca.index.searchsorted(pd.datetime(year+merged_years-1,12,31))]\n",
    "    \n",
    "    plt.annotate('{0}'.format(year), xy=(dpca.loc[year_start,pc1],dpca.loc[year_start,pc2]), xytext=(dpca.loc[year_start,pc1],dpca.loc[year_start,pc2]))\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show the data using the left singular vectors from the decoder weight matrix. Note that the sign of the first principal component has been changed for ease of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "merged_years = 1\n",
    "pc1 = 0\n",
    "pc2 = 1\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "plt.title('Projection on {0}. and {1}. PC'.format(pc1+1,pc2+1))\n",
    "plt.axhline(y=0,c=\"grey\",linewidth=1.0,zorder=0)\n",
    "plt.axvline(x=0,c=\"grey\",linewidth=1.0,zorder=0)\n",
    "    \n",
    "sc = plt.scatter(-dae.loc[:,pc1],dae.loc[:,pc2], c=[d.year for d in dae.index], cmap='rainbow')\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_ticks(ticks=np.unique([d.year for d in dae.index])[::1])\n",
    "cb.set_ticklabels(np.unique([d.year for d in dae.index])[::1])\n",
    "\n",
    "for year in range(dae.index.min().year,dae.index.max().year+1,merged_years):\n",
    "    year_start = dae.index[dae.index.searchsorted(pd.datetime(year,1,1))]\n",
    "    year_end = dae.index[dae.index.searchsorted(pd.datetime(year+merged_years-1,12,31))]\n",
    "    plt.annotate('{0}'.format(year), xy=(-dae.loc[year_start,pc1],dae.loc[year_start,pc2]), xytext=(-dae.loc[year_start,pc1],dae.loc[year_start,pc2]))\n",
    "\n",
    "    #plt.annotate('{0}'.format(year), xy=dae.loc[year_start,pc1],dae.loc[year_start,pc2]), xytext=(dae.loc[year_start,pc1],dae.loc[year_start,pc2]))\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the reconstruction error of the linear autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_l=ae.predict(df)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(df.index, pd.DataFrame(y_hat_l-df))\n",
    "plt.xlim(df.index.min(), df.index.max())\n",
    "# plt.ylim(0, 0.1)\n",
    "plt.ylim(-0.008, 0.008)\n",
    "plt.axhline(y=0,c=\"grey\",linewidth=0.5,zorder=0)\n",
    "for i in range(df.index.min().year, df.index.max().year+1):\n",
    "    plt.axvline(x=df.index[df.index.searchsorted(pd.datetime(i,1,1))-1],\n",
    "                c=\"grey\", linewidth=0.5, zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Autoencoder\n",
    "def DeepAE(y, dimension, learning_rate = 1e-4, regularization = 5e-4, epochs=10):\n",
    "    input = Input(shape=(y.shape[1],))\n",
    "    encoded1 = Dense(np.int(2*dimension), activation='tanh',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(input)\n",
    "    encoded2 = Dense(dimension, activation='tanh',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(encoded1)\n",
    "    decoded1 = Dense(np.int(y.shape[1]/2), activation='tanh',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(encoded2)\n",
    "    decoded2 = Dense(y.shape[1], activation='tanh',\n",
    "                    kernel_regularizer=regularizers.l2(regularization))(decoded1)\n",
    "    autoencoder = models.Model(input, decoded2)\n",
    "    autoencoder.compile(optimizer=optimizers.adam(lr=learning_rate), loss='mean_squared_error')\n",
    "    autoencoder.fit(y, y, epochs=epochs, batch_size=4, shuffle=True)\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae = DeepAE(df, 3, regularization=0, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the reconstruction error of the linear autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_d=dae.predict(df)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(df.index, pd.DataFrame(y_hat_d-df))\n",
    "plt.xlim(df.index.min(), df.index.max())\n",
    "plt.ylim(-0.008, 0.008)\n",
    "plt.axhline(y=0,c=\"grey\",linewidth=0.5,zorder=0)\n",
    "for i in range(df.index.min().year, df.index.max().year+1):\n",
    "    plt.axvline(x=df.index[df.index.searchsorted(pd.datetime(i,1,1))-1],\n",
    "                c=\"grey\", linewidth=0.5, zorder=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
